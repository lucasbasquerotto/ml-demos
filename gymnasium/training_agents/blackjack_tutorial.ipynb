{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Blackjack with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Till Zemann\n",
    "# License: MIT License\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "# Let's start by creating the blackjack environment.\n",
    "# Note: We are going to follow the rules from Sutton & Barto.\n",
    "# Other versions of the game can be found below for you to experiment.\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", sab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other possible environment configurations are:\n",
    "\n",
    "# env = gym.make('Blackjack-v1', natural=True, sab=False)\n",
    "# Whether to give an additional reward for starting with a natural blackjack, i.e. starting with an ace and ten (sum is 21).\n",
    "\n",
    "# env = gym.make('Blackjack-v1', natural=False, sab=False)\n",
    "# Whether to follow the exact rules outlined in the book by Sutton and Barto. If `sab` is `True`, the keyword argument `natural` will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment to get the first observation\n",
    "done = False\n",
    "observation, info = env.reset()\n",
    "\n",
    "# observation = (16, 9, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a random action from all valid actions\n",
    "action = env.action_space.sample()\n",
    "# action=1\n",
    "\n",
    "# execute the action in our environment and receive infos from the environment\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# observation=(24, 10, False)\n",
    "# reward=-1.0\n",
    "# terminated=True\n",
    "# truncated=False\n",
    "# info={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackjackAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "axs[0].set_title(\"Episode rewards\")\n",
    "# compute and assign a rolling average of the data to provide a smoother graph\n",
    "reward_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "axs[1].set_title(\"Episode lengths\")\n",
    "length_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "axs[2].set_title(\"Training Error\")\n",
    "training_error_moving_average = (\n",
    "    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grids(agent, usable_ace=False):\n",
    "    \"\"\"Create value and policy grid given an agent.\"\"\"\n",
    "    # convert our state-action values to state values\n",
    "    # and build a policy dictionary that maps observations to actions\n",
    "    state_value = defaultdict(float)\n",
    "    policy = defaultdict(int)\n",
    "    for obs, action_values in agent.q_values.items():\n",
    "        state_value[obs] = float(np.max(action_values))\n",
    "        policy[obs] = int(np.argmax(action_values))\n",
    "\n",
    "    player_count, dealer_count = np.meshgrid(\n",
    "        # players count, dealers face-up card\n",
    "        np.arange(12, 22),\n",
    "        np.arange(1, 11),\n",
    "    )\n",
    "\n",
    "    # create the value grid for plotting\n",
    "    value = np.apply_along_axis(\n",
    "        lambda obs: state_value[(obs[0], obs[1], usable_ace)],\n",
    "        axis=2,\n",
    "        arr=np.dstack([player_count, dealer_count]),\n",
    "    )\n",
    "    value_grid = player_count, dealer_count, value\n",
    "\n",
    "    # create the policy grid for plotting\n",
    "    policy_grid = np.apply_along_axis(\n",
    "        lambda obs: policy[(obs[0], obs[1], usable_ace)],\n",
    "        axis=2,\n",
    "        arr=np.dstack([player_count, dealer_count]),\n",
    "    )\n",
    "    return value_grid, policy_grid\n",
    "\n",
    "\n",
    "def create_plots(value_grid, policy_grid, title: str):\n",
    "    \"\"\"Creates a plot using a value and policy grid.\"\"\"\n",
    "    # create a new figure with 2 subplots (left: state values, right: policy)\n",
    "    player_count, dealer_count, value = value_grid\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # plot the state values\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "    ax1.plot_surface(\n",
    "        player_count,\n",
    "        dealer_count,\n",
    "        value,\n",
    "        rstride=1,\n",
    "        cstride=1,\n",
    "        cmap=\"viridis\",\n",
    "        edgecolor=\"none\",\n",
    "    )\n",
    "    plt.xticks(range(12, 22), range(12, 22))\n",
    "    plt.yticks(range(1, 11), [\"A\"] + list(range(2, 11)))\n",
    "    ax1.set_title(f\"State values: {title}\")\n",
    "    ax1.set_xlabel(\"Player sum\")\n",
    "    ax1.set_ylabel(\"Dealer showing\")\n",
    "    ax1.zaxis.set_rotate_label(False)\n",
    "    ax1.set_zlabel(\"Value\", fontsize=14, rotation=90)\n",
    "    ax1.view_init(20, 220)\n",
    "\n",
    "    # plot the policy\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"Accent_r\", cbar=False)\n",
    "    ax2.set_title(f\"Policy: {title}\")\n",
    "    ax2.set_xlabel(\"Player sum\")\n",
    "    ax2.set_ylabel(\"Dealer showing\")\n",
    "    ax2.set_xticklabels(range(12, 22))\n",
    "    ax2.set_yticklabels([\"A\"] + list(range(2, 11)), fontsize=12)\n",
    "\n",
    "    # add a legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=\"lightgreen\", edgecolor=\"black\", label=\"Hit\"),\n",
    "        Patch(facecolor=\"grey\", edgecolor=\"black\", label=\"Stick\"),\n",
    "    ]\n",
    "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# state values & policy with usable ace (ace counts as 11)\n",
    "value_grid, policy_grid = create_grids(agent, usable_ace=True)\n",
    "fig1 = create_plots(value_grid, policy_grid, title=\"With usable ace\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
