{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 01 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: *Self-Play*\n",
    "\n",
    "**Q**\n",
    "\n",
    "Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?\n",
    "\n",
    "**A**\n",
    "\n",
    "If the reinforcement learning algorithm played against itself with both sides learning, it would adapt over time and could learn a different policy than if it played against a random opponent. In self-play, both sides would improve as they exploit each other’s weaknesses, and with sufficient exploration, the agent should ultimately converge to the optimal policy for tic-tac-toe.\n",
    "\n",
    "Exploration is crucial here; without it, the agent might prematurely settle on suboptimal strategies due to repetitive moves against itself. However, with enough exploration, the agent would encounter a wide range of possible states, eventually refining its policy until any further games tend to end in a draw (the optimal outcome when both players are playing perfectly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: *Symmetries*\n",
    "\n",
    "**Q**\n",
    "\n",
    "Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?\n",
    "\n",
    "**A**\n",
    "\n",
    "Due to the symmetry, a state can be considered as any of the 4 different positions that can result when rotating the board. So, the state space would reduce by about 4 times (less than 4 times tough, because some configurations are the same after rotating for more than 1 of the 4 cases). It could improve even more with reflections on top of rotations (4 reflections, which would generate 16 positions, although some of them would be the same, like rotating 180º and reflecting vertically, which is the same as the original reflected horizontally).\n",
    "\n",
    "The reduction in the number of possible states would end up reducing the memory needed to store the values of all states. Furthermore, when one value is changed in a state, it updates all positions of the board that correspond to that state, making learning faster. Even if the opponent did not take advantage of symmetries, using them would still be useful due to the reasons said previously. \n",
    "\n",
    "Although the final values of using symmetrical equivalent positions may not end up being exactly the same as when not using them, because the changes in value depend on how the opponent makes their moves (but, after a suficiently large number of plays, the difference will generally be small), it would still find the optimal actions for playing against that opponent (if the opponent also plays with optimal actions, the games will end in a draw, which would be the best possible scenario in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: *Greedy Play* \n",
    "\n",
    "**Q**\n",
    "\n",
    "Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur?\n",
    "\n",
    "**A**\n",
    "\n",
    "In the specific proposed scenario, the greedy player will play better than the nongreedy player after a sufficiently huge number of plays.\n",
    "\n",
    "The scenario is that winning gives a reward of 1, losing gives a reward of 0, a draw gives a reward of 0.5, and the initial values are 0.5. Assuming $0 \\lt \\alpha \\lt 1$, the updates make the state go toward 1 when winning and toward 0 when losing, but never actually reaching 0 or 1. This means that every time the agent wins, it will increase the values of the chosen positions, because those were already the best actions, or one of the best actions, according to the actual values (which will make them more likely to be chosen, possibly removing the possibility of choosing actions with the same value), and when losing it will decrease the values of those states, making the agent avoid them (although it may need to lose more times to decrease the value enough for that).\n",
    "\n",
    "The tic-tac-toe game has specific states that are necessarily better than the others, and the result of the actions are deterministic (they can be considered afterstates), so, after a huge number of games, the greedy player will end up favoring these states, because although it does not explore directly, it will end up exploring indirectly because it's playing against a nongreedy player that over time will choose different actions and make the greedy one lose when it does not have a good enough policy. \n",
    "\n",
    "This would not be the case if the greedy agent played with a deterministic agent, even if the actions of the other agent were suboptimal, because if the game ends in a draw, the values may not end up being updated (all of them starts with the same value 0.5), and considering that the greedy agent is deterministic (for example, always choosing the 1st action when more than 1 action has the same value), as a result, the greedy agent may end up always drawing against a suboptimal deterministic agent, even if it could always win. The question is not about playing against a deterministic agent tough, but against a nongreedy player.\n",
    "\n",
    "A possible problem is taking too long to choose good actions. For example, in a play in which it wins because the nongreedy player has chosen bad moves in scenarios that it (the nongreedy player) could play much better, may end up making the greedy player give high values to bad states, that may end up taking too long to reduce the values over time (they should have low value, because those were overall bad states)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4: *Learning from Exploration*\n",
    "\n",
    "**Q**\n",
    "\n",
    "Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n",
    "\n",
    "**A**\n",
    "\n",
    "When we do not learn from exploratory moves, the set of probabilities will converge to the optimal policy, because it will only consider the actions that will lead to the states with the highest values (although it might differ a bit because the path followed by the agent will be affected by the exploration).\n",
    "\n",
    "When we learn from exploratory moves, the set of probabilities will converge to the $\\epsilon$-greedy policy, because it will learn exactly the values of the states by follwing such a policy.\n",
    "\n",
    "Assuming that we do continue to make exploratory moves, the best set to learn is the set that does not include exploratory moves, but only if after learning there will be no exploration, or it decreases to 0 over time (otherwise, see the next paragraph), because it will focus on the best actions (but still exploring), possibly making the training converge faster, by not giving much importance to random explorations (which could decrease the values of actually good states, because some action taken from it is very bad, even if there's an action that will always be very good).\n",
    "\n",
    "The one that will result in more wins, even after the training, assuming that we do continue to make exploratory moves, is the one that includes exploratory moves, because it will give more precise values for that scenario (and will prefer the better values, but will still explore). One example is the deterministic CliffWalking environment, in which the optimal policy is walking very close to the cliff, but if there's exploration, the agent should go a bit more distant because one random action leading to the cliff would be very bad, so learning the exploratory moves would actually be better in this case, even if it differs from the true optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5: *Other Improvements*\n",
    "\n",
    "**Q**\n",
    "\n",
    "Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?\n",
    "\n",
    "**A**\n",
    "\n",
    "Some things I can think of:\n",
    "\n",
    "1. Decrease the degree of exploration over time (to converge into the optimal policy).\n",
    "\n",
    "2. Decrease $\\alpha$ over time (to stabilize the values and decrease variance).\n",
    "\n",
    "3. Apply UCB, to still explore states not explored, or explored too long ago (this may not be so good for the tic-tac-toe environment because it has just few states and the environment itself does not change over time).\n",
    "\n",
    "4. Run many pairs of agents in parallel with different seeds (to reduce variance), possibly with different policies and value functions (to avoid bias), making them play against one another, then improve their policies at each iteration, and at the end generate a new value function based on the mean of the value functions of each parallel agent with more wins (possibly training it a bit more to fix any problems caused by using the mean of different value functions). This approach may be overly complex for tic-tac-toe, tough.\n",
    "\n",
    "5. Apply different starting values for the states. Assuming that winning gives a reward of 1, losing gives a reward of 0 and a draw gives a reward of 0.5, a good initial value for all states is 0.5"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
