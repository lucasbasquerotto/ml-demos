{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Example 4.1, if π is the equiprobable random policy, what is $q_π(11, down)$? What is $q_π(7, down)$?\n",
    "\n",
    "**A**\n",
    "\n",
    "The equiprobable random policy means equal chances of any of the 4 actions/directions being chosen:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi(a|s) = \\frac{1}{4} = 0.25\n",
    "\\end{align*}\n",
    "\n",
    "The equation for $q_{\\pi}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{.}{=} \\mathbb{E}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The value of $R_k = -1$ for any k (for any action the reward is -1). \n",
    "\n",
    "Considering $s_T$ the terminal state, if $S_t=s=11$ and $A_t=a=down$, then $S_{t+1}=s_T$ (based on the example, because below the state 11 is the terminal state).\n",
    "\n",
    "This is an undiscounted episodic task, so $\\gamma = 1$.\n",
    "\n",
    "Solving for $q_{\\pi}(11, down)$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(11, down) &= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=11, A_t=down] \\\\\n",
    "&= \\mathbb{E}[-1 + 1 \\cdot v_{\\pi}(s_T) | S_t=11, A_t=down] \\\\\n",
    "&= \\mathbb{E}[-1 + 1 \\cdot 0 | S_t=11, A_t=down] \\\\\n",
    "&= -1\n",
    "\\end{align*}\n",
    "\n",
    "Similarly, if $S_t=7$ and $A_t=down$, then $S_{t+1}=11$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(7, down) &= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=7, A_t=down] \\\\\n",
    "&= \\mathbb{E}[-1 + 1 \\cdot v_{\\pi}(11) | S_t=7, A_t=down] \\\\\n",
    "&= -1 + v_{\\pi}(11)\n",
    "\\end{align*}\n",
    "\n",
    "The example declared:\n",
    "\n",
    "> The final estimate is in fact $v_{\\pi}$, which in this case gives for each state the negation of the expected number of steps from that state until termination.\n",
    "\n",
    "Considering the above statement, and according to Figure 4.1, that shows the values of $v_{\\pi}$ for each state, we have $v_{\\pi}(11) = -14$.\n",
    "\n",
    "*Note:* It's important to take into account that the policy follows the equiprobable random policy, which means that even if you are in state $S_t=11$, it should not be considered that the expected number of steps to reach the terminal state is 1 (it's not the optimal policy).\n",
    "\n",
    "Solving for $q_{\\pi}(7, down)$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(7, down) = -1 + v_{\\pi}(11) = -1 + (-14) = -15\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is $v_π(15)$ for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is $v_π(15)$ for the equiprobable random policy in this case?\n",
    "\n",
    "**A**\n",
    "\n",
    "If the transitions from the original states are unchanged, this means that their state-values remain the same (as defined in Figure 4.1).\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(15) &= R_{s=15} + \\gamma [\\pi(left | 15) \\cdot v_{\\pi}(12) + \\pi(up | 15) \\cdot v_{\\pi}(13) + \\pi(right | 15) \\cdot v_{\\pi}(14) + \\pi(down | 15) \\cdot v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(15) &= -1 + 1 \\cdot [0.25 \\cdot v_{\\pi}(12) + 0.25 \\cdot v_{\\pi}(13) + 0.25 \\cdot v_{\\pi}(14) + 0.25 \\cdot v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(15) &= -1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14)) + 0.25 \\cdot v_{\\pi}(15) \\\\\n",
    "v_{\\pi}(15) - 0.25 \\cdot v_{\\pi}(15) &= -1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14)) \\\\\n",
    "0.75 \\cdot v_{\\pi}(15) &= -1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14)) \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14))}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + 0.25 \\cdot (-22 + -20 + -14)}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + 0.25 \\cdot -56}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + -14}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-15}{0.75} \\\\\n",
    "v_{\\pi}(15) &= -15 \\cdot \\frac{4}{3} \\\\\n",
    "v_{\\pi}(15) &= -5 \\cdot 4 \\\\\n",
    "v_{\\pi}(15) &= -20 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In the case of the action down from state 13 taking the agent to the new state 15, the state-values will remain the same, both for state 13 as well as state 15, because this is a specific case in which both state values had the same values before the change: -20. Going down from state 13 before ended up in the same state 13 with value -20. Now it will take to state 15 with value -20, and because of this, it will remain unchanged due to the simmetry (going in an iterative way, the difference would be 0 at the first step because the state-values would already be correct). \n",
    "\n",
    "So, $v_{\\pi}(13) = v_{\\pi}(15) = -20$ remain unchanged."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
