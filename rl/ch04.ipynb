{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 04 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Example 4.1, if π is the equiprobable random policy, what is $q_π(11, down)$? What is $q_π(7, down)$?\n",
    "\n",
    "**A**\n",
    "\n",
    "The equiprobable random policy means equal chances of any of the 4 actions/directions being chosen:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi(a|s) = \\frac{1}{4} = 0.25\n",
    "\\end{align*}\n",
    "\n",
    "The equation for $q_{\\pi}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{.}{=} \\mathbb{E}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The value of $R_k = -1$ for any k (for any action the reward is -1). \n",
    "\n",
    "Considering $s_T$ the terminal state, if $S_t=s=11$ and $A_t=a=down$, then $S_{t+1}=s_T$ (based on the example, because below the state 11 is the terminal state).\n",
    "\n",
    "This is an undiscounted episodic task, so $\\gamma = 1$.\n",
    "\n",
    "Solving for $q_{\\pi}(11, down)$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(11, down) &= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=11, A_t=down] \\\\\n",
    "&= \\mathbb{E}[-1 + 1 \\cdot v_{\\pi}(s_T) | S_t=11, A_t=down] \\\\\n",
    "&= \\mathbb{E}[-1 + 1 \\cdot 0 | S_t=11, A_t=down] \\\\\n",
    "&= -1\n",
    "\\end{align*}\n",
    "\n",
    "Similarly, if $S_t=7$ and $A_t=down$, then $S_{t+1}=11$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(7, down) &= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=7, A_t=down] \\\\\n",
    "&= \\mathbb{E}[-1 + 1 \\cdot v_{\\pi}(11) | S_t=7, A_t=down] \\\\\n",
    "&= -1 + v_{\\pi}(11)\n",
    "\\end{align*}\n",
    "\n",
    "The example declared:\n",
    "\n",
    "> The final estimate is in fact $v_{\\pi}$, which in this case gives for each state the negation of the expected number of steps from that state until termination.\n",
    "\n",
    "Considering the above statement, and according to Figure 4.1, that shows the values of $v_{\\pi}$ for each state, we have $v_{\\pi}(11) = -14$.\n",
    "\n",
    "*Note:* It's important to take into account that the policy follows the equiprobable random policy, which means that even if you are in state $S_t=11$, it should not be considered that the expected number of steps to reach the terminal state is 1 (it's not the optimal policy).\n",
    "\n",
    "Solving for $q_{\\pi}(7, down)$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(7, down) = -1 + v_{\\pi}(11) = -1 + (-14) = -15\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is $v_π(15)$ for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is $v_π(15)$ for the equiprobable random policy in this case?\n",
    "\n",
    "**A**\n",
    "\n",
    "If the transitions from the original states are unchanged, this means that their state-values remain the same (as defined in Figure 4.1).\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(15) &= R_{s=15} + \\gamma [\\pi(left | 15) \\cdot v_{\\pi}(12) + \\pi(up | 15) \\cdot v_{\\pi}(13) + \\pi(right | 15) \\cdot v_{\\pi}(14) + \\pi(down | 15) \\cdot v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(15) &= -1 + 1 \\cdot [0.25 \\cdot v_{\\pi}(12) + 0.25 \\cdot v_{\\pi}(13) + 0.25 \\cdot v_{\\pi}(14) + 0.25 \\cdot v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(15) &= -1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14)) + 0.25 \\cdot v_{\\pi}(15) \\\\\n",
    "v_{\\pi}(15) - 0.25 \\cdot v_{\\pi}(15) &= -1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14)) \\\\\n",
    "0.75 \\cdot v_{\\pi}(15) &= -1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14)) \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14))}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + 0.25 \\cdot (-22 + -20 + -14)}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + 0.25 \\cdot -56}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + -14}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-15}{0.75} \\\\\n",
    "v_{\\pi}(15) &= -15 \\cdot \\frac{4}{3} \\\\\n",
    "v_{\\pi}(15) &= -5 \\cdot 4 \\\\\n",
    "v_{\\pi}(15) &= -20 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In the case of the action down from state 13 taking the agent to the new state 15, the state-values will remain the same, both for state 13 as well as state 15, because this is a specific case in which both state values had the same values before the change: -20. Going down from state 13 before ended up in the same state 13 with value -20. Now it will take to state 15 with value -20, and because of this, it will remain unchanged due to the simmetry (going in an iterative way, the difference would be 0 at the first step because the state-values would already be correct). \n",
    "\n",
    "So, $v_{\\pi}(13) = v_{\\pi}(15) = -20$ remain unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3\n",
    "\n",
    "**Q**\n",
    "\n",
    "What are the equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_π$ and its successive approximations by a sequence of functions q0, q1, q2, ...?\n",
    "\n",
    "**A**\n",
    "\n",
    "The equations for $v_{\\pi}$ are:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{.}{=} \\mathbb{E}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s] \\tag{4.3} \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\pi}(s')] \\tag{4.4}\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{k+1}(s) &\\stackrel{.}{=} \\mathbb{E}[R_{t+1} + \\gamma v_k (S_{t+1}) | S_t=s] \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_k (s')] \\tag{4.5}\n",
    "\\end{align*}\n",
    "\n",
    "Doing the same for $q_{\\pi}$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{.}{=} \\mathbb{E}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t=s, A_t=a] \\tag{equivalent to 4.3} \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma \\sum_{a'} \\pi(a'|s') q_{\\pi}(s', a')\\right] \\tag{equivalent to 4.4}\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{k+1}(s, a) &\\stackrel{.}{=} \\mathbb{E}[R_{t+1} + \\gamma q_k (S_{t+1}, A_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma \\sum_{a'} \\pi(a'|s') q_k (s', a')\\right] \\tag{equivalent to 4.5}\n",
    "\\end{align*}\n",
    "\n",
    "**Iterative Policy Evaluation, for estimating $Q \\approx q_{\\pi}$**\n",
    "\n",
    "> Input $\\pi$, the policy to be evaluated\n",
    ">\n",
    "> Algorithm parameter: a small threshold $\\theta > 0$ determining accuracy of estimation\n",
    ">\n",
    "> Initialize $Q(s, a)$, for all $s \\in \\mathcal{S^+}$ and $a \\in \\mathcal{A(s)}$, arbitrarily except that $Q(terminal, a)=0$\n",
    ">\n",
    "> Loop:\n",
    ">\n",
    "> ....$\\Delta \\gets 0$\n",
    ">\n",
    "> ....Loop for each $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A(s)}$:\n",
    ">\n",
    "> ........$q \\gets Q(s, a)$\n",
    ">\n",
    "> ........$Q(s, a) \\gets \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma \\sum_{a'} \\pi(a'|s') Q(s', a')\\right] $\n",
    ">\n",
    "> ........$\\Delta \\gets max(\\Delta, |q - Q(s, a)|)$\n",
    ">\n",
    "> until $\\Delta < \\theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.4\n",
    "\n",
    "**Q**\n",
    "\n",
    "The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is okay for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed.\n",
    "\n",
    "**A**\n",
    "\n",
    "The policy iteration was defined as follows:\n",
    "\n",
    "**Policy Iteration (using iterative policy evaluation) for estimating $\\pi \\approx \\pi_*$**\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "$V(s) \\in \\mathbb{R}$ and $\\pi(s) \\in \\mathcal{A}(s)$ arbitrarily for all $s \\in \\mathcal{S}$\n",
    "\n",
    "2. Policy Evaluation\n",
    "\n",
    "> Loop:\n",
    ">\n",
    "> ....$\\Delta \\gets 0$\n",
    ">\n",
    "> ....Loop for each $s \\in \\mathcal{S}$:\n",
    ">\n",
    "> ........$v \\gets V(s)$\n",
    ">\n",
    "> ........$V(s) \\gets \\sum_{s', r} p(s', r | s, \\pi(s))\\left[r + \\gamma V(s')\\right]$\n",
    ">\n",
    "> ........$\\Delta \\gets max(\\Delta, |v - V(s)|)$\n",
    ">\n",
    "> until $\\Delta < \\theta$ (a small positive number determining the accuracy of estimation)\n",
    "\n",
    "3. Policy Improvement\n",
    "\n",
    "> Loop:\n",
    ">\n",
    "> ....*policy-stable* $\\gets$ *true*\n",
    ">\n",
    "> ....For each $s \\in \\mathcal{S}$:\n",
    ">\n",
    "> ........*old-action* $\\gets \\pi(s)$\n",
    ">\n",
    "> ........$\\pi(s) \\gets \\operatorname*{argmax}_a \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma V(s')\\right]$\n",
    ">\n",
    "> ........If *old-action* $\\neq \\pi(s)$, then *policy-stable* $\\gets$ *false*\n",
    ">\n",
    "> If *policy-stable*, then stop and return $V \\approx v_*$, and $\\pi \\approx \\pi_*$; else go to 2\n",
    "\n",
    "**Actual solution**\n",
    "\n",
    "The Policy Improvement step (3) should not consider the policy as not stable when the argmax gives a different action, but the action value of using that action is the same as the old action. Different actions may have the same value, and in this case the policy does not need to be changed (and changing it can cause an infinite loop in which it changes from one to the other policy, and vice-versa, without stop).\n",
    "\n",
    "3. (New) Policy Improvement\n",
    "\n",
    "> Loop:\n",
    ">\n",
    "> ....*policy-stable* $\\gets$ *true*\n",
    ">\n",
    "> ....For each $s \\in \\mathcal{S}$:\n",
    ">\n",
    "> ........*old-action* $\\gets \\pi(s)$\n",
    ">\n",
    "> ........*tmp-action* $\\gets \\operatorname*{argmax}_a \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma V(s')\\right]$\n",
    ">\n",
    "> ........*old-value* $\\gets \\sum_{s', r} p(s', r | s,$ *old-action*$)\\left[r + \\gamma V(s')\\right]$\n",
    ">\n",
    "> ........*tmp-value* $\\gets \\sum_{s', r} p(s', r | s,$ *tmp-action*$)\\left[r + \\gamma V(s')\\right]$\n",
    ">\n",
    "> ........if *old-value* < *tmp-value*:\n",
    ">\n",
    "> ............$\\pi(s) \\gets$ *tmp-action*\n",
    ">\n",
    "> ........If *old-action* $\\neq \\pi(s)$, then *policy-stable* $\\gets$ *false*\n",
    ">\n",
    "> If *policy-stable*, then stop and return $V \\approx v_*$, and $\\pi \\approx \\pi_*$; else go to 2\n",
    "\n",
    "Another way to solve it is to make argmax always return the first action (assuming the actions are always iterated in the same order to get the result of argmax)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
