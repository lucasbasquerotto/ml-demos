{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 04 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Example 4.1, if π is the equiprobable random policy, what is $q_π(11, down)$? What is $q_π(7, down)$?\n",
    "\n",
    "**A**\n",
    "\n",
    "The equiprobable random policy means equal chances of any of the 4 actions/directions being chosen:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi(a|s) = \\frac{1}{4} = 0.25\n",
    "\\end{align*}\n",
    "\n",
    "The equation for $q_{\\pi}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{.}{=} \\mathbb{E}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The value of $R_k = -1$ for any k (for any action the reward is -1). \n",
    "\n",
    "Considering $s_T$ the terminal state, if $S_t=s=11$ and $A_t=a=down$, then $S_{t+1}=s_T$ (based on the example, because below the state 11 is the terminal state).\n",
    "\n",
    "This is an undiscounted episodic task, so $\\gamma = 1$.\n",
    "\n",
    "Solving for $q_{\\pi}(11, down)$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(11, down) &= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=11, A_t=down] \\\\\n",
    "&= \\mathbb{E}[-1 + 1 \\cdot v_{\\pi}(s_T) | S_t=11, A_t=down] \\\\\n",
    "&= \\mathbb{E}[-1 + 1 \\cdot 0 | S_t=11, A_t=down] \\\\\n",
    "&= -1\n",
    "\\end{align*}\n",
    "\n",
    "Similarly, if $S_t=7$ and $A_t=down$, then $S_{t+1}=11$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(7, down) &= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=7, A_t=down] \\\\\n",
    "&= \\mathbb{E}[-1 + 1 \\cdot v_{\\pi}(11) | S_t=7, A_t=down] \\\\\n",
    "&= -1 + v_{\\pi}(11)\n",
    "\\end{align*}\n",
    "\n",
    "The example declared:\n",
    "\n",
    "> The final estimate is in fact $v_{\\pi}$, which in this case gives for each state the negation of the expected number of steps from that state until termination.\n",
    "\n",
    "Considering the above statement, and according to Figure 4.1, that shows the values of $v_{\\pi}$ for each state, we have $v_{\\pi}(11) = -14$.\n",
    "\n",
    "*Note:* It's important to take into account that the policy follows the equiprobable random policy, which means that even if you are in state $S_t=11$, it should not be considered that the expected number of steps to reach the terminal state is 1 (it's not the optimal policy).\n",
    "\n",
    "Solving for $q_{\\pi}(7, down)$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(7, down) = -1 + v_{\\pi}(11) = -1 + (-14) = -15\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is $v_π(15)$ for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is $v_π(15)$ for the equiprobable random policy in this case?\n",
    "\n",
    "**A**\n",
    "\n",
    "If the transitions from the original states are unchanged, this means that their state-values remain the same (as defined in Figure 4.1).\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(15) &= R_{s=15} + \\gamma [\\pi(left | 15) \\cdot v_{\\pi}(12) + \\pi(up | 15) \\cdot v_{\\pi}(13) + \\pi(right | 15) \\cdot v_{\\pi}(14) + \\pi(down | 15) \\cdot v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(15) &= -1 + 1 \\cdot [0.25 \\cdot v_{\\pi}(12) + 0.25 \\cdot v_{\\pi}(13) + 0.25 \\cdot v_{\\pi}(14) + 0.25 \\cdot v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(15) &= -1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14)) + 0.25 \\cdot v_{\\pi}(15) \\\\\n",
    "v_{\\pi}(15) - 0.25 \\cdot v_{\\pi}(15) &= -1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14)) \\\\\n",
    "0.75 \\cdot v_{\\pi}(15) &= -1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14)) \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + 0.25 \\cdot (v_{\\pi}(12) + v_{\\pi}(13) + \\cdot v_{\\pi}(14))}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + 0.25 \\cdot (-22 + -20 + -14)}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + 0.25 \\cdot -56}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-1 + -14}{0.75} \\\\\n",
    "v_{\\pi}(15) &= \\frac{-15}{0.75} \\\\\n",
    "v_{\\pi}(15) &= -15 \\cdot \\frac{4}{3} \\\\\n",
    "v_{\\pi}(15) &= -5 \\cdot 4 \\\\\n",
    "v_{\\pi}(15) &= -20 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In the case of the action down from state 13 taking the agent to the new state 15, the state-values will remain the same, both for state 13 as well as state 15, because this is a specific case in which both state values had the same values before the change: -20. Going down from state 13 before ended up in the same state 13 with value -20. Now it will take to state 15 with value -20, and because of this, it will remain unchanged due to the simmetry (going in an iterative way, the difference would be 0 at the first step because the state-values would already be correct). \n",
    "\n",
    "So, $v_{\\pi}(13) = v_{\\pi}(15) = -20$ remain unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3\n",
    "\n",
    "**Q**\n",
    "\n",
    "What are the equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_π$ and its successive approximations by a sequence of functions q0, q1, q2, ...?\n",
    "\n",
    "**A**\n",
    "\n",
    "The equations for $v_{\\pi}$ are:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{.}{=} \\mathbb{E}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s] \\tag{4.3} \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_{\\pi}(s')] \\tag{4.4}\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{k+1}(s) &\\stackrel{.}{=} \\mathbb{E}[R_{t+1} + \\gamma v_k (S_{t+1}) | S_t=s] \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_k (s')] \\tag{4.5}\n",
    "\\end{align*}\n",
    "\n",
    "Doing the same for $q_{\\pi}$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{.}{=} \\mathbb{E}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t=s, A_t=a] \\tag{equivalent to 4.3} \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma \\sum_{a'} \\pi(a'|s') q_{\\pi}(s', a')\\right] \\tag{equivalent to 4.4}\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{k+1}(s, a) &\\stackrel{.}{=} \\mathbb{E}[R_{t+1} + \\gamma q_k (S_{t+1}, A_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma \\sum_{a'} \\pi(a'|s') q_k (s', a')\\right] \\tag{equivalent to 4.5}\n",
    "\\end{align*}\n",
    "\n",
    "**Iterative Policy Evaluation, for estimating $Q \\approx q_{\\pi}$**\n",
    "\n",
    "> Input $\\pi$, the policy to be evaluated\n",
    ">\n",
    "> Algorithm parameter: a small threshold $\\theta > 0$ determining accuracy of estimation\n",
    ">\n",
    "> Initialize $Q(s, a)$, for all $s \\in \\mathcal{S^+}$ and $a \\in \\mathcal{A(s)}$, arbitrarily except that $Q(terminal, a)=0$\n",
    ">\n",
    "> Loop:\n",
    ">\n",
    "> ....$\\Delta \\gets 0$\n",
    ">\n",
    "> ....Loop for each $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A(s)}$:\n",
    ">\n",
    "> ........$q \\gets Q(s, a)$\n",
    ">\n",
    "> ........$Q(s, a) \\gets \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma \\sum_{a'} \\pi(a'|s') Q(s', a')\\right] $\n",
    ">\n",
    "> ........$\\Delta \\gets max(\\Delta, |q - Q(s, a)|)$\n",
    ">\n",
    "> until $\\Delta < \\theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.4\n",
    "\n",
    "**Q**\n",
    "\n",
    "The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is okay for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed.\n",
    "\n",
    "**A**\n",
    "\n",
    "The policy iteration was defined as follows:\n",
    "\n",
    "**Policy Iteration (using iterative policy evaluation) for estimating $\\pi \\approx \\pi_*$**\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "$V(s) \\in \\mathbb{R}$ and $\\pi(s) \\in \\mathcal{A}(s)$ arbitrarily for all $s \\in \\mathcal{S}$\n",
    "\n",
    "2. Policy Evaluation\n",
    "\n",
    "> Loop:\n",
    ">\n",
    "> ....$\\Delta \\gets 0$\n",
    ">\n",
    "> ....Loop for each $s \\in \\mathcal{S}$:\n",
    ">\n",
    "> ........$v \\gets V(s)$\n",
    ">\n",
    "> ........$V(s) \\gets \\sum_{s', r} p(s', r | s, \\pi(s))\\left[r + \\gamma V(s')\\right]$\n",
    ">\n",
    "> ........$\\Delta \\gets max(\\Delta, |v - V(s)|)$\n",
    ">\n",
    "> until $\\Delta < \\theta$ (a small positive number determining the accuracy of estimation)\n",
    "\n",
    "3. Policy Improvement\n",
    "\n",
    "> Loop:\n",
    ">\n",
    "> ....*policy-stable* $\\gets$ *true*\n",
    ">\n",
    "> ....For each $s \\in \\mathcal{S}$:\n",
    ">\n",
    "> ........*old-action* $\\gets \\pi(s)$\n",
    ">\n",
    "> ........$\\pi(s) \\gets \\operatorname*{argmax}_a \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma V(s')\\right]$\n",
    ">\n",
    "> ........If *old-action* $\\neq \\pi(s)$, then *policy-stable* $\\gets$ *false*\n",
    ">\n",
    "> If *policy-stable*, then stop and return $V \\approx v_*$, and $\\pi \\approx \\pi_*$; else go to 2\n",
    "\n",
    "**Actual solution**\n",
    "\n",
    "The Policy Improvement step (3) should not consider the policy as not stable when the argmax gives a different action, but the action value of using that action is the same as the old action. Different actions may have the same value, and in this case the policy does not need to be changed (and changing it can cause an infinite loop in which it changes from one to the other policy, and vice-versa, without stop).\n",
    "\n",
    "3. (New) Policy Improvement\n",
    "\n",
    "> Loop:\n",
    ">\n",
    "> ....*policy-stable* $\\gets$ *true*\n",
    ">\n",
    "> ....For each $s \\in \\mathcal{S}$:\n",
    ">\n",
    "> ........*old-action* $\\gets \\pi(s)$\n",
    ">\n",
    "> ........*tmp-action* $\\gets \\operatorname*{argmax}_a \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma V(s')\\right]$\n",
    ">\n",
    "> ........*old-value* $\\gets \\sum_{s', r} p(s', r | s,$ *old-action*$)\\left[r + \\gamma V(s')\\right]$\n",
    ">\n",
    "> ........*tmp-value* $\\gets \\sum_{s', r} p(s', r | s,$ *tmp-action*$)\\left[r + \\gamma V(s')\\right]$\n",
    ">\n",
    "> ........if *old-value* < *tmp-value*:\n",
    ">\n",
    "> ............$\\pi(s) \\gets$ *tmp-action*\n",
    ">\n",
    "> ........If *old-action* $\\neq \\pi(s)$, then *policy-stable* $\\gets$ *false*\n",
    ">\n",
    "> If *policy-stable*, then stop and return $V \\approx v_*$, and $\\pi \\approx \\pi_*$; else go to 2\n",
    "\n",
    "Another way to solve it is to make argmax always return the first action (assuming the actions are always iterated in the same order to get the result of argmax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.5\n",
    "\n",
    "**Q**\n",
    "\n",
    "How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $v_*$. Please pay\n",
    "special attention to this exercise, because the ideas involved will be used throughout the rest of the book.\n",
    "\n",
    "**A**\n",
    "\n",
    "The policy iteration for action values is defined as follows:\n",
    "\n",
    "**Policy Iteration (using iterative policy evaluation) for estimating $\\pi \\approx \\pi_*$**\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "$Q(s, a) \\in \\mathbb{R}$ and $\\pi(s) \\in \\mathcal{A}(s)$ arbitrarily for all $s \\in \\mathcal{S}$ and all $a \\in \\mathcal{A}(s)$\n",
    "\n",
    "2. Policy Evaluation\n",
    "\n",
    "> Loop:\n",
    ">\n",
    "> ....$\\Delta \\gets 0$\n",
    ">\n",
    "> ....Loop for each $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}(s)$:\n",
    ">\n",
    "> ........$q \\gets Q(s, a)$\n",
    ">\n",
    "> ........$Q(s, a) \\gets \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma Q(s', \\pi(s'))\\right]$\n",
    ">\n",
    "> ........$\\Delta \\gets max(\\Delta, |q - Q(s, a)|)$\n",
    ">\n",
    "> until $\\Delta < \\theta$ (a small positive number determining the accuracy of estimation)\n",
    "\n",
    "3. Policy Improvement\n",
    "\n",
    "> Loop:\n",
    ">\n",
    "> ....*policy-stable* $\\gets$ *true*\n",
    ">\n",
    "> ....For each $s \\in \\mathcal{S}$:\n",
    ">\n",
    "> ........*old-action* $\\gets \\pi(s)$\n",
    ">\n",
    "> ........$\\pi(s) \\gets \\operatorname*{argmax}_a \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma Q(s', \\pi(s'))\\right]$\n",
    ">\n",
    "> ........If *old-action* $\\neq \\pi(s)$, then *policy-stable* $\\gets$ *false*\n",
    ">\n",
    "> If *policy-stable*, then stop and return $Q \\approx q_*$, and $\\pi \\approx \\pi_*$; else go to 2\n",
    "\n",
    "*Note:* This iteration has the same issue of possibility of infinite loop due to 2 different policies having the same values, as described in the previous exercise, and it can be fixed in the same way (either using a temporary variable to store the new action, and updating the policy only if the value of the new action is better than the old action, or using always the first action of argmax over an ordered list of possible actions in the state s). To keep it the most similar with the Policy Iteration for state values, these changes were not made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.6\n",
    "\n",
    "**Q**\n",
    "\n",
    "Suppose you are restricted to considering only policies that are $\\epsilon$-soft, meaning that the probability of selecting each action in each state, s, is at least $\\frac{\\epsilon}{|\\mathcal{A}(s)|}$. Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm for $v_*$ on page 80.\n",
    "\n",
    "**A**\n",
    "\n",
    "We need $0 \\le \\epsilon \\lt 1$, because the sum of the probabilities for all actions will be more or equal to $\\epsilon$, and a value of 1 would force the policy to be an equal distributed random policy.\n",
    "\n",
    "Or explained in another way, $\\epsilon$ must not be less than 0, because a negative probability would make no sense, and it should not be more than 1, otherwise the sum of probabilities would be more than 1. It also must not be 1, otherwise the policy could not be changed by the policy iteration, because all actions would need to have equal probability $\\left(\\frac{\\epsilon}{|\\mathcal{A}(s)|}\\right)$.\n",
    "\n",
    "The case for $\\epsilon = 0$ is actually the case described in the policy iteration algorithm, in which it tries to always choose the best action with probability 1 (deterministically).\n",
    "\n",
    "For the general case of $0 \\le \\epsilon \\lt 1$ we need to change:\n",
    "\n",
    "- Step 3: $\\pi$ should not receive the best action, but maximize its probability, definining the probability of the action a (that maximizes the return at the right of argmax) as $1 - \\epsilon \\cdot \\frac{|\\mathcal{A}(s)| - 1}{|\\mathcal{A}(s)|}$. All other (non-maximizing) actions should receive the least possible probability, that is, $\\frac{\\epsilon}{|\\mathcal{A}(s)|}$. Observe that the probability of the best action will be higher than all the others, because $1 - \\epsilon \\cdot \\frac{|\\mathcal{A}(s)| - 1}{|\\mathcal{A}(s)|} = \\frac{|\\mathcal{A}(s)| - \\epsilon |\\mathcal{A}(s)| + \\epsilon}{|\\mathcal{A}(s)|} = \\frac{|\\mathcal{A}(s)| - \\epsilon |\\mathcal{A}(s)|}{|\\mathcal{A}(s)|} + \\frac{\\epsilon}{|\\mathcal{A}(s)|} > \\frac{\\epsilon}{|\\mathcal{A}(s)|}$ because $|\\mathcal{A}(s)| - \\epsilon |\\mathcal{A}(s)| > 0$ when $0 \\le \\epsilon \\lt 1$ (the condition).\n",
    "\n",
    "- Step 2: calling $\\pi$ does not return an action anymore, but a distribution of probabilities over actions, so we would have: $V(s) \\gets \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a)\\left[r + \\gamma V(s')\\right]$\n",
    "\n",
    "- Step 1: $\\pi$ isn't an action anymore, but a distribution of probabilities over actions, so we would have: $V(s) \\in \\mathbb{R}$ and $\\pi(a|s) \\in \\left[\\frac{\\epsilon}{|\\mathcal{A}(s)|}, 1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s)|}\\right]$ arbitrarily, but making sure that $\\sum_a \\pi(a|s) = 1$, for all $s \\in \\mathcal{S}$ and all $a \\in \\mathcal{A}(s)$ (in a programing context, $\\pi$ can be defined as a 2-argument function (or 2-dimension array) $\\pi(s, a)$ that gives the probability of choosing a given s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.7 (programming) \n",
    "\n",
    "**Q**\n",
    "\n",
    "Write a program for policy iteration and re-solve Jack’s car rental problem with the following changes. One of Jack’s employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs $2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of $4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem.\n",
    "\n",
    "**Original**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AdaptedModel(BaseModel):\n",
    "    name: str\n",
    "\n",
    "    @staticmethod\n",
    "    def base_path() -> str:\n",
    "        return f'./data/jack_rental'\n",
    "\n",
    "    def save(self):\n",
    "        base_path = self.base_path()\n",
    "        path = f'{base_path}/{self.name}.json'\n",
    "        tmp_path = f'{base_path}/{self.name}.tmp.json'\n",
    "\n",
    "        if not os.path.exists(base_path):\n",
    "            os.makedirs(base_path)\n",
    "\n",
    "        with open(tmp_path, 'w') as f:\n",
    "            f.write(self.model_dump_json())\n",
    "\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "\n",
    "        os.rename(tmp_path, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, name: str) -> typing.Self | None:\n",
    "        base_path = cls.base_path()\n",
    "        path = f'{base_path}/{name}.json'\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            return None\n",
    "\n",
    "        with open(path, 'r') as f:\n",
    "            data = f.read()\n",
    "\n",
    "        return cls.model_validate_json(data)\n",
    "\n",
    "class JackSCarRentalEnv(AdaptedModel):\n",
    "    price_rent: int\n",
    "    price_per_move: int\n",
    "    l_req_1: int\n",
    "    l_req_2: int\n",
    "    l_ret_1: int\n",
    "    l_ret_2: int\n",
    "    max_stay: int\n",
    "    max_move: int\n",
    "    gamma: float\n",
    "    theta: float\n",
    "\n",
    "    states: list[tuple[int, int]]\n",
    "    actions_per_state: list[list[tuple[int, int]]]\n",
    "    p: list[list[list[tuple[int, list[tuple[int, float]]]]]]\n",
    "\n",
    "class JackSCarRentalAgent(AdaptedModel):\n",
    "    policy_stable: bool\n",
    "    values: list[float]\n",
    "    policy: list[list[float]]\n",
    "    epsilon: float\n",
    "\n",
    "    @classmethod\n",
    "    def initialize(cls, name: str, env: JackSCarRentalEnv, epsilon = 0.0):\n",
    "        deterministic = epsilon == 0.0\n",
    "\n",
    "        states = env.states\n",
    "        actions_per_state = env.actions_per_state\n",
    "\n",
    "        # initialize the state values V(s)\n",
    "        values = [0.0] * len(states)\n",
    "\n",
    "        # initialize the policy pi(a|s)\n",
    "        # initially it is assumed that all actions are equally likely if not deterministic,\n",
    "        # else the first action is chosen initially with probability 1.0\n",
    "        policy = [\n",
    "            [\n",
    "                ((1.0 if i_a == 0 else 0.0) if deterministic else (1.0/len(actions)))\n",
    "                for i_a, _ in enumerate(actions)\n",
    "            ]\n",
    "            for actions in actions_per_state\n",
    "        ]\n",
    "\n",
    "        return cls(\n",
    "            name=name,\n",
    "            policy_stable=False,\n",
    "            values=values,\n",
    "            policy=policy,\n",
    "            epsilon=epsilon)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, name: str, env: JackSCarRentalEnv, epsilon = 0.0):\n",
    "        agent = super().load(name)\n",
    "\n",
    "        if agent is None:\n",
    "            return cls.initialize(name=name, env=env, epsilon=epsilon)\n",
    "\n",
    "        return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def make_env(name: str) -> JackSCarRentalEnv:\n",
    "    saved_env = JackSCarRentalEnv.load(name)\n",
    "\n",
    "    if saved_env:\n",
    "        print('loaded')\n",
    "        return saved_env\n",
    "\n",
    "    import datetime\n",
    "    print(datetime.datetime.now().strftime(\"%H:%M:%S\"), 'start')\n",
    "\n",
    "    price_rent = 10\n",
    "    price_per_move = 2\n",
    "\n",
    "    l_req_1 = 3\n",
    "    l_req_2 = 4\n",
    "    l_ret_1 = 3\n",
    "    l_ret_2 = 2\n",
    "\n",
    "    max_stay = 20\n",
    "    max_move = 5\n",
    "\n",
    "    gamma = 0.9\n",
    "    theta = 1e-4\n",
    "\n",
    "    # [(num_cars_1, num_cars_2), ...]\n",
    "    # num_cars_1: number of cars at location 1\n",
    "    # num_cars_2: number of cars at location 2\n",
    "    states = [(n1, n2) for n1 in range(max_stay + 1) for n2 in range(max_stay + 1)]\n",
    "\n",
    "    # [actions_for_state_1, actions_for_state_2, ...]\n",
    "    # actions_for_state_1 = [(move_cars_1, move_cars_2), ...]\n",
    "    # move_cars_1: move cars from location 1 to location 2\n",
    "    # move_cars_2: move cars from location 2 to location 1\n",
    "    actions_per_state = [\n",
    "        [(m1, m2) for m1 in range(min(max_move, n1) + 1) for m2 in range(min(max_move, n2) + 1)]\n",
    "        for n1, n2 in states\n",
    "    ]\n",
    "\n",
    "    def factorial(n: int):\n",
    "        result = 1\n",
    "        while n > 1:\n",
    "            result *= n\n",
    "            n -= 1\n",
    "        return result\n",
    "\n",
    "    def poisson(lda: int, n: int):\n",
    "        return ((lda**n)/factorial(n))*math.exp(-lda)\n",
    "\n",
    "    # returns an array that represents that at index i, it has all possible number of cars rented and\n",
    "    # probabilities considering i cars at the end of the next day\n",
    "    def next_state_info_probs(new_s: int, l_req: int, l_ret: int) -> list[list[tuple[int, float]]]:\n",
    "        sp_info_probs: list[list[tuple[int, float]]] = [\n",
    "            [] for _ in range(max_stay + 1)\n",
    "        ]\n",
    "\n",
    "        total_prob_req = 0.0\n",
    "        for n_req in range(new_s + 1):\n",
    "            prob_req = (\n",
    "                poisson(lda=l_req, n=n_req)\n",
    "                if n_req < new_s\n",
    "                # If it tries to rent more cars than available,\n",
    "                # the probability is the total (1) minus the sum\n",
    "                # of the previous probabilities (in which it\n",
    "                # tries to rent less cars), because if it tries to\n",
    "                # rent more cars than available, it will actually\n",
    "                # rent all the available cars (but not more than that)\n",
    "                else 1.0 - total_prob_req)\n",
    "            total_prob_req += prob_req\n",
    "            total_prob_ret = 0.0\n",
    "\n",
    "            max_ret = max_stay - (new_s - n_req)\n",
    "            # will return at most max_ret cars (the rest will\n",
    "            # go to the nationwide company)\n",
    "            for n_ret in range(max_ret + 1):\n",
    "                prob_ret = (\n",
    "                    poisson(lda=l_ret, n=n_ret)\n",
    "                    if n_ret < max_ret\n",
    "                    else 1.0 - total_prob_ret)\n",
    "                total_prob_ret += prob_ret\n",
    "\n",
    "                # new state\n",
    "                sp = new_s - n_req + n_ret\n",
    "\n",
    "                # probability of renting n_req cars and returning\n",
    "                # n_ret cars\n",
    "                prob = prob_req * prob_ret\n",
    "                info_probs = sp_info_probs[sp]\n",
    "                info_probs.append((n_req, prob))\n",
    "\n",
    "        return sp_info_probs\n",
    "\n",
    "    # returns an array that has all possible rewards and probabilities, given a list\n",
    "    # in which each item correspond to a location (in this example, there are 2), and\n",
    "    # each item is a list of tuples in which the first element is the number of cars\n",
    "    # rented at that location, and the second element is the probability of that event;\n",
    "    # it subtracts r_move from the reward, because it is what the action spent to move\n",
    "    # the cars previously\n",
    "    def merge_info_probs(locations: list[list[tuple[int, float]]], r_move: int) -> list[tuple[int, float]]:\n",
    "        result_req: list[tuple[int, float]] = []\n",
    "\n",
    "        # if the number of items for the locations are [n1, n2, ..., nm], then result_req\n",
    "        # will have n1 * n2 * ... * nm items, with each item being a tuple in which the\n",
    "        # first element is the sum of the number of cars rented in all locations, and the\n",
    "        # second element is the product of the probabilities of the events (it may end up\n",
    "        # with the sum of cars rented equal for different cases, because each location may\n",
    "        # end up contributing with a different amount of cars rented that has the same sum)\n",
    "        for info_probs in locations:\n",
    "            prev: list[tuple[int, float]] = result_req\n",
    "            result_req = []\n",
    "\n",
    "            for n_req, prob in info_probs:\n",
    "                if not prev:\n",
    "                    result_req.append((n_req, prob))\n",
    "                else:\n",
    "                    for prev_n_req, prev_prob in prev:\n",
    "                        new_n_req = n_req + prev_n_req\n",
    "                        new_prob = prob * prev_prob\n",
    "                        result_req.append((new_n_req, new_prob))\n",
    "\n",
    "        # For each different case, calculate the reward, and generate a new item for\n",
    "        # different reward, summing the probabilities of the same reward\n",
    "        result: list[tuple[int, float]] = []\n",
    "        diff_rewards: list[int] = []\n",
    "\n",
    "        for n_req, prob in result_req:\n",
    "            r_req = n_req * price_rent\n",
    "            r = r_req - r_move\n",
    "\n",
    "            if r in diff_rewards:\n",
    "                idx = diff_rewards.index(r)\n",
    "                _, prev_prob = result[idx]\n",
    "                result[idx] = (r, prev_prob + prob)\n",
    "            else:\n",
    "                diff_rewards.append(r)\n",
    "                result.append((r, prob))\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Returns a list in which each element is a tuple in which the first element is the\n",
    "    # index of the next state and the second element is a list of tuples in which the first\n",
    "    # element is the reward and the second element is the probability of this event\n",
    "    def prob_fn(s: tuple[int, int], a: tuple[int, int]) -> list[tuple[int, list[tuple[int, float]]]]:\n",
    "        # s: number of cars at the end of the day (at 1, at 2)\n",
    "        # a: number of cars moved (from 1 to 2, from 2 to 1)\n",
    "        # sp: number of cars at the end of the next day\n",
    "        s_1, s_2 = s\n",
    "        a_1, a_2 = a\n",
    "        # sp_1, sp_2 = sp\n",
    "\n",
    "        # liquid number of cars moved from 1 to 2\n",
    "        # (negative means that it was moved from 2 to 1)\n",
    "        diff_m = a_1 - a_2\n",
    "\n",
    "        # price paid to move the cars\n",
    "        r_move = abs(diff_m) * price_per_move\n",
    "\n",
    "        # number of cars after the move\n",
    "        new_s_1 = max(s_1 - diff_m, max_stay)\n",
    "        new_s_2 = max(s_2 + diff_m, max_stay)\n",
    "\n",
    "        # at index i, it has all possible number of cars rented and\n",
    "        # probabilities considering i cars at the end of the next day\n",
    "        sp_1_info_probs = next_state_info_probs(\n",
    "            new_s=new_s_1,\n",
    "            l_req=l_req_1,\n",
    "            l_ret=l_ret_1)\n",
    "        sp_2_info_probs = next_state_info_probs(\n",
    "            new_s=new_s_2,\n",
    "            l_req=l_req_2,\n",
    "            l_ret=l_ret_2)\n",
    "\n",
    "        next_info = [\n",
    "            (\n",
    "                i_sp,\n",
    "                merge_info_probs([sp_1_info_probs[sp_1], sp_2_info_probs[sp_2]], r_move=r_move)\n",
    "            )\n",
    "            for i_sp, (sp_1, sp_2) in enumerate(states)\n",
    "        ]\n",
    "\n",
    "        # print('1', s_1, a_1, new_s_1)\n",
    "        # print('2', s_2, a_2, new_s_2)\n",
    "        # print('len1', [len(l) for l in sp_1_info_probs])\n",
    "        # print('len2', [len(l) for l in sp_2_info_probs])\n",
    "        # print('sum1', sum([prob for l in sp_1_info_probs for _, prob in l]))\n",
    "        # print('sum2', sum([prob for l in sp_2_info_probs for _, prob in l]))\n",
    "        # print('next_info', next_info)\n",
    "        # raise Exception('stop')\n",
    "\n",
    "        return next_info\n",
    "\n",
    "    print(datetime.datetime.now().strftime(\"%H:%M:%S\"), 'p')\n",
    "\n",
    "    # the probability of the next state and reward given the current state and action\n",
    "    # p[idx_state][idx_action] = [(idx_next_state, [(reward, probability), ...]), ...]\n",
    "    p: list[list[list[tuple[int, list[tuple[int, float]]]]]] = [\n",
    "        [prob_fn(s, a) for a in actions_per_state[i_s]]\n",
    "        for i_s, s in enumerate(states)\n",
    "    ]\n",
    "\n",
    "    print(datetime.datetime.now().strftime(\"%H:%M:%S\"), 'result')\n",
    "\n",
    "    result = JackSCarRentalEnv(\n",
    "        name=name,\n",
    "        price_rent=price_rent,\n",
    "        price_per_move=price_per_move,\n",
    "        l_req_1=l_req_1,\n",
    "        l_req_2=l_req_2,\n",
    "        l_ret_1=l_ret_1,\n",
    "        l_ret_2=l_ret_2,\n",
    "        max_stay=max_stay,\n",
    "        max_move=max_move,\n",
    "        gamma=gamma,\n",
    "        theta=theta,\n",
    "        states=states,\n",
    "        actions_per_state=actions_per_state,\n",
    "        p=p)\n",
    "\n",
    "    print(datetime.datetime.now().strftime(\"%H:%M:%S\"), 'end')\n",
    "\n",
    "    result.save()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jack’s Car Rental Agent (optimal policy)\n",
    "def optimize_agent(env: JackSCarRentalEnv, agent: JackSCarRentalAgent, debug=False):\n",
    "    if agent.policy_stable:\n",
    "        return agent\n",
    "\n",
    "    import datetime\n",
    "\n",
    "    gamma = env.gamma\n",
    "    theta = env.theta\n",
    "\n",
    "    states = env.states\n",
    "    actions_per_state = env.actions_per_state\n",
    "    p = env.p\n",
    "\n",
    "    def policy_evaluation(outer_counter: int):\n",
    "        values = agent.values\n",
    "        policy = agent.policy\n",
    "\n",
    "        counter = 0\n",
    "        while True:\n",
    "            delta = 0.0\n",
    "\n",
    "            for i_s, _ in enumerate(states):\n",
    "                v = values[i_s]\n",
    "                new_v = 0.0\n",
    "\n",
    "                for i_a, _ in enumerate(actions_per_state[i_s]):\n",
    "                    next_info = p[i_s][i_a]\n",
    "\n",
    "                    for i_sp, info_probs in next_info:\n",
    "                        for r, prob in info_probs:\n",
    "                            new_v += policy[i_s][i_a] * prob * (r + gamma * values[i_sp])\n",
    "\n",
    "                values[i_s] = new_v\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "\n",
    "            if debug:\n",
    "                counter += 1\n",
    "                date = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "                print(f\"{date} [{outer_counter} - {counter}] delta: {delta}\")\n",
    "\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "    def policy_improvement():\n",
    "        policy_stable = agent.policy_stable\n",
    "        values = agent.values\n",
    "        policy = agent.policy\n",
    "        epsilon = agent.epsilon\n",
    "\n",
    "        for i_s, _ in enumerate(states):\n",
    "            old_actions = policy[i_s]\n",
    "            a_amount = len(old_actions)\n",
    "            main_prob = 1 - epsilon + (epsilon/a_amount)\n",
    "            other_prob = epsilon/a_amount\n",
    "\n",
    "            max_old_action_prob = max(old_actions)\n",
    "            old_best_i_a = (\n",
    "                old_actions.index(max_old_action_prob)\n",
    "                if max_old_action_prob >= main_prob\n",
    "                else -1)\n",
    "\n",
    "            best_i_a = -1\n",
    "            best_value = 0\n",
    "\n",
    "            for i_a, _ in enumerate(old_actions):\n",
    "                next_info = p[i_s][i_a]\n",
    "                value = 0\n",
    "\n",
    "                for i_sp, info_probs in next_info:\n",
    "                    for r, prob in info_probs:\n",
    "                        value += prob * (r + gamma * values[i_sp])\n",
    "\n",
    "                # update only when it's greater than the best value, to avoid\n",
    "                # an infinite loop of a policy changing and returning to the\n",
    "                # previous policy\n",
    "                if (best_i_a < 0) or (value > best_value):\n",
    "                    best_i_a = i_a\n",
    "                    best_value = value\n",
    "\n",
    "            new_actions = [\n",
    "                (\n",
    "                    main_prob\n",
    "                    if i_a == best_i_a\n",
    "                    else other_prob\n",
    "                )\n",
    "                for i_a in range(a_amount)\n",
    "            ]\n",
    "            policy[i_s] = new_actions\n",
    "\n",
    "            if old_best_i_a != best_i_a:\n",
    "                policy_stable = False\n",
    "\n",
    "        return policy_stable\n",
    "\n",
    "    def run():\n",
    "        if debug:\n",
    "            date = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print('=' * 80)\n",
    "            print(f'Started at {date}')\n",
    "            print('-' * 80)\n",
    "            max_p = 0\n",
    "            counter = 0\n",
    "            for i_ps, ps in enumerate(p):\n",
    "                for i_pa, pa in enumerate(ps):\n",
    "                    for pspr in pa:\n",
    "                        psp, rs = pspr\n",
    "                        p_len = len(rs)\n",
    "                        counter += p_len or 1\n",
    "\n",
    "                        if p_len > max_p:\n",
    "                            print('s, a, sp', i_ps, i_pa, psp)\n",
    "                            max_p = p_len\n",
    "            print('counter', counter)\n",
    "            print('max amount of rewards for some next action', max_p)\n",
    "            print('-' * 80)\n",
    "\n",
    "        outer_counter = 0\n",
    "\n",
    "        policy_stable = False\n",
    "\n",
    "        while not policy_stable:\n",
    "            outer_counter += 1\n",
    "            policy_evaluation(outer_counter=outer_counter)\n",
    "            policy_stable = policy_improvement()\n",
    "            agent.save()\n",
    "\n",
    "        if debug:\n",
    "            date = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print('-' * 80)\n",
    "            print(f'Finished at {date}')\n",
    "            print('-' * 80)\n",
    "            print('values', agent.values)\n",
    "            print('-' * 80)\n",
    "            print('policy', agent.policy)\n",
    "            print('=' * 80)\n",
    "\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(env: JackSCarRentalEnv, agent: JackSCarRentalAgent):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    def plot_actions():\n",
    "        states_1 = [s[0] for s in env.states]\n",
    "        states_2 = [s[1] for s in env.states]\n",
    "        idxs_opt_actions = [\n",
    "            np.argmax(agent.policy[i_s])\n",
    "            for i_s in env.states\n",
    "        ]\n",
    "        max_actions = [\n",
    "            env.actions_per_state[i_s][i_a]\n",
    "            for i_s, i_a in enumerate(idxs_opt_actions)\n",
    "        ]\n",
    "        max_actions_values = [\n",
    "            a[0] - a[1]\n",
    "            for a in max_actions\n",
    "        ]\n",
    "\n",
    "        # plot with y axis being state 1, x axis being state 2,\n",
    "        # and the color being the optimal action\n",
    "        plt.scatter(states_2, states_1, c=max_actions_values)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_values():\n",
    "        states_1 = [s[0] for s in env.states]\n",
    "        states_2 = [s[1] for s in env.states]\n",
    "        values = agent.values\n",
    "\n",
    "        # plot with y axis being state 1, x axis being state 2,\n",
    "        # and the color being the value of the state\n",
    "        plt.scatter(states_2, states_1, c=values)\n",
    "        plt.show()\n",
    "\n",
    "    plot_actions()\n",
    "    plot_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment\n",
    "env = make_env('original_env')\n",
    "# create the agent\n",
    "agent = JackSCarRentalAgent.load(name='original_agent', env=env, epsilon=0.0)\n",
    "# optimize the policy\n",
    "optimize_agent(env=env, agent=agent, debug=True)\n",
    "# plot the actions and values\n",
    "plot(env=env, agent=agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
