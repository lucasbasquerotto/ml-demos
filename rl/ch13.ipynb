{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "Use your knowledge of the gridworld and its dynamics to determine an exact symbolic expression for the optimal probability of selecting the right action in Example 13.1.\n",
    "\n",
    "**A**\n",
    "\n",
    "Considering the values of each state from left to right being $V_1$ $(V_{S_0}), V_2, V_3$ and $V_G$ $(V_{terminal} = 0)$, and defining them as a function of the other state-values and the probability of choosing the right action, $p = \\pi(right)$, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V_1(p) &= -1 + p V_2 + (1 - p) V_1(p)\n",
    "\\\\\n",
    "V_2(p) &= -1 + p V_1 + (1 - p) V_3(p)\n",
    "\\\\\n",
    "V_3(p) &= -1 + p V_G + (1 - p) V_2(p) = -1 + (1 - p) V_2(p)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The equations above are derived from the general definition of state-values:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V(s) &\\doteq \\sum_a \\pi(a | s) \\sum_{r, s'} p(r, s' | s, a) [r + V(s')] \n",
    "\\\\\n",
    "&= \\sum_a \\pi(a | s) \\sum_{s'} p(s' | s, a) \\cdot [-1 + V(s')] \n",
    "\\\\\n",
    "&= -1 + \\sum_a \\pi(a | s) \\sum_{s'} p(s' | s, a) V(s')\n",
    "\\\\\n",
    "&= -1 + [\\pi(right) \\cdot V(s_{right}')] + [\\pi(left) \\cdot V(s_{left}')]\n",
    "\\\\\n",
    "&= -1 + p V(s_{right}') + (1 - p) V(s_{left}')\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(in the given environment, the reward is always -1 $(r(s, a) = -1)$, and the transitions are deterministic, that is, given a state $s$ and an action $a$, there's exactly 1 possible next state $s'$, so $p(s' | s, a) = 1$)\n",
    "\n",
    "The probability of choosing an action is given by $\\pi(right)$ instead of $\\pi(right | s)$ because, due to the use of function approximation, all the states are seen by the agent as a single state, so the policy will only define probabilities for choosing an action, independently of the actual state, that maximizes the final return (this is the real challenge, otherwise, if the actual states were known by the agent, it could define the best action for the state using a deterministic policy: $\\pi(right | s_{left}) = 1$, $\\pi(left | s_{middle}) = 1$ and $\\pi(right | s_{right}) = 1$).\n",
    "\n",
    "Then, the best probability of choosing the right action is given by:\n",
    "\n",
    "$$\n",
    "\\pi(right) = \\operatorname*{argmax}_p \\begin{cases}\n",
    "  -1 + p V_2(p) + (1 - p) V_1(p) \\\\\n",
    "  -1 + p V_1(p) + (1 - p) V_3(p) \\\\\n",
    "  -1 + (1 - p) V_2(p)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The episodes start at the leftmost state $(S_0)$, so the objective is actually to maximize $V_1 = V_{S_0}$:\n",
    "\n",
    "$$\n",
    "\\pi(right) = \\operatorname*{argmax}_p V_1(p)\n",
    "$$\n",
    "\n",
    "(the value of $V_1$ depends on $V_2$, that depends on $V_3$, so $p$ must be chosen in such a way that the start state is the best possible, which depends on the changes in the other states due to $p$).\n",
    "\n",
    "The above definition is enough to answer this exercise, but to verify the actual probability we can simplify the equations, defining $V_1(p)$ as a function that depends only on $p$, without other state-values, and then find the value of $p$ that maximizes $V_1(p)$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V_3 &= -1 + (1 - p) V_2\n",
    "\\\\\n",
    "V_3 &= -1 + (1 - p)  [-1 + p V_1 + (1 - p) V_3]\n",
    "\\\\\n",
    "V_3 &= -1 + p - 1 + p(1 - p)V_1 + (1 - p)^2 V_3\n",
    "\\\\\n",
    "V_3 &= -1 + p - 1 + p(1 - p)V_1 + (1 - 2p + p^2) V_3\n",
    "\\\\\n",
    "V_3 &= p - 2 + p(1 - p)V_1 + [V_3 - 2pV_3 + p^2 V_3]\n",
    "\\\\\n",
    "2pV_3 - p^2 V_3 &= p(1 - p)V_1 + p - 2\n",
    "\\\\\n",
    "V_3 &= \\frac{p(1 - p)V_1 + p - 2}{2p - p^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V_1 &= -1 + p V_2 + (1 - p) V_1\n",
    "\\\\\n",
    "V_1 &= p V_2 - 1 + V_1 - p V_1\n",
    "\\\\\n",
    "p V_1 &= p V_2 - 1\n",
    "\\\\\n",
    "V_1 &= V_2 - \\frac{1}{p}\n",
    "\\\\\n",
    "V_1 &= -1 + p V_1 + (1 - p) V_3 - \\frac{1}{p}\n",
    "\\\\\n",
    "(1 - p)V_1 &= (1 - p) \\frac{p(1 - p)V_1 + p - 2}{2p - p^2} - \\frac{1}{p} - 1\n",
    "\\\\\n",
    "V_1 &= \\frac{p(1 - p)V_1 + p - 2}{2p - p^2} - \\frac{1}{p(1 - p)} - \\frac{1}{1 - p}\n",
    "\\\\\n",
    "V_1 &= \\frac{p(1 - p)V_1 + p - 2}{p(2 - p)} - \\frac{1}{p(1 - p)} - \\frac{1}{1 - p}\n",
    "\\\\\n",
    "V_1 &= \\frac{(1 - p)[p(1 - p)V_1 + p - 2] - [2 - p] - p[2 - p]}{p(1 - p)(2 - p)}\n",
    "\\\\\n",
    "p(1 - p)(2 - p) V_1 &= p(1 - p)^2V_1 + p(1 - p) - 2(1 - p) - (2 - p) - (2p - p^2)\n",
    "\\\\\n",
    "p(1 - p)[(2 - p) - (1 - p)] V_1 &= p(1 - p) - 2(1 - p) - (2 - p) - (2p - p^2)\n",
    "\\\\\n",
    "p(1 - p)[2 - p - 1 + p] V_1 &= p - p^2 + 2p - 2 + p - 2 + p^2 - 2p\n",
    "\\\\\n",
    "p(1 - p) V_1 &= 2p - 4\n",
    "\\\\\n",
    "V_1 &= \\frac{2p - 4}{p(1 - p)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's consider the function:\n",
    "\n",
    "$$\n",
    "f(p) = \\frac{2p - 4}{p(1 - p)}\n",
    "$$\n",
    "\n",
    "To find the critical points, we need to take the derivative of $f(p)$ with respect to $p$, then set it equal to zero.\n",
    "\n",
    "**Differentiate the numerator and the denominator using the quotient rule:**\n",
    "\n",
    "The quotient rule states that for a function $f(p) = \\frac{g(p)}{h(p)}$, the derivative is:\n",
    "\n",
    "$$\n",
    "f'(p) = \\frac{g'(p)h(p) - g(p)h'(p)}{[h(p)]^2}\n",
    "$$\n",
    "\n",
    "Here, we have:\n",
    "\n",
    "- $g(p) = 2p - 4$\n",
    "- $h(p) = p(1 - p) = p - p^2$\n",
    "\n",
    "Now, let's compute the derivatives:\n",
    "\n",
    "- $g'(p) = 2$\n",
    "- $h'(p) = 1 - 2p$\n",
    "\n",
    "Now, applying the quotient rule:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f'(p) &= \\frac{2(p - p^2) - (2p - 4)(1 - 2p)}{(p - p^2)^2}\n",
    "\\\\\n",
    "&= \\frac{(2p - 2p^2) - (2p - 4p^2 - 4 + 8p)}{(p - p^2)^2}\n",
    "\\\\\n",
    "&= \\frac{2p - 2p^2 - 2p + 4p^2 + 4 - 8p}{(p - p^2)^2}\n",
    "\\\\\n",
    "&= \\frac{2p^2 - 8p + 4}{(p - p^2)^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Set $f'(p) = 0$**\n",
    "\n",
    "To find the critical points, we set the numerator equal to zero:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "2p^2 - 8p + 4 &= 0\n",
    "\\\\\n",
    "p^2 - 4p + 2 &= 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now solve this quadratic equation using the quadratic formula:\n",
    "\n",
    "$$\n",
    "p = \\frac{-(-4) \\pm \\sqrt{(-4)^2 - 4(1)(2)}}{2(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p = \\frac{4 \\pm \\sqrt{16 - 8}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p = \\frac{4 \\pm \\sqrt{8}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p = \\frac{4 \\pm 2\\sqrt{2}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p = 2 \\pm \\sqrt{2}\n",
    "$$\n",
    "\n",
    "Thus, the two possible values of $p$ are:\n",
    "\n",
    "$$\n",
    "p = 2 + \\sqrt{2} \\quad \\text{or} \\quad p = 2 - \\sqrt{2}\n",
    "$$\n",
    "\n",
    "**Analyze the critical points**\n",
    "\n",
    "Since $p$ must be between 0 and 1 (as $p$ is a probability), the only valid solution is:\n",
    "\n",
    "$$\n",
    "p = 2 - \\sqrt{2}\n",
    "$$\n",
    "\n",
    "Thus, the value of $p$ that maximizes $f(p)$ is:\n",
    "\n",
    "$$\n",
    "p = 2 - \\sqrt{2} \\approx 0.5858\n",
    "$$\n",
    "\n",
    "So, the optimal probability of selecting the right action is:\n",
    "\n",
    "$$\n",
    "\\pi(right) = 2 - \\sqrt{2} \\approx 0.5858\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.2\n",
    "\n",
    "**Q**\n",
    "\n",
    "Generalize the box on page 199, the policy gradient theorem (13.5), the proof of the policy gradient theorem (page 325), and the steps leading to the REINFORCE update equation (13.8), so that (13.8) ends up with a factor of $\\gamma^t$ and thus aligns with the general algorithm given in the pseudocode.\n",
    "\n",
    "**A**\n",
    "\n",
    "The policy gradient theorem (13.5) is:\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a q_{\\pi}(s, a) \\nabla \\pi(a | s, \\theta) \\tag{13.5}\n",
    "$$\n",
    "\n",
    "The REINFORCE update equation (13.8) is:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} \\doteq \\theta_t + \\alpha G_t \\frac{\\nabla \\pi(A_t | S_t, \\theta_t)}{\\pi(A_t | S_t, \\theta_t)} \\tag{13.8}\n",
    "$$\n",
    "\n",
    "In the box on page 199, we have:\n",
    "\n",
    "$$\n",
    "\\eta(s) = h(s) + \\sum_{\\overline{s}} \\eta(\\overline{s}) \\sum_a \\pi(a | \\overline{s}) p(s | \\overline{s}, a), \\quad \\text{for all } s \\in \\mathcal{S} \\tag{9.2}\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "\\mu(s) = \\frac{\\eta(s)}{\\sum_{s'} \\eta(s')}, \\quad \\text{for all } s \\in \\mathcal{S} \\tag{9.3}\n",
    "$$\n",
    "\n",
    "The proof of the policy gradient theorem (page 325) is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla v_{\\pi} (s) &= \\nabla \\left[ \\sum_a \\pi(a | s) q_{\\pi} (s, a) \\right] , \\quad \\text{for all } s \\in \\mathcal{S} \\tag{Exercise 3.18}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\nabla q_{\\pi}(s, a) \\right] \\tag{product rule of calculus}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\nabla \\sum_{s', r} p(s', r | s, a)(r + v_{\\pi}(s')) \\right] \\tag{Exercise 3.19 and Equation 3.2}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\sum_{s'} p(s' | s, a) \\nabla v_{\\pi}(s') \\right] \\tag{Eq. 3.4}\n",
    "\\\\\n",
    "&= \\sum_a \\bigg[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\sum_{s'} p(s' | s, a) \\tag{unrolling}\n",
    "\\\\\n",
    "&\\quad \\quad \\quad \\sum_{a'} \\bigg[\\nabla \\pi(a'|s') q_{\\pi}(s', a') + \\pi(a'|s') \\sum_{s''} p(s'' | s', a') \\nabla v_{\\pi}(s'') \\bigg] \\bigg]\n",
    "\\\\\n",
    "&= \\sum_{x \\in \\mathcal{S}} \\sum_{k = 0}^{\\infty} Pr(s \\to x, k, \\pi) \\sum_a \\nabla \\pi(a|x) q_{\\pi}(x, a)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "after repeated unrolling, where $Pr(s \\to x, k, \\pi)$ is the probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\\pi$. It is then immediate that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla J(\\theta) &= \\nabla v_{\\pi}(s_0)\n",
    "\\\\\n",
    "&= \\sum_s \\left( \\sum_{k=0}^{\\infty} Pr(s_0 \\to s, k, \\pi) \\right) \\sum_a \\nabla \\pi(a | s) q_{\\pi}(s, a)\n",
    "\\\\\n",
    "&= \\sum_s \\eta (s) \\sum_a \\nabla \\pi(a|s) q_{\\pi} (s, a) \\tag{box page 199}\n",
    "\\\\\n",
    "&= \\sum_{s'} \\eta (s') \\sum_s \\frac{\\eta (s)}{\\sum_{s'} \\eta (s')} \\sum_a \\nabla \\pi(a|s) q_{\\pi} (s, a)\n",
    "\\\\\n",
    "&= \\sum_{s'} \\eta (s') \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a|s) q_{\\pi} (s, a) \\tag{Eq. 9.3}\n",
    "\\\\\n",
    "&\\propto \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a|s) q_{\\pi} (s, a) \\tag{Q.E.D.}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To include the discount factor $\\gamma$, we change the policy gradient theorem to define $q_{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a)(r + \\gamma v_{\\pi}(s'))$ (instead of $q_{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a)(r + v_{\\pi}(s'))$):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla v_{\\pi} (s) &= \\nabla \\left[ \\sum_a \\pi(a | s) q_{\\pi} (s, a) \\right] , \\quad \\text{for all } s \\in \\mathcal{S} \\tag{Exercise 3.18}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\nabla q_{\\pi}(s, a) \\right] \\tag{product rule of calculus}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\nabla \\sum_{s', r} p(s', r | s, a)(r + \\gamma v_{\\pi}(s')) \\right] \\tag{Exercise 3.19 and Equation 3.2}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\sum_{s'} p(s' | s, a) \\gamma \\nabla v_{\\pi}(s') \\right] \\tag{Eq. 3.4, $\\gamma$ is constant}\n",
    "\\\\\n",
    "&= \\sum_a \\bigg[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\sum_{s'} p(s' | s, a) \\gamma \\tag{unrolling}\n",
    "\\\\\n",
    "&\\quad \\quad \\quad \\sum_{a'} \\bigg[\\nabla \\pi(a'|s') q_{\\pi}(s', a') + \\pi(a'|s') \\sum_{s''} p(s'' | s', a') \\gamma \\nabla v_{\\pi}(s'') \\bigg] \\bigg]\n",
    "\\\\\n",
    "&= \\sum_{x \\in \\mathcal{S}} \\sum_{k = 0}^{\\infty} \\left[ Pr(s \\to x, k, \\pi) \\gamma^k \\right] \\sum_a \\nabla \\pi(a|x) q_{\\pi}(x, a)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And then:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla J(\\theta) &= \\nabla v_{\\pi}(s_0)\n",
    "\\\\\n",
    "&= \\sum_s \\left( \\sum_{k=0}^{\\infty} \\left[ Pr(s_0 \\to s, k, \\pi) \\gamma^k \\right] \\right) \\sum_a \\nabla \\pi(a | s, \\theta) q_{\\pi}(s, a)\n",
    "\\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\sum_s \\eta (s) \\gamma^t \\sum_a \\nabla \\pi(a|s, \\theta) q_{\\pi} (s, a) \\right] \\tag{replacing $\\gamma^k$ by the sample $\\gamma^t$ at the time step $t$ in the expectation*}\n",
    "\\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^t \\sum_{s'} \\eta (s') \\sum_s \\frac{\\eta (s)}{\\sum_{s'} \\eta (s')} \\sum_a \\nabla \\pi(a|s, \\theta) q_{\\pi} (s, a) \\right]\n",
    "\\\\\n",
    "&= \\left[\\sum_{s'} \\eta (s') \\right] \\mathbb{E}_{\\pi} \\left[ \\gamma^t \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a|s, \\theta) q_{\\pi} (s, a) \\right] \\tag{Eq. 9.3}\n",
    "\\\\\n",
    "&\\propto \\mathbb{E}_{\\pi} \\left[ \\gamma^t \\sum_s \\mu(s) \\sum_a q_{\\pi} (s, a) \\nabla \\pi(a|s, \\theta) \\right]\n",
    "\\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^t \\sum_a \\pi(a|S_t, \\theta) q_{\\pi} (S_t, a) \\frac{\\nabla \\pi(a|S_t, \\theta)}{\\pi(a|S_t, \\theta)} \\right] \\tag{replacing $s$ by the sample $S_t \\sim \\pi$}\n",
    "\\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^t q_{\\pi} (S_t, A_t) \\frac{\\nabla \\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t, \\theta)} \\right] \\tag{replacing $a$ by the sample $A_t \\sim \\pi$}\n",
    "\\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^t G_t \\frac{\\nabla \\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t, \\theta)} \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} \\doteq \\theta_t + \\alpha \\gamma^t G_t \\frac{\\nabla \\pi(A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} = \\theta_t + \\alpha \\gamma^t G_t \\nabla \\ln \\pi(A_t|S_t, \\theta_t)\n",
    "$$\n",
    "\n",
    "as defined in the pseudocode.\n",
    "\n",
    "*\\*The substitution of $\\sum_{k=0}^{\\infty} Pr(s_0 \\to s, k, \\pi) = \\eta (s)$ in the original is mathematically obscure (considering only the given information), even tough it's understandable intuitively. The discount factor $\\gamma$  to the power of $k$ is intuitively equivalent to $\\gamma^t$ at the time-step $t$, considering that $k$ corresponds to the time-steps (in the iteration) in which the state happened, although it's not completely clear mathematically that $\\sum_{k=0}^{\\infty} \\left[ Pr(s_0 \\to s, k, \\pi) \\gamma^k \\right] = \\eta (s) \\gamma^t$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.3\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Section 13.1 we considered policy parameterizations using the soft-max in action preferences (13.2) with linear action preferences (13.3). For this parameterization, prove that the eligibility vector is\n",
    "\n",
    "$$\n",
    "\\nabla \\ln \\pi(a|s, \\theta) = \\textbf{x}(s, a) - \\sum_b \\pi(b|s, \\theta) \\textbf{x}(s, b)\n",
    "$$\n",
    "\n",
    "using the definitions and elementary calculus.\n",
    "\n",
    "**A**\n",
    "\n",
    "The soft-max in action preferences (13.2) is defined below:\n",
    "\n",
    "$$\n",
    "\\pi(a|s, \\theta) \\doteq \\frac{e^{h(s, a, \\theta)}}{\\sum_b e^{h(s, b, \\theta)}} \\tag{13.2}\n",
    "$$\n",
    "\n",
    "When the action preferences are linear , we have:\n",
    "\n",
    "$$\n",
    "h(s, a, \\theta) = \\theta^T \\textbf{x}(s, a) \\tag{13.3}\n",
    "$$\n",
    "\n",
    "The derivative of $\\ln x$ is $\\frac{1}{x}$ as demonstrated below:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d}{dx} (\\ln x) &= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{\\ln{(x + \\Delta x)} - \\ln{(x)}}{\\Delta x}\n",
    "\\\\\n",
    "&= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{\\ln{(\\frac{x + \\Delta x}{x})}}{\\Delta x} \\tag{using the logaritmic property}\n",
    "\\\\\n",
    "&= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{\\ln{(1 + \\frac{\\Delta x}{x})}}{\\Delta x}\n",
    "\\\\\n",
    "&\\approx \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{\\frac{\\Delta x}{x}}{\\Delta x} \\tag{when $\\frac{\\Delta x}{x}$ is small, which is the case when $\\Delta x \\to 0$}\n",
    "\\\\\n",
    "&= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{1}{x}\n",
    "\\\\\n",
    "&= \\frac{1}{x}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Alternatively, making $y = \\ln x$, which means that $e^y = x$, and also having in mind that $\\frac{d e^x}{dx} = e^x$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d e^y}{dx} &= \\frac{dx}{dx}\n",
    "\\\\\n",
    "\\frac{d e^y}{dy} \\frac{dy}{dx} &= 1\n",
    "\\\\\n",
    "e^y \\frac{d \\ln x}{dx} &= 1\n",
    "\\\\\n",
    "x \\frac{d \\ln x}{dx} &= 1\n",
    "\\\\\n",
    "\\frac{d \\ln x}{dx} &= \\frac{1}{x}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Also, $\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) = \\frac{g(x) f'(x) - f(x) g'(x)}{[g(x)]^2}$ as demonstrated below, considering $h(x) \\doteq \\frac{f(x)}{g(x)}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h'(x) &\\doteq \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{h(x + \\Delta x) - h(x)}{\\Delta x}\n",
    "\\\\\n",
    "\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) &= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{\\frac{f(x + \\Delta x)}{g(x + \\Delta x)} - \\frac{f(x)}{g(x)}}{\\Delta x}\n",
    "\\\\\n",
    "\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) &= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{\\frac{f(x + \\Delta x)g(x) - f(x)g(x + \\Delta x)}{g(x) g(x + \\Delta x)}}{\\Delta x}\n",
    "\\\\\n",
    "\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) &= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{f(x + \\Delta x)g(x) - f(x)g(x + \\Delta x)}{g(x) g(x + \\Delta x) \\Delta x}\n",
    "\\\\\n",
    "\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) &= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{\\frac{f(x + \\Delta x)}{\\Delta x}g(x) - f(x)\\frac{g(x + \\Delta x)}{\\Delta x}}{g(x)g(x + \\Delta x)}\n",
    "\\\\\n",
    "\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) &= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{\\frac{f(x + \\Delta x)}{\\Delta x}g(x) - \\frac{f(x)g(x)}{\\Delta x} + \\frac{f(x)g(x)}{\\Delta x} - f(x)\\frac{g(x + \\Delta x)}{\\Delta x}}{g(x)g(x + \\Delta x)}\n",
    "\\\\\n",
    "\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) &= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{\\left[ \\frac{f(x + \\Delta x)}{\\Delta x}g(x) - \\frac{f(x)}{\\Delta x} g(x) \\right] - \\left[ f(x)\\frac{g(x + \\Delta x)}{\\Delta x} - f(x) \\frac{g(x)}{\\Delta x} \\right]}{g(x)g(x + \\Delta x)}\n",
    "\\\\\n",
    "\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) &= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{\\frac{f(x + \\Delta x) - f(x)}{\\Delta x}g(x) - f(x)\\frac{g(x + \\Delta x) - g(x)}{\\Delta x}}{g(x)g(x + \\Delta x)}\n",
    "\\\\\n",
    "\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) &= \\operatorname*{lim}_{\\Delta x \\to 0} \\frac{f'(x)g(x) - f(x)g'(x)}{g(x)g(x + \\Delta x)}\n",
    "\\\\\n",
    "\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) &= \\frac{f'(x)g(x) - f(x)g'(x)}{[g(x)]^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The extension to gradients is applied naturally.\n",
    "\n",
    "Making $f(\\theta) \\doteq e^{h(s, a, \\theta)}$ ($s$ and $a$ are constant w.r.t. $\\theta$) and $g(\\theta) \\doteq \\sum_b e^{h(s, b, \\theta)}$, with:\n",
    "\n",
    "$$\n",
    "q(\\theta) \\doteq \\frac{f(\\theta)}{g(\\theta)} = \\frac{e^{h(s, a, \\theta)}}{\\sum_b e^{h(s, b, \\theta)}} = \\pi(a|s, \\theta)\n",
    "$$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} f(\\theta) &= \\nabla_{\\theta} e^{h(s, a, \\theta)}\n",
    "\\\\\n",
    "&= \\nabla_h e^{h(s, a, \\theta)} \\nabla_{\\theta} h(s, a, \\theta)\n",
    "\\\\\n",
    "&= e^{h(s, a, \\theta)} \\nabla_{\\theta} [\\theta^T \\textbf{x}(s, a)]\n",
    "\\\\\n",
    "&= e^{h(s, a, \\theta)} \\textbf{x}(s, a)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} g(\\theta) &= \\nabla_{\\theta} \\sum_b e^{h(s, b, \\theta)}\n",
    "\\\\\n",
    "&= \\sum_b \\nabla_{\\theta} e^{h(s, b, \\theta)}\n",
    "\\\\\n",
    "&= \\sum_b \\nabla_h e^{h(s, b, \\theta)} \\nabla_{\\theta} h(s, b, \\theta)\n",
    "\\\\\n",
    "&= \\sum_b e^{h(s, b, \\theta)} \\nabla_{\\theta} [\\theta^T \\textbf{x}(s, b)]\n",
    "\\\\\n",
    "&= \\sum_b e^{h(s, b, \\theta)} \\textbf{x}(s, b)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\ln \\pi(a|s, \\theta) &= \\nabla_{\\theta} \\ln q(\\theta)\n",
    "\\\\\n",
    "&= [\\nabla_q \\ln q(\\theta)] [\\nabla_{\\theta} q(\\theta)]\n",
    "\\\\\n",
    "&= \\left[ \\frac{1}{q(\\theta)} \\right] \\left[ \\nabla_{\\theta} \\frac{f(\\theta)}{g(\\theta)} \\right]\n",
    "\\\\\n",
    "&= \\left[ \\frac{g(\\theta)}{f(\\theta)} \\right] \\left[ \\frac{[\\nabla_{\\theta} f(\\theta)] g(\\theta) - f(\\theta) [\\nabla_{\\theta} g(\\theta)]}{[g(\\theta)]^2} \\right]\n",
    "\\\\\n",
    "&= \\frac{[\\nabla_{\\theta} f(\\theta)] g(\\theta) - f(\\theta) [\\nabla_{\\theta} g(\\theta)]}{f(\\theta) g(\\theta)}\n",
    "\\\\\n",
    "&= \\frac{\\nabla_{\\theta} f(\\theta)}{f(\\theta)} - \\frac{\\nabla_{\\theta} g(\\theta)}{g(\\theta)}\n",
    "\\\\\n",
    "&= \\frac{e^{h(s, a, \\theta)} \\textbf{x}(s, a)}{e^{h(s, a, \\theta)}} - \\frac{\\sum_b e^{h(s, b, \\theta)} \\textbf{x}(s, b)}{\\sum_c e^{h(s, c, \\theta)}}\n",
    "\\\\\n",
    "&= \\textbf{x}(s, a) - \\sum_b \\frac{e^{h(s, b, \\theta)}}{\\sum_{c} e^{h(s, c, \\theta)}} \\textbf{x}(s, b)\n",
    "\\\\\n",
    "&= \\textbf{x}(s, a) - \\sum_b \\pi(b|s, \\theta) \\textbf{x}(s, b)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.4\n",
    "\n",
    "**Q**\n",
    "\n",
    "Show that for the gaussian policy parameterization (13.19) the eligibility vector has the following two parts:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla \\ln \\pi(a | s, \\theta_{\\mu}) &= \\frac{\\nabla \\pi(a | s, \\theta_{\\mu})}{\\pi(a | s, \\theta)} = \\frac{1}{\\sigma(s, \\theta)^2} (a - \\mu(s, \\theta)) \\textbf{x}_{\\mu}(s)\\text{, and}\n",
    "\\\\\n",
    "\\nabla \\ln \\pi(a | s, \\theta_{\\sigma}) &= \\frac{\\nabla \\pi(a | s, \\theta_{\\sigma})}{\\pi(a | s, \\theta)} = \\left( \\frac{(a - \\mu(s, \\theta))^2}{\\sigma(s, \\theta)^2} - 1 \\right) \\textbf{x}_{\\sigma}(s)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**A**\n",
    "\n",
    "The gaussian policy parameterization (13.19) is:\n",
    "\n",
    "$$\n",
    "\\pi(a | s, \\theta) \\doteq \\frac{1}{\\sigma(s, \\theta) \\sqrt{2 \\pi}} \\exp{\\left(- \\frac{(a - \\mu(s, \\theta))^2}{2 \\sigma(s, \\theta)^2} \\right)} \\tag{3.19}\n",
    "$$\n",
    "\n",
    "where $\\mu : \\mathcal{S} \\times \\mathbb{R}^{d'} \\to \\mathbb{R}$ and $\\sigma : \\mathcal{S} \\times \\mathbb{R}^{d'} \\to \\mathbb{R}^+$ are two parameterized function approximators.\n",
    "\n",
    "Also:\n",
    "\n",
    "$$\n",
    "\\mu(s, \\theta) \\doteq \\theta_{\\mu}^T \\textbf{x}_{\\mu}(s) \\quad \\text{and} \\quad \\sigma(s, \\theta) \\doteq \\exp{(\\theta_{\\sigma}^T \\textbf{x}_{\\sigma}(s))} \\tag{3.20}\n",
    "$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d}{dx} \\ln{f(x)} &= \\left[ \\frac{d}{d f(x)} \\ln{f(x)} \\right] \\left[ \\frac{d}{dx} f(x) \\right] \\tag{chain rule of calculus}\n",
    "\\\\\n",
    "&= \\frac{1}{f(x)} \\frac{d}{dx} f(x) \\tag{because $\\frac{d \\ln x}{dx} = \\frac{1}{x}$ as demonstrated in Exercise 13.3}\n",
    "\\\\\n",
    "&= \\frac{\\frac{d}{dx} f(x)}{f(x)} \\tag{a1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The previous demonstration can be extended to gradients and prove the first equality of each of the 2 parts of the question of this exercise.\n",
    "\n",
    "Defining $f(\\theta)$ as:\n",
    "\n",
    "$$\n",
    "f(\\theta) \\doteq - \\frac{(a - \\mu(s, \\theta_{\\mu}))^2}{2 \\sigma(s, \\theta_{\\sigma})^2} \\tag{a2}\n",
    "$$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla \\ln \\pi(a | s, \\theta_{\\mu}) &= \\frac{\\nabla \\pi(a | s, \\theta_{\\mu})}{\\pi(a | s, \\theta)} \\tag{from a1}\n",
    "\\\\\n",
    "&= \\frac{\\nabla_{\\theta_{\\mu}} \\frac{1}{\\sigma(s, \\theta_{\\sigma}) \\sqrt{2 \\pi}} \\exp{\\left(- \\frac{(a - \\mu(s, \\theta_{\\mu}))^2}{2 \\sigma(s, \\theta_{\\sigma})^2} \\right)}}{\\frac{1}{\\sigma(s, \\theta) \\sqrt{2 \\pi}} \\exp{\\left(- \\frac{(a - \\mu(s, \\theta))^2}{2 \\sigma(s, \\theta)^2} \\right)}}\n",
    "\\\\\n",
    "&= \\frac{1}{\\sigma(s, \\theta_{\\sigma}) \\sqrt{2 \\pi}} \\frac{\\nabla_{\\theta_{\\mu}} \\exp{\\left(- \\frac{(a - \\mu(s, \\theta_{\\mu}))^2}{2 \\sigma(s, \\theta_{\\sigma})^2} \\right)}}{\\frac{1}{\\sigma(s, \\theta) \\sqrt{2 \\pi}} \\exp{\\left(- \\frac{(a - \\mu(s, \\theta))^2}{2 \\sigma(s, \\theta)^2} \\right)}}\n",
    "\\\\\n",
    "&= \\frac{\\nabla_{\\theta_{\\mu}} \\exp{\\left(- \\frac{(a - \\mu(s, \\theta_{\\mu}))^2}{2 \\sigma(s, \\theta_{\\sigma})^2} \\right)}}{\\exp{\\left(- \\frac{(a - \\mu(s, \\theta))^2}{2 \\sigma(s, \\theta)^2} \\right)}}\n",
    "\\\\\n",
    "&= \\frac{\\nabla_{\\theta_{\\mu}} \\exp{f(\\theta)}}{\\exp{f(\\theta)}} \\tag{from a2}\n",
    "\\\\\n",
    "&= \\frac{\\left[ \\nabla_f \\exp{f(\\theta)} \\right] \\left[ \\nabla_{\\theta_{\\mu}} f(\\theta) \\right]}{\\exp{f(\\theta)}}\n",
    "\\\\\n",
    "&= \\frac{\\exp{f(\\theta)} \\left[ \\nabla_{\\theta_{\\mu}} f(\\theta) \\right]}{\\exp{f(\\theta)}}\n",
    "\\\\\n",
    "&= \\nabla_{\\theta_{\\mu}} f(\\theta)\n",
    "\\\\\n",
    "&= \\nabla_{\\theta_{\\mu}} \\left[ - \\frac{(a - \\mu(s, \\theta_{\\mu}))^2}{2 \\sigma(s, \\theta_{\\sigma})^2} \\right]\n",
    "\\\\\n",
    "&= - \\frac{1}{2 \\sigma(s, \\theta_{\\sigma})^2} \\nabla_{\\theta_{\\mu}} (a - \\mu(s, \\theta_{\\mu}))^2\n",
    "\\\\\n",
    "&= - \\frac{1}{2 \\sigma(s, \\theta_{\\sigma})^2} \\cdot 2 (a - \\mu(s, \\theta_{\\mu})) \\nabla_{\\theta_{\\mu}} (a - \\mu(s, \\theta_{\\mu})) \\tag{chain rule of gradients}\n",
    "\\\\\n",
    "&= - \\frac{a - \\mu(s, \\theta_{\\mu})}{\\sigma(s, \\theta_{\\sigma})^2} \\nabla_{\\theta_{\\mu}} (a - \\mu(s, \\theta_{\\mu}))\n",
    "\\\\\n",
    "&= - \\frac{a - \\mu(s, \\theta_{\\mu})}{\\sigma(s, \\theta_{\\sigma})^2} \\cdot (-1) \\cdot \\nabla_{\\theta_{\\mu}} \\mu(s, \\theta_{\\mu}) \\tag{chain rule of gradients}\n",
    "\\\\\n",
    "&= \\frac{a - \\mu(s, \\theta_{\\mu})}{\\sigma(s, \\theta_{\\sigma})^2} \\nabla_{\\theta_{\\mu}} [\\theta_{\\mu}^T \\textbf{x}_{\\mu}(s)]\n",
    "\\\\\n",
    "&= \\frac{a - \\mu(s, \\theta_{\\mu})}{\\sigma(s, \\theta_{\\sigma})^2} \\textbf{x}_{\\mu}(s) \\tag{first part proved}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the second part we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla \\ln \\pi(a | s, \\theta_{\\mu}) &= \\frac{\\nabla \\pi(a | s, \\theta_{\\mu})}{\\pi(a | s, \\theta)} \\tag{from a1}\n",
    "\\\\\n",
    "&= \\frac{\\nabla_{\\theta_{\\sigma}} \\frac{1}{\\sigma(s, \\theta_{\\sigma}) \\sqrt{2 \\pi}} \\exp{f(\\theta)}}{\\frac{1}{\\sigma(s, \\theta) \\sqrt{2 \\pi}} \\exp{f(\\theta)}}\n",
    "\\\\\n",
    "&= \\frac{1}{\\sqrt{2 \\pi}} \\frac{\\nabla_{\\theta_{\\sigma}} \\frac{1}{\\sigma(s, \\theta_{\\sigma})} \\exp{f(\\theta)}}{\\frac{1}{\\sigma(s, \\theta) \\sqrt{2 \\pi}} \\exp{f(\\theta)}}\n",
    "\\\\\n",
    "&= \\frac{\\nabla_{\\theta_{\\sigma}} \\frac{1}{\\sigma(s, \\theta_{\\sigma})} \\exp{f(\\theta)}}{\\frac{1}{\\sigma(s, \\theta)} \\exp{f(\\theta)}}\n",
    "\\\\\n",
    "&= \\frac{\\left[ \\nabla_{\\theta_{\\sigma}} \\frac{1}{\\sigma(s, \\theta_{\\sigma})} \\right] \\exp{f(\\theta)} + \\frac{1}{\\sigma(s, \\theta_{\\sigma})} \\left[ \\nabla_{\\theta_{\\sigma}} \\exp{f(\\theta)} \\right]}{\\frac{1}{\\sigma(s, \\theta)} \\exp{f(\\theta)}} \\tag{product rule of calculus}\n",
    "\\\\\n",
    "&= \\frac{\\nabla_{\\theta_{\\sigma}} \\frac{1}{\\sigma(s, \\theta_{\\sigma})}}{\\frac{1}{\\sigma(s, \\theta)}} + \\frac{\\nabla_{\\theta_{\\sigma}} \\exp{f(\\theta)}}{\\exp{f(\\theta)}}\n",
    "\\\\\n",
    "&= \\frac{\\left[ \\nabla_{\\sigma} \\frac{1}{\\sigma(s, \\theta_{\\sigma})} \\right] \\left[ \\nabla_{\\theta_{\\sigma}} \\sigma(s, \\theta_{\\sigma}) \\right]}{\\frac{1}{\\sigma(s, \\theta)}} + \\frac{[\\nabla_f \\exp{f(\\theta)}] [\\nabla_{\\theta_{\\sigma}} f(\\theta)]}{\\exp{f(\\theta)}} \\tag{chain rule of gradients}\n",
    "\\\\\n",
    "&= \\frac{\\left[ - \\frac{1}{\\sigma(s, \\theta_{\\sigma})^2} \\right] \\left[ \\nabla_{\\theta_{\\sigma}} \\sigma(s, \\theta_{\\sigma}) \\right]}{\\frac{1}{\\sigma(s, \\theta)}} + \\frac{\\exp{f(\\theta)} [\\nabla_{\\theta_{\\sigma}} f(\\theta)]}{\\exp{f(\\theta)}}\n",
    "\\\\\n",
    "&= -\\frac{\\nabla_{\\theta_{\\sigma}} \\sigma(s, \\theta_{\\sigma})}{\\sigma(s, \\theta)} + \\nabla_{\\theta_{\\sigma}} f(\\theta)\n",
    "\\\\\n",
    "&= -\\frac{\\nabla_{\\theta_{\\sigma}} \\exp{(\\theta_{\\sigma}^T \\textbf{x}_{\\sigma}(s))}}{\\sigma(s, \\theta)} + \\nabla_{\\theta_{\\sigma}} \\left[ - \\frac{(a - \\mu(s, \\theta_{\\mu}))^2}{2 \\sigma(s, \\theta_{\\sigma})^2} \\right]\n",
    "\\\\\n",
    "&= -\\frac{\\exp{(\\theta_{\\sigma}^T \\textbf{x}_{\\sigma}(s))} \\nabla_{\\theta_{\\sigma}} [\\theta_{\\sigma}^T \\textbf{x}_{\\sigma}(s)]}{\\sigma(s, \\theta)} - (a - \\mu(s, \\theta_{\\mu}))^2 \\nabla_{\\theta_{\\sigma}} \\frac{1}{2 \\sigma(s, \\theta_{\\sigma})^2}\n",
    "\\\\\n",
    "&= -\\frac{\\sigma(s, \\theta) \\nabla_{\\theta_{\\sigma}} [\\theta_{\\sigma}^T \\textbf{x}_{\\sigma}(s)]}{\\sigma(s, \\theta)} - (a - \\mu(s, \\theta_{\\mu}))^2 \\frac{1}{2} \\frac{-2}{\\sigma(s, \\theta_{\\sigma})^3} \\nabla_{\\theta_{\\sigma}} \\sigma(s, \\theta_{\\sigma})\n",
    "\\\\\n",
    "&= -\\textbf{x}_{\\sigma}(s) + (a - \\mu(s, \\theta_{\\mu}))^2 \\frac{1}{\\sigma(s, \\theta_{\\sigma})^3} \\nabla_{\\theta_{\\sigma}} \\exp{(\\theta_{\\sigma}^T \\textbf{x}_{\\sigma}(s))}\n",
    "\\\\\n",
    "&= -\\textbf{x}_{\\sigma}(s) + (a - \\mu(s, \\theta_{\\mu}))^2 \\frac{1}{\\sigma(s, \\theta_{\\sigma})^3} \\exp{(\\theta_{\\sigma}^T \\textbf{x}_{\\sigma}(s))} \\nabla_{\\theta_{\\sigma}} [\\theta_{\\sigma}^T \\textbf{x}_{\\sigma}(s)]\n",
    "\\\\\n",
    "&= -\\textbf{x}_{\\sigma}(s) + (a - \\mu(s, \\theta_{\\mu}))^2 \\frac{1}{\\sigma(s, \\theta_{\\sigma})^3} \\sigma(s, \\theta_{\\sigma}) \\nabla_{\\theta_{\\sigma}} [\\theta_{\\sigma}^T \\textbf{x}_{\\sigma}(s)]\n",
    "\\\\\n",
    "&= -\\textbf{x}_{\\sigma}(s) + (a - \\mu(s, \\theta_{\\mu}))^2 \\frac{1}{\\sigma(s, \\theta_{\\sigma})^2} \\nabla_{\\theta_{\\sigma}} [\\theta_{\\sigma}^T \\textbf{x}_{\\sigma}(s)]\n",
    "\\\\\n",
    "&= -\\textbf{x}_{\\sigma}(s) + \\frac{(a - \\mu(s, \\theta_{\\mu}))^2}{\\sigma(s, \\theta_{\\sigma})^2} \\textbf{x}_{\\sigma}(s)\n",
    "\\\\\n",
    "&= \\frac{(a - \\mu(s, \\theta_{\\mu}))^2}{\\sigma(s, \\theta_{\\sigma})^2} \\textbf{x}_{\\sigma}(s) - \\textbf{x}_{\\sigma}(s)\n",
    "\\\\\n",
    "&= \\left[ \\frac{(a - \\mu(s, \\theta_{\\mu}))^2}{\\sigma(s, \\theta_{\\sigma})^2} - 1 \\right] \\textbf{x}_{\\sigma}(s) \\tag{second part proved}\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
