{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "Use your knowledge of the gridworld and its dynamics to determine an exact symbolic expression for the optimal probability of selecting the right action in Example 13.1.\n",
    "\n",
    "**A**\n",
    "\n",
    "Considering the values of each state from left to right being $V_1$ $(V_{S_0}), V_2, V_3$ and $V_G$ $(V_{terminal} = 0)$, and defining them as a function of the other state-values and the probability of choosing the right action, $p = \\pi(right)$, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V_1(p) &= -1 + p V_2 + (1 - p) V_1(p)\n",
    "\\\\\n",
    "V_2(p) &= -1 + p V_1 + (1 - p) V_3(p)\n",
    "\\\\\n",
    "V_3(p) &= -1 + p V_G + (1 - p) V_2(p) = -1 + (1 - p) V_2(p)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The equations above are derived from the general definition of state-values:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V(s) &\\doteq \\sum_a \\pi(a | s) \\sum_{r, s'} p(r, s' | s, a) [r + V(s')] \n",
    "\\\\\n",
    "&= \\sum_a \\pi(a | s) \\sum_{s'} p(s' | s, a) \\cdot [-1 + V(s')] \n",
    "\\\\\n",
    "&= -1 + \\sum_a \\pi(a | s) \\sum_{s'} p(s' | s, a) V(s')\n",
    "\\\\\n",
    "&= -1 + [\\pi(right) \\cdot V(s_{right}')] + [\\pi(left) \\cdot V(s_{left}')]\n",
    "\\\\\n",
    "&= -1 + p V(s_{right}') + (1 - p) V(s_{left}')\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(in the given environment, the reward is always -1 $(r(s, a) = -1)$, and the transitions are deterministic, that is, given a state $s$ and an action $a$, there's exactly 1 possible next state $s'$, so $p(s' | s, a) = 1$)\n",
    "\n",
    "The probability of choosing an action is given by $\\pi(right)$ instead of $\\pi(right | s)$ because, due to the use of function approximation, all the states are seen by the agent as a single state, so the policy will only define probabilities for choosing an action, independently of the actual state, that maximizes the final return (this is the real challenge, otherwise, if the actual states were known by the agent, it could define the best action for the state using a deterministic policy: $\\pi(right | s_{left}) = 1$, $\\pi(left | s_{middle}) = 1$ and $\\pi(right | s_{right}) = 1$).\n",
    "\n",
    "Then, the best probability of choosing the right action is given by:\n",
    "\n",
    "$$\n",
    "\\pi(right) = \\operatorname*{argmax}_p \\begin{cases}\n",
    "  -1 + p V_2(p) + (1 - p) V_1(p) \\\\\n",
    "  -1 + p V_1(p) + (1 - p) V_3(p) \\\\\n",
    "  -1 + (1 - p) V_2(p)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The episodes start at the leftmost state $(S_0)$, so the objective is actually to maximize $V_1 = V_{S_0}$:\n",
    "\n",
    "$$\n",
    "\\pi(right) = \\operatorname*{argmax}_p V_1(p)\n",
    "$$\n",
    "\n",
    "(the value of $V_1$ depends on $V_2$, that depends on $V_3$, so $p$ must be chosen in such a way that the start state is the best possible, which depends on the changes in the other states due to $p$).\n",
    "\n",
    "The above definition is enough to answer this exercise, but to verify the actual probability we can simplify the equations, defining $V_1(p)$ as a function that depends only on $p$, without other state-values, and then find the value of $p$ that maximizes $V_1(p)$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V_3 &= -1 + (1 - p) V_2\n",
    "\\\\\n",
    "V_3 &= -1 + (1 - p)  [-1 + p V_1 + (1 - p) V_3]\n",
    "\\\\\n",
    "V_3 &= -1 + p - 1 + p(1 - p)V_1 + (1 - p)^2 V_3\n",
    "\\\\\n",
    "V_3 &= -1 + p - 1 + p(1 - p)V_1 + (1 - 2p + p^2) V_3\n",
    "\\\\\n",
    "V_3 &= p - 2 + p(1 - p)V_1 + [V_3 - 2pV_3 + p^2 V_3]\n",
    "\\\\\n",
    "2pV_3 - p^2 V_3 &= p(1 - p)V_1 + p - 2\n",
    "\\\\\n",
    "V_3 &= \\frac{p(1 - p)V_1 + p - 2}{2p - p^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V_1 &= -1 + p V_2 + (1 - p) V_1\n",
    "\\\\\n",
    "V_1 &= p V_2 - 1 + V_1 - p V_1\n",
    "\\\\\n",
    "p V_1 &= p V_2 - 1\n",
    "\\\\\n",
    "V_1 &= V_2 - \\frac{1}{p}\n",
    "\\\\\n",
    "V_1 &= -1 + p V_1 + (1 - p) V_3 - \\frac{1}{p}\n",
    "\\\\\n",
    "(1 - p)V_1 &= (1 - p) \\frac{p(1 - p)V_1 + p - 2}{2p - p^2} - \\frac{1}{p} - 1\n",
    "\\\\\n",
    "V_1 &= \\frac{p(1 - p)V_1 + p - 2}{2p - p^2} - \\frac{1}{p(1 - p)} - \\frac{1}{1 - p}\n",
    "\\\\\n",
    "V_1 &= \\frac{p(1 - p)V_1 + p - 2}{p(2 - p)} - \\frac{1}{p(1 - p)} - \\frac{1}{1 - p}\n",
    "\\\\\n",
    "V_1 &= \\frac{(1 - p)[p(1 - p)V_1 + p - 2] - [2 - p] - p[2 - p]}{p(1 - p)(2 - p)}\n",
    "\\\\\n",
    "p(1 - p)(2 - p) V_1 &= p(1 - p)^2V_1 + p(1 - p) - 2(1 - p) - (2 - p) - (2p - p^2)\n",
    "\\\\\n",
    "p(1 - p)[(2 - p) - (1 - p)] V_1 &= p(1 - p) - 2(1 - p) - (2 - p) - (2p - p^2)\n",
    "\\\\\n",
    "p(1 - p)[2 - p - 1 + p] V_1 &= p - p^2 + 2p - 2 + p - 2 + p^2 - 2p\n",
    "\\\\\n",
    "p(1 - p) V_1 &= 2p - 4\n",
    "\\\\\n",
    "V_1 &= \\frac{2p - 4}{p(1 - p)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's consider the function:\n",
    "\n",
    "$$\n",
    "f(p) = \\frac{2p - 4}{p(1 - p)}\n",
    "$$\n",
    "\n",
    "To find the critical points, we need to take the derivative of $f(p)$ with respect to $p$, then set it equal to zero.\n",
    "\n",
    "**Differentiate the numerator and the denominator using the quotient rule:**\n",
    "\n",
    "The quotient rule states that for a function $f(p) = \\frac{g(p)}{h(p)}$, the derivative is:\n",
    "\n",
    "$$\n",
    "f'(p) = \\frac{g'(p)h(p) - g(p)h'(p)}{[h(p)]^2}\n",
    "$$\n",
    "\n",
    "Here, we have:\n",
    "\n",
    "- $g(p) = 2p - 4$\n",
    "- $h(p) = p(1 - p) = p - p^2$\n",
    "\n",
    "Now, let's compute the derivatives:\n",
    "\n",
    "- $g'(p) = 2$\n",
    "- $h'(p) = 1 - 2p$\n",
    "\n",
    "Now, applying the quotient rule:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f'(p) &= \\frac{2(p - p^2) - (2p - 4)(1 - 2p)}{(p - p^2)^2}\n",
    "\\\\\n",
    "&= \\frac{(2p - 2p^2) - (2p - 4p^2 - 4 + 8p)}{(p - p^2)^2}\n",
    "\\\\\n",
    "&= \\frac{2p - 2p^2 - 2p + 4p^2 + 4 - 8p}{(p - p^2)^2}\n",
    "\\\\\n",
    "&= \\frac{2p^2 - 8p + 4}{(p - p^2)^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Set $f'(p) = 0$**\n",
    "\n",
    "To find the critical points, we set the numerator equal to zero:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "2p^2 - 8p + 4 &= 0\n",
    "\\\\\n",
    "p^2 - 4p + 2 &= 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now solve this quadratic equation using the quadratic formula:\n",
    "\n",
    "$$\n",
    "p = \\frac{-(-4) \\pm \\sqrt{(-4)^2 - 4(1)(2)}}{2(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p = \\frac{4 \\pm \\sqrt{16 - 8}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p = \\frac{4 \\pm \\sqrt{8}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p = \\frac{4 \\pm 2\\sqrt{2}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p = 2 \\pm \\sqrt{2}\n",
    "$$\n",
    "\n",
    "Thus, the two possible values of $p$ are:\n",
    "\n",
    "$$\n",
    "p = 2 + \\sqrt{2} \\quad \\text{or} \\quad p = 2 - \\sqrt{2}\n",
    "$$\n",
    "\n",
    "**Analyze the critical points**\n",
    "\n",
    "Since $p$ must be between 0 and 1 (as $p$ is a probability), the only valid solution is:\n",
    "\n",
    "$$\n",
    "p = 2 - \\sqrt{2}\n",
    "$$\n",
    "\n",
    "Thus, the value of $p$ that maximizes $f(p)$ is:\n",
    "\n",
    "$$\n",
    "p = 2 - \\sqrt{2} \\approx 0.5858\n",
    "$$\n",
    "\n",
    "So, the optimal probability of selecting the right action is:\n",
    "\n",
    "$$\n",
    "\\pi(right) = 2 - \\sqrt{2} \\approx 0.5858\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.2\n",
    "\n",
    "**Q**\n",
    "\n",
    "Generalize the box on page 199, the policy gradient theorem (13.5), the proof of the policy gradient theorem (page 325), and the steps leading to the REINFORCE update equation (13.8), so that (13.8) ends up with a factor of $\\gamma^t$ and thus aligns with the general algorithm given in the pseudocode.\n",
    "\n",
    "**A**\n",
    "\n",
    "The policy gradient theorem (13.5) is:\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a q_{\\pi}(s, a) \\nabla \\pi(a | s, \\theta) \\tag{13.5}\n",
    "$$\n",
    "\n",
    "The REINFORCE update equation (13.8) is:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} \\doteq \\theta_t + \\alpha G_t \\frac{\\nabla \\pi(A_t | S_t, \\theta_t)}{\\pi(A_t | S_t, \\theta_t)} \\tag{13.8}\n",
    "$$\n",
    "\n",
    "In the box on page 199, we have:\n",
    "\n",
    "$$\n",
    "\\eta(s) = h(s) + \\sum_{\\overline{s}} \\eta(\\overline{s}) \\sum_a \\pi(a | \\overline{s}) p(s | \\overline{s}, a), \\quad \\text{for all } s \\in \\mathcal{S} \\tag{9.2}\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "\\mu(s) = \\frac{\\eta(s)}{\\sum_{s'} \\eta(s')}, \\quad \\text{for all } s \\in \\mathcal{S} \\tag{9.3}\n",
    "$$\n",
    "\n",
    "The proof of the policy gradient theorem (page 325) is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla v_{\\pi} (s) &= \\nabla \\left[ \\sum_a \\pi(a | s) q_{\\pi} (s, a) \\right] , \\quad \\text{for all } s \\in \\mathcal{S} \\tag{Exercise 3.18}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\nabla q_{\\pi}(s, a) \\right] \\tag{product rule of calculus}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\nabla \\sum_{s', r} p(s', r | s, a)(r + v_{\\pi}(s')) \\right] \\tag{Exercise 3.19 and Equation 3.2}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\sum_{s'} p(s' | s, a) \\nabla v_{\\pi}(s') \\right] \\tag{Eq. 3.4}\n",
    "\\\\\n",
    "&= \\sum_a \\bigg[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\sum_{s'} p(s' | s, a) \\tag{unrolling}\n",
    "\\\\\n",
    "&\\quad \\quad \\quad \\sum_{a'} \\bigg[\\nabla \\pi(a'|s') q_{\\pi}(s', a') + \\pi(a'|s') \\sum_{s''} p(s'' | s', a') \\nabla v_{\\pi}(s'') \\bigg] \\bigg]\n",
    "\\\\\n",
    "&= \\sum_{x \\in \\mathcal{S}} \\sum_{k = 0}^{\\infty} Pr(s \\to x, k, \\pi) \\sum_a \\nabla \\pi(a|x) q_{\\pi}(x, a)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "after repeated unrolling, where $Pr(s \\to x, k, \\pi)$ is the probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\\pi$. It is then immediate that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla J(\\theta) &= \\nabla v_{\\pi}(s_0)\n",
    "\\\\\n",
    "&= \\sum_s \\left( \\sum_{k=0}^{\\infty} Pr(s_0 \\to s, k, \\pi) \\right) \\sum_a \\nabla \\pi(a | s) q_{\\pi}(s, a)\n",
    "\\\\\n",
    "&= \\sum_s \\eta (s) \\sum_a \\nabla \\pi(a|s) q_{\\pi} (s, a) \\tag{box page 199}\n",
    "\\\\\n",
    "&= \\sum_{s'} \\eta (s') \\sum_s \\frac{\\eta (s)}{\\sum_{s'} \\eta (s')} \\sum_a \\nabla \\pi(a|s) q_{\\pi} (s, a)\n",
    "\\\\\n",
    "&= \\sum_{s'} \\eta (s') \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a|s) q_{\\pi} (s, a) \\tag{Eq. 9.3}\n",
    "\\\\\n",
    "&\\propto \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a|s) q_{\\pi} (s, a) \\tag{Q.E.D.}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To include the discount factor $\\gamma$, we change the policy gradient theorem to define $q_{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a)(r + \\gamma v_{\\pi}(s'))$ (instead of $q_{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a)(r + v_{\\pi}(s'))$):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla v_{\\pi} (s) &= \\nabla \\left[ \\sum_a \\pi(a | s) q_{\\pi} (s, a) \\right] , \\quad \\text{for all } s \\in \\mathcal{S} \\tag{Exercise 3.18}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\nabla q_{\\pi}(s, a) \\right] \\tag{product rule of calculus}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\nabla \\sum_{s', r} p(s', r | s, a)(r + \\gamma v_{\\pi}(s')) \\right] \\tag{Exercise 3.19 and Equation 3.2}\n",
    "\\\\\n",
    "&= \\sum_a \\left[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\sum_{s'} p(s' | s, a) \\gamma \\nabla v_{\\pi}(s') \\right] \\tag{Eq. 3.4, $\\gamma$ is constant}\n",
    "\\\\\n",
    "&= \\sum_a \\bigg[\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\sum_{s'} p(s' | s, a) \\gamma \\tag{unrolling}\n",
    "\\\\\n",
    "&\\quad \\quad \\quad \\sum_{a'} \\bigg[\\nabla \\pi(a'|s') q_{\\pi}(s', a') + \\pi(a'|s') \\sum_{s''} p(s'' | s', a') \\gamma \\nabla v_{\\pi}(s'') \\bigg] \\bigg]\n",
    "\\\\\n",
    "&= \\sum_{x \\in \\mathcal{S}} \\sum_{k = 0}^{\\infty} \\left[ Pr(s \\to x, k, \\pi) \\gamma^k \\right] \\sum_a \\nabla \\pi(a|x) q_{\\pi}(x, a)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And then:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla J(\\theta) &= \\nabla v_{\\pi}(s_0)\n",
    "\\\\\n",
    "&= \\sum_s \\left( \\sum_{k=0}^{\\infty} \\left[ Pr(s_0 \\to s, k, \\pi) \\gamma^k \\right] \\right) \\sum_a \\nabla \\pi(a | s, \\theta) q_{\\pi}(s, a)\n",
    "\\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\sum_s \\eta (s) \\gamma^t \\sum_a \\nabla \\pi(a|s, \\theta) q_{\\pi} (s, a) \\right] \\tag{replacing $\\gamma^k$ by the sample $\\gamma^t$ at the time step $t$ in the expectation*}\n",
    "\\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^t \\sum_{s'} \\eta (s') \\sum_s \\frac{\\eta (s)}{\\sum_{s'} \\eta (s')} \\sum_a \\nabla \\pi(a|s, \\theta) q_{\\pi} (s, a) \\right]\n",
    "\\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^t \\sum_{s'} \\eta (s') \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a|s, \\theta) q_{\\pi} (s, a) \\right] \\tag{Eq. 9.3}\n",
    "\\\\\n",
    "&\\propto \\mathbb{E}_{\\pi} \\left[ \\gamma^t \\sum_s \\mu(s) \\sum_a q_{\\pi} (s, a) \\nabla \\pi(a|s, \\theta) \\right]\n",
    "\\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^t \\sum_a \\pi(a|S_t, \\theta) q_{\\pi} (S_t, a) \\frac{\\nabla \\pi(a|S_t, \\theta)}{\\pi(a|S_t, \\theta)} \\right] \\tag{replacing $s$ by the sample $S_t \\sim \\pi$}\n",
    "\\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^t q_{\\pi} (S_t, A_t) \\frac{\\nabla \\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t, \\theta)} \\right] \\tag{replacing $a$ by the sample $A_t \\sim \\pi$}\n",
    "\\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^t G_t \\frac{\\nabla \\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t, \\theta)} \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} \\doteq \\theta_t + \\alpha \\gamma^t G_t \\frac{\\nabla \\pi(A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} = \\theta_t + \\alpha \\gamma^t G_t \\nabla ln \\pi(A_t|S_t, \\theta_t)\n",
    "$$\n",
    "\n",
    "as defined in the pseudocode.\n",
    "\n",
    "*\\*The substitution of $\\sum_{k=0}^{\\infty} Pr(s_0 \\to s, k, \\pi) = \\eta (s)$ in the original is mathematically obscure (considering only the given information), even tough it's understandable intuitively. The discount factor $\\gamma$  to the power of $k$ is intuitively equivalent to $\\gamma^t$ at the time-step $t$, considering that $k$ corresponds to the time-steps (in the iteration) in which the state happened, although it's not completely clear mathematically that $\\sum_{k=0}^{\\infty} \\left[ Pr(s_0 \\to s, k, \\pi) \\gamma^k \\right] = \\eta (s) \\gamma^t$.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
