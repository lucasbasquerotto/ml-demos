{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "Just as the return can be written recursively in terms of the first reward and itself one-step later (3.9), so can the-return. Derive the analogous recursive relationship from (12.2) and (12.1).\n",
    "\n",
    "**A**\n",
    "\n",
    "The return written recursively in terms of the first reward and itself one-step later (3.9):\n",
    "\n",
    "\\begin{align*}\n",
    "G_t &\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ... \\\\\n",
    "&= R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + ...) \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1} \\tag{3.9}\n",
    "\\end{align*}\n",
    "\n",
    "Equation 12.2 is:\n",
    "\n",
    "$$\n",
    "G_t^{\\lambda} \\doteq (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_{t:t+n} \\tag{12.2}\n",
    "$$\n",
    "\n",
    "Equation 12.1 is:\n",
    "\n",
    "$$\n",
    "G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}_{t+n-1}), \\quad 0 \\leq t \\leq T - n \\tag{12.1}\n",
    "$$\n",
    "\n",
    "From 12.1 and considering the reasoning used in 3.9, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} &\\doteq R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}_{t+n-1}) \\\\\n",
    "&= R_{t+1} + \\gamma (R_{t+2} + ... + \\gamma^{n-2} R_{t+n} + \\gamma^{n-1} \\widehat{v}(S_{t+n}, \\textbf{w}_{t+n-1})) \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1:t+n}, \\quad 0 \\leq t \\leq T - n \\tag{a1}\n",
    "\\end{align*}\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t^{\\lambda} &\\doteq (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_{t:t+n} \\\\\n",
    "&= (1 - \\lambda) \\left[ \\lambda^{1-1} G_{t:t+1} + \\sum_{n=2}^{\\infty} \\lambda^{n-1} G_{t:t+n} \\right] \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\sum_{n=2}^{\\infty} \\lambda^{n-1} G_{t:t+n} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{(n+1)-1} G_{t:t+(n+1)} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^n G_{t:t+(n+1)} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\lambda \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_{t:t+(n+1)} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\lambda \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\left(R_{t+1} + \\gamma G_{t+1:t+(n+1)} \\right) \\tag{from a1} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\lambda \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\left(R_{t+1} + \\gamma G_{t+1:(t+1)+n} \\right) \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + \\left[ (1 - \\lambda) \\lambda \\sum_{n=1}^{\\infty} \\lambda^{n-1} R_{t+1} \\right] + \\left[ (1 - \\lambda) \\lambda \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\gamma G_{t+1:(t+1)+n} \\right] \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + \\left[ (1 - \\lambda) \\lambda R_{t+1} \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\right] + \\lambda \\gamma \\left[ (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_{t+1:(t+1)+n} \\right] \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + \\left[ (1 - \\lambda) \\lambda R_{t+1} \\operatorname*{lim}_{k \\to \\infty} \\left( \\frac{1 - \\lambda^k}{1 - \\lambda} \\right) \\right] + \\lambda \\gamma G_{t+1}^{\\lambda} \\tag{from 12.2} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + \\left[ (1 - \\lambda) \\lambda R_{t+1} \\frac{1}{1 - \\lambda} \\right] + \\lambda \\gamma G_{t+1}^{\\lambda} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + \\lambda R_{t+1} + \\lambda \\gamma G_{t+1}^{\\lambda} \\\\\n",
    "&= (1 - \\lambda) [ R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) ] + \\lambda R_{t+1} + \\lambda \\gamma G_{t+1}^{\\lambda} \\\\\n",
    "&= R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) - \\lambda R_{t+1} - \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) + \\lambda R_{t+1} + \\lambda \\gamma G_{t+1}^{\\lambda} \\\\\n",
    "&= R_{t+1} + (1 - \\lambda) \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) + \\lambda \\gamma G_{t+1}^{\\lambda}\n",
    "\\end{align*}\n",
    "\n",
    "with $\\widehat{v}(S_{t+1}, \\textbf{w}_t)$ being the estimated value of the next state ($S_{t+1}$) at time $t$ (this can be calculated right after the action is executed at the time $t$, giving the reward $R_{t+1}$ and next state $S_{t+1}$; $\\textbf{w}_t$ is the weights vector at the time-step $t$).\n",
    "\n",
    "Note that:\n",
    "\n",
    "- $G_t^0 = R_{t+1} + (1 - 0) \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) + 0 \\cdot \\gamma G_{t+1}^0 = R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t)$, which is the target of TD(0).\n",
    "\n",
    "- $G_t^1 = R_{t+1} + (1 - 1) \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) + 1 \\cdot \\gamma G_{t+1}^1 = R_{t+1} + \\gamma G_{t+1}^1 = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 G_{t+2}^1 = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+2} + ... = G_t$, which is the target of Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.2\n",
    "\n",
    "**Q**\n",
    "\n",
    "The parameter $\\lambda$ characterizes how fast the exponential weighting in Figure 12.2 falls off, and thus how far into the future the $\\lambda$-return algorithm looks in determining its update. But a rate factor such as $\\lambda$ is sometimes an awkward way of characterizing the speed of the decay. For some purposes it is better to specify a time constant, or half-life. What is the equation relating and the half-life, $\\tau_{\\lambda}$, the time by which the weighting sequence will have fallen to half of its initial value?\n",
    "\n",
    "**A**\n",
    "\n",
    "Equation 12.2 is:\n",
    "\n",
    "$$\n",
    "G_t^{\\lambda} \\doteq (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_{t:t+n} \\tag{12.2}\n",
    "$$\n",
    "\n",
    "The $n\\text{th}$ weight is given by:\n",
    "\n",
    "$$\n",
    "W_n = (1 - \\lambda) \\lambda^{n-1}\n",
    "$$\n",
    "\n",
    "What we want is to find $n = \\tau_{\\lambda}$ such that $W_{\\tau_{\\lambda}} = \\frac{1}{2} W_1$:\n",
    "\n",
    "\\begin{align*}\n",
    "W_{\\tau_{\\lambda}} &= \\frac{1}{2} W_1 \\\\\n",
    "(1 - \\lambda) \\lambda^{\\tau_{\\lambda}-1} &= \\frac{1}{2} (1 - \\lambda) \\lambda^{1-1} \\\\\n",
    "\\lambda^{\\tau_{\\lambda}-1} &= \\frac{1}{2} \\\\\n",
    "\\tau_{\\lambda} - 1 &= log_{\\lambda} \\frac{1}{2} \\\\\n",
    "\\tau_{\\lambda} &= 1 - log_{\\lambda} 2, \\quad 0 \\lt \\lambda \\lt 1\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.3\n",
    "\n",
    "\n",
    "**Q**\n",
    "\n",
    "Some insight into how TD($\\lambda$) can closely approximate the offline $\\lambda$-return algorithm can be gained by seeing that the latterâ€™s error term (in brackets in (12.4)) can be written as the sum of TD errors (12.6) for a single fixed $\\textbf{w}$. Show this, following the pattern of (6.6), and using the recursive relationship for the $\\lambda$-return you obtained in Exercise 12.1.\n",
    "\n",
    "**A**\n",
    "\n",
    "Equation 12.4 is:\n",
    "\n",
    "$$\n",
    "\\textbf{w}_{t+1} \\doteq \\textbf{w}_t + \\alpha [G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}_t)] \\nabla \\widehat{v}(S_t, \\textbf{w}_t), \\quad t = 0, ..., T - 1 \\tag{12.4}\n",
    "$$\n",
    "\n",
    "The error term in 12.4 is:\n",
    "\n",
    "$$\n",
    "G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}_t)\n",
    "$$\n",
    "\n",
    "Equation 12.6 (TD error) is:\n",
    "\n",
    "$$\n",
    "\\delta_t \\doteq R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) - \\widehat{v}(S_t, \\textbf{w}_t) \\tag{12.6}\n",
    "$$\n",
    "\n",
    "The equation 6.6 is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1}) \\tag{from (3.9)} \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V(S_{t+2})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V(S_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\tag{6.6}\n",
    "\\end{align*}\n",
    "\n",
    "So, for a single fixed $\\textbf{w}$ (so $\\textbf{w}_k = \\textbf{w}$ and $\\delta_t \\doteq R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) - \\widehat{v}(S_t, \\textbf{w})$):\n",
    "\n",
    "\\begin{align*}\n",
    "G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}) &= R_{t+1} + (1 - \\lambda) \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) + \\lambda \\gamma G_{t+1}^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}) + \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) - \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) \\tag{from exercise 12.1} \\\\\n",
    "&= R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) - \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) + \\lambda \\gamma G_{t+1}^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}) + \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) - \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) \\\\\n",
    "&= [R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) - \\widehat{v}(S_t, \\textbf{w})] + [\\lambda \\gamma G_{t+1}^{\\lambda} - \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w})] \\\\\n",
    "&= \\delta_t + \\lambda \\gamma [G_{t+1}^{\\lambda} - \\widehat{v}(S_{t+1}, \\textbf{w})] \\\\\n",
    "&= \\delta_t + \\lambda \\gamma \\delta_{t+1} + \\lambda^2 \\gamma^2 [G_{t+2}^{\\lambda} - \\widehat{v}(S_{t+2}, \\textbf{w})] \\\\\n",
    "&= \\delta_t + \\lambda \\gamma \\delta_{t+1} + \\lambda^2 \\gamma^2 \\delta_{t+2} + ... + \\lambda^{T-t-1} \\gamma^{T-t-1} \\delta_{T-1} + \\lambda^{T-t} \\gamma^{T-t} [G_T^{\\lambda} - \\widehat{v}(S_T, \\textbf{w})] \\\\\n",
    "&= \\delta_t + \\lambda \\gamma \\delta_{t+1} + \\lambda^2 \\gamma^2 \\delta_{t+2} + ... + \\lambda^{T-t-1} \\gamma^{T-t-1} \\delta_{T-1} + \\lambda^{T-t} \\gamma^{T-t} [0 - 0] \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\lambda^{k-t} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.4\n",
    "\n",
    "**Q**\n",
    "\n",
    "Use your result from the preceding exercise to show that, if the weight updates over an episode were computed on each step but not actually used to change the weights ($\\textbf{w}$ remained fixed), then the sum of TD($\\lambda$)â€™s weight updates would be the same as the sum of the offline $\\lambda$-return algorithmâ€™s updates.\n",
    "\n",
    "**A**\n",
    "\n",
    "If $\\textbf{w}$ remained fixed, the result of the previous exercise can be applied, that is:\n",
    "\n",
    "$$\n",
    "G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}) = \\sum_{k=t}^{T-1} \\lambda^{k-t} \\gamma^{k-t} \\delta_k\n",
    "$$\n",
    "\n",
    "The TD($\\lambda$)â€™s weight update for a given time-step $t$ is:\n",
    "\n",
    "$$\n",
    "\\textbf{w}_{t+1} \\doteq \\textbf{w}_t + \\alpha \\delta_t \\textbf{z}_t \\tag{12.7}\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{z}_{-1} &\\doteq \\textbf{0}, \\\\\n",
    "\\textbf{z}_t &\\doteq \\gamma \\lambda \\textbf{z}_{t-1} + \\nabla \\widehat{v}(S_t, \\textbf{w}_t), \\quad 0 \\leq t \\leq T \\tag{12.5}\n",
    "\\end{align*}\n",
    "\n",
    "The definition of $\\textbf{z}_t$ can be extended to:\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{z}_t &\\doteq \\gamma \\lambda \\textbf{z}_{t-1} + \\nabla \\widehat{v}(S_t, \\textbf{w}_t) \\\\\n",
    "&= \\gamma^{t+1} \\lambda^{t+1} \\textbf{z}_{-1} + \\gamma^t \\lambda^t \\nabla \\widehat{v}(S_0, \\textbf{w}_0) + ... + \\gamma \\lambda \\nabla \\widehat{v}(S_{t-1}, \\textbf{w}_{t-1}) + \\nabla \\widehat{v}(S_t, \\textbf{w}_t) \\\\\n",
    "&= \\gamma^t \\lambda^t \\nabla \\widehat{v}(S_0, \\textbf{w}_0) + ... + \\gamma \\lambda \\nabla \\widehat{v}(S_{t-1}, \\textbf{w}_{t-1}) + \\nabla \\widehat{v}(S_t, \\textbf{w}_t) \\\\\n",
    "&= \\sum_{k=0}^t \\gamma^{t-k} \\lambda^{t-k} \\nabla \\widehat{v}(S_k, \\textbf{w}_k) \\tag{a2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The offline $\\lambda$-return algorithmâ€™s update is:\n",
    "\n",
    "$$\n",
    "\\textbf{w}_{t+1} \\doteq \\textbf{w}_t + \\alpha [G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}_t)] \\nabla \\widehat{v}(S_t, \\textbf{w}_t), \\quad t = 0, ..., T - 1 \\tag{12.4}\n",
    "$$\n",
    "\n",
    "The sum of the offline $\\lambda$-return algorithmâ€™s updates is:\n",
    "\n",
    "$$\n",
    "\\sum_{t = 0}^{T-1} [\\textbf{w}_{t+1} - \\textbf{w}_t] = \\sum_{t = 0}^{T-1} \\alpha [G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}_t)] \\nabla \\widehat{v}(S_t, \\textbf{w}_t)\n",
    "$$\n",
    "\n",
    "The sum of the TD($\\lambda$)â€™s weight updates is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{t = 0}^{T-1} [\\textbf{w}_{t+1} - \\textbf{w}_t] &= \\sum_{t = 0}^{T-1} \\alpha \\delta_t \\textbf{z}_t \\\\\n",
    "&= \\sum_{t = 0}^{T-1} \\alpha \\delta_t \\sum_{k=0}^t \\gamma^{t-k} \\lambda^{t-k} \\nabla \\widehat{v}(S_k, \\textbf{w}) \\tag{from a2} \\\\\n",
    "&= \\sum_{t = 0}^{T-1} \\alpha \\nabla \\widehat{v}(S_t, \\textbf{w}) \\sum_{k=t}^{T-1} \\gamma^{k-t} \\lambda^{k-t} \\delta_k \\tag{a3, explained below} \\\\\n",
    "&= \\sum_{t = 0}^{T-1} \\alpha [G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w})] \\nabla \\widehat{v}(S_t, \\textbf{w}) \\tag{from exercise 12.3} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This proves that as long as the weights aren't updated during the episode, the sum of the TD($\\lambda$)â€™s weight updates would be the same as the sum of the offline $\\lambda$-return algorithmâ€™s updates.\n",
    "\n",
    "*Note:* For any functions $f$, $g$ and $h$ that accept one, one and two integer parameters in $[0, T - 1]$, respectively, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{t = 0}^{T-1} f(t) \\sum_{k=0}^t g(k) h(t, k) &= \\left[f(0) \\sum_{k=0}^0 g(k) h(0, k) \\right] + \\left[f(1) \\sum_{k=0}^1 g(k) h(1, k)\\right] + ... + \\left[f(T-1) \\sum_{k=0}^{T-1} g(k) h(T-1, k)\\right] \\\\\n",
    "&= [(f(0) g(0) h(0, 0))] \\\\\n",
    "&+ [(f(1) g(0) h(1, 0)) + (f(1) g(1) h(1, 1))] \\\\\n",
    "&+ ... \\\\\n",
    "&+ [(f(T-1) g(0) h(T-1, 0)) + (f(T-1) g(1) h(T-1, 1)) + ... + (f(T-1) g(T-1) h(T-1, T-1))] \\\\\n",
    "&= g(0) [(f(0) h(0, 0)) + (f(1) h(1, 0)) + ... + (f(T-1) h(T-1, 0))] \\\\\n",
    "&+ g(1) [(f(1) h(1, 1)) + (f(2) h(2, 1)) + ... + (f(T-1) h(T-1, 1))] \\\\\n",
    "&+ ... \\\\\n",
    "&+ g(T-1) [(f(T-1) h(T-1, T-1))] \\\\\n",
    "&= \\left[g(0) \\sum_{k=0}^{T-1} f(k) h(k, 0) \\right] + \\left[g(1) \\sum_{k=1}^{T-1} f(k) h(k, 1)\\right] + ... + \\left[g(T-1) \\sum_{k=T-1}^{T-1} f(k) h(k, T-1)\\right] \\\\\n",
    "&= \\sum_{t = 0}^{T-1} g(t) \\sum_{k=t}^{T-1} f(k) h(k, t)\n",
    "\\end{align*}\n",
    "\n",
    "Making $f(t) = \\alpha \\delta_t$, $g(k) = \\nabla \\widehat{v}(S_k, \\textbf{w})$ and $h(t, k) = \\gamma^{t-k} \\lambda^{t-k}$ we have:\n",
    "\n",
    "$$\n",
    "\\sum_{t = 0}^{T-1} \\alpha \\delta_t \\sum_{k=0}^t \\gamma^{t-k} \\lambda^{t-k} \\nabla \\widehat{v}(S_k, \\textbf{w}) = \\sum_{t = 0}^{T-1} \\alpha \\nabla \\widehat{v}(S_t, \\textbf{w}) \\sum_{k=t}^{T-1} \\gamma^{k-t} \\lambda^{k-t} \\delta_k\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
