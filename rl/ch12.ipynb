{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "Just as the return can be written recursively in terms of the first reward and itself one-step later (3.9), so can the-return. Derive the analogous recursive relationship from (12.2) and (12.1).\n",
    "\n",
    "**A**\n",
    "\n",
    "The return written recursively in terms of the first reward and itself one-step later (3.9):\n",
    "\n",
    "\\begin{align*}\n",
    "G_t &\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ... \\\\\n",
    "&= R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + ...) \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1} \\tag{3.9}\n",
    "\\end{align*}\n",
    "\n",
    "Equation 12.2 is:\n",
    "\n",
    "$$\n",
    "G_t^{\\lambda} \\doteq (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_{t:t+n} \\tag{12.2}\n",
    "$$\n",
    "\n",
    "Equation 12.1 is:\n",
    "\n",
    "$$\n",
    "G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}_{t+n-1}), \\quad 0 \\leq t \\leq T - n \\tag{12.1}\n",
    "$$\n",
    "\n",
    "From 12.1 and considering the reasoning used in 3.9, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} &\\doteq R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}_{t+n-1}) \\\\\n",
    "&= R_{t+1} + \\gamma (R_{t+2} + ... + \\gamma^{n-2} R_{t+n} + \\gamma^{n-1} \\widehat{v}(S_{t+n}, \\textbf{w}_{t+n-1})) \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1:t+n}, \\quad 0 \\leq t \\leq T - n \\tag{a1}\n",
    "\\end{align*}\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t^{\\lambda} &\\doteq (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_{t:t+n} \\\\\n",
    "&= (1 - \\lambda) \\left[ \\lambda^{1-1} G_{t:t+1} + \\sum_{n=2}^{\\infty} \\lambda^{n-1} G_{t:t+n} \\right] \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\sum_{n=2}^{\\infty} \\lambda^{n-1} G_{t:t+n} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{(n+1)-1} G_{t:t+(n+1)} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^n G_{t:t+(n+1)} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\lambda \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_{t:t+(n+1)} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\lambda \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\left(R_{t+1} + \\gamma G_{t+1:t+(n+1)} \\right) \\tag{from a1} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + (1 - \\lambda) \\lambda \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\left(R_{t+1} + \\gamma G_{t+1:(t+1)+n} \\right) \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + \\left[ (1 - \\lambda) \\lambda \\sum_{n=1}^{\\infty} \\lambda^{n-1} R_{t+1} \\right] + \\left[ (1 - \\lambda) \\lambda \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\gamma G_{t+1:(t+1)+n} \\right] \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + \\left[ (1 - \\lambda) \\lambda R_{t+1} \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\right] + \\lambda \\gamma \\left[ (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_{t+1:(t+1)+n} \\right] \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + \\left[ (1 - \\lambda) \\lambda R_{t+1} \\operatorname*{lim}_{k \\to \\infty} \\left( \\frac{1 - \\lambda^k}{1 - \\lambda} \\right) \\right] + \\lambda \\gamma G_{t+1}^{\\lambda} \\tag{from 12.2} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + \\left[ (1 - \\lambda) \\lambda R_{t+1} \\frac{1}{1 - \\lambda} \\right] + \\lambda \\gamma G_{t+1}^{\\lambda} \\\\\n",
    "&= (1 - \\lambda) G_{t:t+1} + \\lambda R_{t+1} + \\lambda \\gamma G_{t+1}^{\\lambda} \\\\\n",
    "&= (1 - \\lambda) [ R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) ] + \\lambda R_{t+1} + \\lambda \\gamma G_{t+1}^{\\lambda} \\\\\n",
    "&= R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) - \\lambda R_{t+1} - \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) + \\lambda R_{t+1} + \\lambda \\gamma G_{t+1}^{\\lambda} \\\\\n",
    "&= R_{t+1} + (1 - \\lambda) \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) + \\lambda \\gamma G_{t+1}^{\\lambda}\n",
    "\\end{align*}\n",
    "\n",
    "with $\\widehat{v}(S_{t+1}, \\textbf{w}_t)$ being the estimated value of the next state ($S_{t+1}$) at time $t$ (this can be calculated right after the action is executed at the time $t$, giving the reward $R_{t+1}$ and next state $S_{t+1}$; $\\textbf{w}_t$ is the weights vector at the time-step $t$).\n",
    "\n",
    "Note that:\n",
    "\n",
    "- $G_t^0 = R_{t+1} + (1 - 0) \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) + 0 \\cdot \\gamma G_{t+1}^0 = R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t)$, which is the target of TD(0).\n",
    "\n",
    "- $G_t^1 = R_{t+1} + (1 - 1) \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) + 1 \\cdot \\gamma G_{t+1}^1 = R_{t+1} + \\gamma G_{t+1}^1 = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 G_{t+2}^1 = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+2} + ... = G_t$, which is the target of Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.2\n",
    "\n",
    "**Q**\n",
    "\n",
    "The parameter $\\lambda$ characterizes how fast the exponential weighting in Figure 12.2 falls off, and thus how far into the future the $\\lambda$-return algorithm looks in determining its update. But a rate factor such as $\\lambda$ is sometimes an awkward way of characterizing the speed of the decay. For some purposes it is better to specify a time constant, or half-life. What is the equation relating and the half-life, $\\tau_{\\lambda}$, the time by which the weighting sequence will have fallen to half of its initial value?\n",
    "\n",
    "**A**\n",
    "\n",
    "Equation 12.2 is:\n",
    "\n",
    "$$\n",
    "G_t^{\\lambda} \\doteq (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_{t:t+n} \\tag{12.2}\n",
    "$$\n",
    "\n",
    "The $n\\text{th}$ weight is given by:\n",
    "\n",
    "$$\n",
    "W_n = (1 - \\lambda) \\lambda^{n-1}\n",
    "$$\n",
    "\n",
    "What we want is to find $n = \\tau_{\\lambda}$ such that $W_{\\tau_{\\lambda}} = \\frac{1}{2} W_1$:\n",
    "\n",
    "\\begin{align*}\n",
    "W_{\\tau_{\\lambda}} &= \\frac{1}{2} W_1 \\\\\n",
    "(1 - \\lambda) \\lambda^{\\tau_{\\lambda}-1} &= \\frac{1}{2} (1 - \\lambda) \\lambda^{1-1} \\\\\n",
    "\\lambda^{\\tau_{\\lambda}-1} &= \\frac{1}{2} \\\\\n",
    "\\tau_{\\lambda} - 1 &= log_{\\lambda} \\frac{1}{2} \\\\\n",
    "\\tau_{\\lambda} &= 1 - log_{\\lambda} 2, \\quad 0 \\lt \\lambda \\lt 1\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.3\n",
    "\n",
    "\n",
    "**Q**\n",
    "\n",
    "Some insight into how TD($\\lambda$) can closely approximate the offline $\\lambda$-return algorithm can be gained by seeing that the latter’s error term (in brackets in (12.4)) can be written as the sum of TD errors (12.6) for a single fixed $\\textbf{w}$. Show this, following the pattern of (6.6), and using the recursive relationship for the $\\lambda$-return you obtained in Exercise 12.1.\n",
    "\n",
    "**A**\n",
    "\n",
    "Equation 12.4 is:\n",
    "\n",
    "$$\n",
    "\\textbf{w}_{t+1} \\doteq \\textbf{w}_t + \\alpha [G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}_t)] \\nabla \\widehat{v}(S_t, \\textbf{w}_t), \\quad t = 0, ..., T - 1 \\tag{12.4}\n",
    "$$\n",
    "\n",
    "The error term in 12.4 is:\n",
    "\n",
    "$$\n",
    "G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}_t)\n",
    "$$\n",
    "\n",
    "Equation 12.6 (TD error) is:\n",
    "\n",
    "$$\n",
    "\\delta_t \\doteq R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) - \\widehat{v}(S_t, \\textbf{w}_t) \\tag{12.6}\n",
    "$$\n",
    "\n",
    "The equation 6.6 is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1}) \\tag{from (3.9)} \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V(S_{t+2})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V(S_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\tag{6.6}\n",
    "\\end{align*}\n",
    "\n",
    "So, for a single fixed $\\textbf{w}$ (so $\\textbf{w}_k = \\textbf{w}$ and $\\delta_t \\doteq R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) - \\widehat{v}(S_t, \\textbf{w})$):\n",
    "\n",
    "\\begin{align*}\n",
    "G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}) &= R_{t+1} + (1 - \\lambda) \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) + \\lambda \\gamma G_{t+1}^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}) + \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) - \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) \\tag{from exercise 12.1} \\\\\n",
    "&= R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) - \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) + \\lambda \\gamma G_{t+1}^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}) + \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) - \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) \\\\\n",
    "&= [R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}) - \\widehat{v}(S_t, \\textbf{w})] + [\\lambda \\gamma G_{t+1}^{\\lambda} - \\lambda \\gamma \\widehat{v}(S_{t+1}, \\textbf{w})] \\\\\n",
    "&= \\delta_t + \\lambda \\gamma [G_{t+1}^{\\lambda} - \\widehat{v}(S_{t+1}, \\textbf{w})] \\\\\n",
    "&= \\delta_t + \\lambda \\gamma \\delta_{t+1} + \\lambda^2 \\gamma^2 [G_{t+2}^{\\lambda} - \\widehat{v}(S_{t+2}, \\textbf{w})] \\\\\n",
    "&= \\delta_t + \\lambda \\gamma \\delta_{t+1} + \\lambda^2 \\gamma^2 \\delta_{t+2} + ... + \\lambda^{T-t-1} \\gamma^{T-t-1} \\delta_{T-1} + \\lambda^{T-t} \\gamma^{T-t} [G_T^{\\lambda} - \\widehat{v}(S_T, \\textbf{w})] \\\\\n",
    "&= \\delta_t + \\lambda \\gamma \\delta_{t+1} + \\lambda^2 \\gamma^2 \\delta_{t+2} + ... + \\lambda^{T-t-1} \\gamma^{T-t-1} \\delta_{T-1} + \\lambda^{T-t} \\gamma^{T-t} [0 - 0] \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\lambda^{k-t} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.4\n",
    "\n",
    "**Q**\n",
    "\n",
    "Use your result from the preceding exercise to show that, if the weight updates over an episode were computed on each step but not actually used to change the weights ($\\textbf{w}$ remained fixed), then the sum of TD($\\lambda$)’s weight updates would be the same as the sum of the offline $\\lambda$-return algorithm’s updates.\n",
    "\n",
    "**A**\n",
    "\n",
    "If $\\textbf{w}$ remained fixed, the result of the previous exercise can be applied, that is:\n",
    "\n",
    "$$\n",
    "G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}) = \\sum_{k=t}^{T-1} \\lambda^{k-t} \\gamma^{k-t} \\delta_k\n",
    "$$\n",
    "\n",
    "The TD($\\lambda$)’s weight update for a given time-step $t$ is:\n",
    "\n",
    "$$\n",
    "\\textbf{w}_{t+1} \\doteq \\textbf{w}_t + \\alpha \\delta_t \\textbf{z}_t \\tag{12.7}\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{z}_{-1} &\\doteq \\textbf{0}, \\\\\n",
    "\\textbf{z}_t &\\doteq \\gamma \\lambda \\textbf{z}_{t-1} + \\nabla \\widehat{v}(S_t, \\textbf{w}_t), \\quad 0 \\leq t \\leq T \\tag{12.5}\n",
    "\\end{align*}\n",
    "\n",
    "The definition of $\\textbf{z}_t$ can be extended to:\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{z}_t &\\doteq \\gamma \\lambda \\textbf{z}_{t-1} + \\nabla \\widehat{v}(S_t, \\textbf{w}_t) \\\\\n",
    "&= \\gamma^{t+1} \\lambda^{t+1} \\textbf{z}_{-1} + \\gamma^t \\lambda^t \\nabla \\widehat{v}(S_0, \\textbf{w}_0) + ... + \\gamma \\lambda \\nabla \\widehat{v}(S_{t-1}, \\textbf{w}_{t-1}) + \\nabla \\widehat{v}(S_t, \\textbf{w}_t) \\\\\n",
    "&= \\gamma^t \\lambda^t \\nabla \\widehat{v}(S_0, \\textbf{w}_0) + ... + \\gamma \\lambda \\nabla \\widehat{v}(S_{t-1}, \\textbf{w}_{t-1}) + \\nabla \\widehat{v}(S_t, \\textbf{w}_t) \\\\\n",
    "&= \\sum_{k=0}^t \\gamma^{t-k} \\lambda^{t-k} \\nabla \\widehat{v}(S_k, \\textbf{w}_k) \\tag{a2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The offline $\\lambda$-return algorithm’s update is:\n",
    "\n",
    "$$\n",
    "\\textbf{w}_{t+1} \\doteq \\textbf{w}_t + \\alpha [G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}_t)] \\nabla \\widehat{v}(S_t, \\textbf{w}_t), \\quad t = 0, ..., T - 1 \\tag{12.4}\n",
    "$$\n",
    "\n",
    "The sum of the offline $\\lambda$-return algorithm’s updates is:\n",
    "\n",
    "$$\n",
    "\\sum_{t = 0}^{T-1} [\\textbf{w}_{t+1} - \\textbf{w}_t] = \\sum_{t = 0}^{T-1} \\alpha [G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w}_t)] \\nabla \\widehat{v}(S_t, \\textbf{w}_t)\n",
    "$$\n",
    "\n",
    "The sum of the TD($\\lambda$)’s weight updates is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{t = 0}^{T-1} [\\textbf{w}_{t+1} - \\textbf{w}_t] &= \\sum_{t = 0}^{T-1} \\alpha \\delta_t \\textbf{z}_t \\\\\n",
    "&= \\sum_{t = 0}^{T-1} \\alpha \\delta_t \\sum_{k=0}^t \\gamma^{t-k} \\lambda^{t-k} \\nabla \\widehat{v}(S_k, \\textbf{w}) \\tag{from a2} \\\\\n",
    "&= \\sum_{t = 0}^{T-1} \\alpha \\nabla \\widehat{v}(S_t, \\textbf{w}) \\sum_{k=t}^{T-1} \\gamma^{k-t} \\lambda^{k-t} \\delta_k \\tag{a3, explained below} \\\\\n",
    "&= \\sum_{t = 0}^{T-1} \\alpha [G_t^{\\lambda} - \\widehat{v}(S_t, \\textbf{w})] \\nabla \\widehat{v}(S_t, \\textbf{w}) \\tag{from exercise 12.3} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This proves that as long as the weights aren't updated during the episode, the sum of the TD($\\lambda$)’s weight updates would be the same as the sum of the offline $\\lambda$-return algorithm’s updates.\n",
    "\n",
    "*Note:* For any functions $f$, $g$ and $h$ that accept one, one and two integer parameters in $[0, T - 1]$, respectively, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{t = 0}^{T-1} f(t) \\sum_{k=0}^t g(k) h(t, k) &= \\left[f(0) \\sum_{k=0}^0 g(k) h(0, k) \\right] + \\left[f(1) \\sum_{k=0}^1 g(k) h(1, k)\\right] + ... + \\left[f(T-1) \\sum_{k=0}^{T-1} g(k) h(T-1, k)\\right] \\\\\n",
    "&= [(f(0) g(0) h(0, 0))] \\\\\n",
    "&+ [(f(1) g(0) h(1, 0)) + (f(1) g(1) h(1, 1))] \\\\\n",
    "&+ ... \\\\\n",
    "&+ [(f(T-1) g(0) h(T-1, 0)) + (f(T-1) g(1) h(T-1, 1)) + ... + (f(T-1) g(T-1) h(T-1, T-1))] \\\\\n",
    "&= g(0) [(f(0) h(0, 0)) + (f(1) h(1, 0)) + ... + (f(T-1) h(T-1, 0))] \\\\\n",
    "&+ g(1) [(f(1) h(1, 1)) + (f(2) h(2, 1)) + ... + (f(T-1) h(T-1, 1))] \\\\\n",
    "&+ ... \\\\\n",
    "&+ g(T-1) [(f(T-1) h(T-1, T-1))] \\\\\n",
    "&= \\left[g(0) \\sum_{k=0}^{T-1} f(k) h(k, 0) \\right] + \\left[g(1) \\sum_{k=1}^{T-1} f(k) h(k, 1)\\right] + ... + \\left[g(T-1) \\sum_{k=T-1}^{T-1} f(k) h(k, T-1)\\right] \\\\\n",
    "&= \\sum_{t = 0}^{T-1} g(t) \\sum_{k=t}^{T-1} f(k) h(k, t)\n",
    "\\end{align*}\n",
    "\n",
    "Making $f(t) = \\alpha \\delta_t$, $g(k) = \\nabla \\widehat{v}(S_k, \\textbf{w})$ and $h(t, k) = \\gamma^{t-k} \\lambda^{t-k}$ we have:\n",
    "\n",
    "$$\n",
    "\\sum_{t = 0}^{T-1} \\alpha \\delta_t \\sum_{k=0}^t \\gamma^{t-k} \\lambda^{t-k} \\nabla \\widehat{v}(S_k, \\textbf{w}) = \\sum_{t = 0}^{T-1} \\alpha \\nabla \\widehat{v}(S_t, \\textbf{w}) \\sum_{k=t}^{T-1} \\gamma^{k-t} \\lambda^{k-t} \\delta_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.5\n",
    "\n",
    "**Q**\n",
    "\n",
    "Several times in this book (often in exercises) we have established that returns can be written as sums of TD errors if the value function is held constant. Why is (12.10) another instance of this? Prove (12.10).\n",
    "\n",
    "**A**\n",
    "\n",
    "Equation 12.10 is:\n",
    "\n",
    "$$\n",
    "G_{t:t+k}^{\\lambda} = \\widehat{v}(S_t, \\textbf{w}_{t-1}) + \\sum_{i=t}^{t+k-1} (\\gamma \\lambda)^{i-t} \\delta_i' \\tag{12.10}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\delta_t' \\doteq R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\textbf{w}_t) - \\widehat{v}(S_t, \\textbf{w}_{t-1})\n",
    "$$\n",
    "\n",
    "The original equation, 12.9, is:\n",
    "\n",
    "$$\n",
    "G_{t:h}^{\\lambda} \\doteq (1 - \\lambda) \\sum_{n=1}^{h-t-1} \\lambda^{n-1} G_{t:t+n} + \\lambda^{h-t-1} G_{t:h}, \\quad 0 \\leq t \\lt h \\leq T \\tag{12.9}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}_{t+n-1}), \\quad 0 \\leq t \\leq T - n \\tag{12.1}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "G_{t:t+n} \\doteq \\sum_{k=0}^{n-1} \\gamma^k R_{t+k+1} + \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}_{t+n-1}), \\quad 0 \\leq t \\leq T - n\n",
    "$$\n",
    "\n",
    "Also, consider the following:\n",
    "\n",
    "$$\n",
    "(1 - \\lambda) \\sum_{n=a}^b \\lambda^{n-1} = (1 - \\lambda) \\frac{\\lambda^{a-1} - \\lambda^b}{1 - \\lambda} = \\lambda^{a-1} - \\lambda^b \\tag{a4}\n",
    "$$\n",
    "\n",
    "If the value function is held constant, then $\\textbf{w}_t = \\textbf{w}$ for any $t$, so, starting from equation 12.9, we have (making $h = t + k$):\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+k}^{\\lambda} &\\doteq (1 - \\lambda) \\sum_{n=1}^{k-1} \\lambda^{n-1} G_{t:t+n} + \\lambda^{k-1} G_{t:t+k} \\\\\n",
    "\n",
    "&= (1 - \\lambda) \\sum_{n=1}^{k-1} \\lambda^{n-1} \\left[ \\sum_{q=0}^{n-1} \\gamma^q R_{t+q+1} + \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] + \\lambda^{k-1} \\left[ \\sum_{q=0}^{k-1} \\gamma^q R_{t+q+1} + \\gamma^k \\widehat{v}(S_{t+k}, \\textbf{w}) \\right] \\\\\n",
    "\n",
    "&= (1 - \\lambda) \\sum_{n=1}^{k-1} \\lambda^{n-1} \\left[ \\sum_{q=0}^{n-1} \\gamma^q R_{t+q+1} \\right] + (1 - \\lambda) \\sum_{n=1}^{k-1} \\lambda^{n-1} \\left[ \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] + \\lambda^{k-1} \\left[ \\sum_{q=0}^{k-1} \\gamma^q R_{t+q+1} \\right] + \\lambda^{k-1} \\left[ \\gamma^k \\widehat{v}(S_{t+k}, \\textbf{w}) \\right] \\\\\n",
    "\n",
    "&= (1 - \\lambda) \\sum_{q=0}^{k-2} \\left[ \\sum_{n=q+1}^{k-1} \\lambda^{n-1} \\right] \\gamma^q R_{t+q+1} + (1 - \\lambda) \\sum_{n=1}^{k-1} \\lambda^{n-1} \\left[ \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] + \\lambda^{k-1} \\left[ \\sum_{q=0}^{k-1} \\gamma^q R_{t+q+1} \\right] + \\lambda^{k-1} \\left[ \\gamma^k \\widehat{v}(S_{t+k}, \\textbf{w}) \\right] \\\\\n",
    "\n",
    "&= \\left[ \\left( \\sum_{q=0}^{k-2} \\left[ (1 - \\lambda) \\sum_{n=q+1}^{k-1} \\lambda^{n-1} \\right] \\gamma^q R_{t+q+1} \\right) + \\left( \\lambda^{k-1} \\left[ \\sum_{q=0}^{k-1} \\gamma^q R_{t+q+1} \\right] \\right) \\right] + (1 - \\lambda) \\sum_{n=1}^{k-1} \\lambda^{n-1} \\left[ \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] + \\lambda^{k-1} \\left[ \\gamma^k \\widehat{v}(S_{t+k}, \\textbf{w}) \\right] \\\\\n",
    "\n",
    "&= \\left[ \\left( \\sum_{q=0}^{k-2} \\left[ \\lambda^q - \\lambda^{k-1} \\right] \\gamma^q R_{t+q+1} \\right) + \\left( \\lambda^{k-1} \\left[ \\sum_{q=0}^{k-2} \\gamma^q R_{t+q+1} \\right] + \\lambda^{k-1} \\gamma^{k-1} R_{t+k} \\right) \\right] + (1 - \\lambda) \\sum_{n=1}^{k-1} \\lambda^{n-1} \\left[ \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] + \\lambda^{k-1} \\left[ \\gamma^k \\widehat{v}(S_{t+k}, \\textbf{w}) \\right] \\tag{from a4} \\\\\n",
    "\n",
    "&= \\left[ \\left( \\sum_{q=0}^{k-2} \\lambda^q \\gamma^q R_{t+q+1} \\right) - \\left( \\sum_{q=0}^{k-2} \\lambda^{k-1} \\gamma^q R_{t+q+1} \\right) + \\left( \\sum_{q=0}^{k-2} \\lambda^{k-1} \\gamma^q R_{t+q+1} \\right) + \\left( \\lambda^{k-1} \\gamma^{k-1} R_{t+k} \\right) \\right] + (1 - \\lambda) \\sum_{n=1}^{k-1} \\lambda^{n-1} \\left[ \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] + \\lambda^{k-1} \\left[ \\gamma^k \\widehat{v}(S_{t+k}, \\textbf{w}) \\right] \\\\\n",
    "\n",
    "&= \\left[ \\left( \\sum_{q=0}^{k-2} \\lambda^q \\gamma^q R_{t+q+1} \\right) + \\left( \\lambda^{k-1} \\gamma^{k-1} R_{t+k} \\right) \\right] + \\left[ (1 - \\lambda) \\sum_{n=1}^{k-1} \\lambda^{n-1} \\left[ \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] + \\lambda^{k-1} \\left[ \\gamma^k \\widehat{v}(S_{t+k}, \\textbf{w}) \\right] \\right] \\\\\n",
    "\n",
    "&= \\left[ \\sum_{q=0}^{k-1} \\lambda^q \\gamma^q R_{t+q+1} \\right] + \\left[ \\left( \\sum_{n=1}^{k-1} \\lambda^{n-1} \\left[ \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] \\right) - \\left( \\lambda \\sum_{n=1}^{k-1} \\lambda^{n-1} \\left[ \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] \\right) + \\left( \\lambda^{k-1} \\left[ \\gamma^k \\widehat{v}(S_{t+k}, \\textbf{w}) \\right] \\right) \\right] \\\\\n",
    "\n",
    "&= \\left[ \\sum_{n=1}^k \\lambda^{n-1} \\gamma^{n-1} R_{t+n} \\right] + \\left[ \\sum_{n=1}^k \\lambda^{n-1} \\left[ \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] \\right] - \\left[ \\lambda \\sum_{n=1}^{k-1} \\lambda^{n-1} \\left[ \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] \\right] \\\\\n",
    "\n",
    "&= \\left[ \\sum_{n=1}^k \\lambda^{n-1} \\gamma^{n-1} [R_{t+n} + \\gamma \\widehat{v}(S_{t+n}, \\textbf{w})] \\right] - \\left[ \\left( \\sum_{n=1}^{k-1} \\lambda^n \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right) + (\\lambda^0 \\gamma^0 \\widehat{v}(S_{t+0}, \\textbf{w})) - (\\lambda^0 \\gamma^0 \\widehat{v}(S_{t+0}, \\textbf{w})) \\right] \\\\\n",
    "\n",
    "&= \\left[ \\sum_{n=1}^k \\lambda^{n-1} \\gamma^{n-1} [R_{t+n} + \\gamma \\widehat{v}(S_{t+n}, \\textbf{w})] \\right] - \\left[ \\left( \\sum_{n=0}^{k-1} \\lambda^n \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right) - (\\widehat{v}(S_t, \\textbf{w})) \\right] \\\\\n",
    "\n",
    "&= \\widehat{v}(S_t, \\textbf{w}) + \\left[ \\sum_{n=1}^k \\lambda^{n-1} \\gamma^{n-1} [R_{t+n} + \\gamma \\widehat{v}(S_{t+n}, \\textbf{w})] \\right] - \\left[ \\sum_{n=0}^{k-1} \\lambda^n \\gamma^n \\widehat{v}(S_{t+n}, \\textbf{w}) \\right] \\\\\n",
    "\n",
    "&= \\widehat{v}(S_t, \\textbf{w}) + \\left[ \\sum_{n=1}^k \\lambda^{n-1} \\gamma^{n-1} [R_{t+n} + \\gamma \\widehat{v}(S_{t+n}, \\textbf{w})] \\right] - \\left[ \\sum_{n=1}^k \\lambda^{n-1} \\gamma^{n-1} \\widehat{v}(S_{t+n-1}, \\textbf{w}) \\right] \\\\\n",
    "\n",
    "&= \\widehat{v}(S_t, \\textbf{w}) + \\left[ \\sum_{n=1}^k \\lambda^{n-1} \\gamma^{n-1} [R_{t+n} + \\gamma \\widehat{v}(S_{t+n}, \\textbf{w}) - \\widehat{v}(S_{t+n-1}, \\textbf{w})] \\right] \\\\\n",
    "\n",
    "&= \\widehat{v}(S_t, \\textbf{w}) + \\left[ \\sum_{n=1}^k \\lambda^{n-1} \\gamma^{n-1} \\delta_{t+n-1}' \\right]\n",
    "\\end{align*}\n",
    "\n",
    "Readjusting the indices, from $t$ to $t+k-1$, replacing the occurences of $n$ by $i-t+1$ (the same corresponding $k$ terms):\n",
    "\n",
    "$$\n",
    "G_{t:t+k}^{\\lambda} = \\widehat{v}(S_t, \\textbf{w}) + \\left[ \\sum_{i=t}^{t+k-1} \\lambda^{i-t} \\gamma^{i-t} \\delta_i' \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.6\n",
    "\n",
    "**Q**\n",
    "\n",
    "Modify the pseudocode for Sarsa($\\lambda$) to use dutch traces (12.11) without the other distinctive features of a true online algorithm. Assume linear function approximation and binary features.\n",
    "\n",
    "**A**\n",
    "\n",
    "The True Online TD pseudocode provided is:\n",
    "\n",
    "> Input: the policy $\\pi$ to be evaluated<br/>\n",
    "> Input: a feature function $\\textbf{x} : \\mathcal{S}^+ \\to \\mathbb{R}^d$ such that $\\textbf{x}(terminal, \\cdot) = \\textbf{0}$<br/>\n",
    "> Algorithm parameters: step size $\\alpha > 0$, trace decay rate $\\lambda \\in [0, 1]$<br/>\n",
    "> Initialize value-function weights $\\textbf{w} \\in \\mathbb{R}^d$ (e.g., $\\textbf{w} = \\textbf{0}$)<br/>\n",
    ">\n",
    "> Loop for each episode:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Initialize state and obtain initial feature vector $\\textbf{x}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{z} \\gets \\textbf{0} \\quad$ (a d-dimensional vector)<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$V_{old} \\gets 0 \\quad$ (a temporary scalar variable)<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Loop for each step of episode:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Choose $A \\sim \\pi$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Take action A, observe R, $\\textbf{x'}$ (feature vector of the next state)<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V \\gets \\textbf{w}^T \\textbf{x}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V' \\gets \\textbf{w}^T \\textbf{x'}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\delta \\gets R + \\gamma V' - V$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{z} \\gets \\gamma \\lambda \\textbf{z} + (1 - \\alpha \\gamma \\lambda \\textbf{z}^T \\textbf{x}) \\textbf{x}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{w} \\gets \\textbf{w} + \\alpha (\\delta + V - V_{old}) \\textbf{z} - \\alpha (V - V_{old}) \\textbf{x}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V_{old} \\gets V'$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{x} \\gets \\textbf{x'}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;until $\\textbf{x'} = \\textbf{0}$ (signaling arrival at a terminal state)\n",
    "\n",
    "The pseudocode provided for Sarsa($\\lambda$) with binary features and linear function approximation for estimating $\\textbf{w}^T \\textbf{x} \\approx q_{\\pi} or q_*$ is:\n",
    "\n",
    "> Input: a function $\\mathcal{F}(s, a)$ returning a set of (indices of) active features for $s$, $a$<br/>\n",
    "> Input: the policy $\\pi$ (if estimating $q_{\\pi}$)<br/>\n",
    "> Algorithm parameters: step size $\\alpha > 0$, trace decay rate $\\lambda \\in [0, 1]$<br/>\n",
    "> Initialize $\\textbf{w} = (w_1, ..., w_d)^T \\in \\mathbb{R}^d$ (e.g., $\\textbf{w} = \\textbf{0}$), $\\textbf{z} = (z_1, ..., z_d)^T \\in \\mathbb{R}^d$<br/>\n",
    ">\n",
    "> Loop for each episode:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Initialize S<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Choose $A \\sim \\pi(\\cdot | S)$ or $\\epsilon$-greedy according to $\\widehat{q}(S, \\cdot, \\textbf{w})$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{z} \\gets \\textbf{0}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Loop for each step of episode:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Take action A, observe R, S'<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\delta \\gets R$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Loop for $i$ in $\\mathcal{F}(s, a)$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\delta \\gets \\delta - w_i$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$z_i \\gets z_i + 1 \\quad$ (accumulating traces)<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or $z_i \\gets 1 \\quad$ (replacing traces)<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If S' is terminal then:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{w} \\gets \\textbf{w} + \\alpha \\delta \\textbf{z}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Go to next episode<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Choose $A' \\sim \\pi(\\cdot | S')$ or near greedily $\\sim \\widehat{q}(S', \\cdot, \\textbf{w})$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Loop for $i$ in $\\mathcal{F}(s, a)$: $\\delta \\gets \\delta + \\gamma w_i$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{w} \\gets \\textbf{w} \\alpha \\delta \\textbf{z}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{z} \\gets \\gamma \\lambda \\textbf{z}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$S \\gets S'; A \\gets A'$<br/>\n",
    "\n",
    "Equation 12.11 is:\n",
    "\n",
    "$$\n",
    "\\textbf{z}_t \\doteq \\gamma \\lambda \\textbf{z}_{t-1} + (1 - \\alpha \\gamma \\lambda \\textbf{z}_{t-1}^T \\textbf{x}_t) \\textbf{x}_t \\tag{12.11}\n",
    "$$\n",
    "\n",
    "For this example, $\\textbf{x}_t = \\mathcal{F}(S_t, A_t)$, so the equation becomes:\n",
    "\n",
    "$$\n",
    "\\textbf{z}_t \\doteq \\gamma \\lambda \\textbf{z}_{t-1} + (1 - \\alpha \\gamma \\lambda \\textbf{z}_{t-1}^T \\mathcal{F}(S_t, A_t)) \\mathcal{F}(S_t, A_t)\n",
    "$$\n",
    "\n",
    "It's important to note that the vector row $\\textbf{z}_{t-1}^T$ multiplied by the vector column $\\mathcal{F}(S_t, A_t)$ gives a scalar, so the entire term $1 - \\alpha \\gamma \\lambda \\textbf{z}_{t-1}^T \\mathcal{F}(S_t, A_t)$ is a scalar, which will be represented by the variable $\\phi$ in the pseudocode ($\\phi_0 = (1 - 0) = 1$). When the trace is being accumulated, this will be the weight that is multiplied by the features vector.\n",
    "\n",
    "Also, note that except for $\\textbf{z}_{-1} = \\textbf{0}$, the value of $\\textbf{z}$ is partially defined at the end of the previous episode, specifically the left term ($\\gamma \\lambda \\textbf{z}$), while the right term is defined in the current episode. This right term depends on $\\phi$, which depends on the previous value of $\\textbf{z}$, so the value of $\\phi$ used in an episode must be defined in the previous episode, before $\\textbf{z} \\gets \\gamma \\lambda \\textbf{z}$ (with $\\phi_0 = 1$, so that $\\textbf{z}_0 = \\gamma \\lambda \\textbf{z}_{-1} + \\phi_0 \\textbf{x}_0 = 0 + 1 \\cdot \\textbf{x}_0 = \\textbf{x}_0$).\n",
    "\n",
    "The pseudocode for Sarsa($\\lambda$) with dutch traces (12.11) without the other distinctive features of a true online algorithm is defined below:\n",
    "\n",
    "> Input: a function $\\mathcal{F}(s, a)$ returning a set of (indices of) active features for $s$, $a$<br/>\n",
    "> Input: the policy $\\pi$ (if estimating $q_{\\pi}$)<br/>\n",
    "> Algorithm parameters: step size $\\alpha > 0$, trace decay rate $\\lambda \\in [0, 1]$<br/>\n",
    "> Initialize $\\textbf{w} = (w_1, ..., w_d)^T \\in \\mathbb{R}^d$ (e.g., $\\textbf{w} = \\textbf{0}$), $\\textbf{z} = (z_1, ..., z_d)^T \\in \\mathbb{R}^d$<br/>\n",
    ">\n",
    "> Loop for each episode:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Initialize S<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Choose $A \\sim \\pi(\\cdot | S)$ or $\\epsilon$-greedy according to $\\widehat{q}(S, \\cdot, \\textbf{w})$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{z} \\gets \\textbf{0}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$\\phi \\gets 1$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Loop for each step of episode:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Take action A, observe R, S'<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\delta \\gets R$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Loop for $i$ in $\\mathcal{F}(s, a)$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\delta \\gets \\delta - w_i$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$z_i \\gets z_i + \\phi \\quad$ (accumulating traces)<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or $z_i \\gets 1 \\quad$ (replacing traces)<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If S' is terminal then:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{w} \\gets \\textbf{w} + \\alpha \\delta \\textbf{z}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Go to next episode<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Choose $A' \\sim \\pi(\\cdot | S')$ or near greedily $\\sim \\widehat{q}(S', \\cdot, \\textbf{w})$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Loop for $i$ in $\\mathcal{F}(s, a)$: $\\delta \\gets \\delta + \\gamma w_i$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{w} \\gets \\textbf{w} \\alpha \\delta \\textbf{z}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{dot} = 0$ (temporary variable to get the value of $\\textbf{z}_{t-1}^T \\textbf{x}$)<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Loop for $i$ in $\\mathcal{F}(s, a)$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{dot} \\gets s_{dot} + z_i$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi \\gets 1 - \\alpha \\gamma \\lambda s_{dot}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\textbf{z} \\gets \\gamma \\lambda \\textbf{z}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$S \\gets S'; A \\gets A'$<br/>\n",
    "\n",
    "The accumulating trace would be defined with $z_i \\gets z_i + \\phi \\cdot x_i$, but the features are binary and $\\mathcal{F}(s, a)$ is the set of indices $i$ in which $x_i = 1$ for the given state and action (all other indices $j$ have $x_j = 0$), so $z_i + \\phi x_i= z_i + \\phi$ (the entire loop is equivalent to $\\textbf{z} \\gets \\textbf{z} + \\phi \\textbf{x}$, with $\\textbf{z}$ already defined at the end of the previous episode as $\\textbf{z} \\gets \\gamma \\lambda \\textbf{z}$, so this is the update provided by 12.11). Similarly, in the later part of the pseudocode, $s_{dot} \\gets s_{dot} + z_i$ because $s_{dot} + z_i x_i = s_{dot} + z_i$ for the indices $i$ given by $\\mathcal{F}(s, a)$.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
