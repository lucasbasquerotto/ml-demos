{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 02 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "In $\\epsilon$-greedy action selection, for the case of two actions and $\\epsilon = 0.5$, what is the probability that the greedy action is selected?\n",
    "\n",
    "**A**\n",
    "\n",
    "The greedy action is chosen with a probability of $\\epsilon / n$ when exploring (just like every other action) and $(1 - \\epsilon)$ when exploiting, with $n$ being the amount of possible actions. When $\\epsilon = 0.5$ and $n = 2$, the probability that the greedy action is selected is: \n",
    "\n",
    "$$\n",
    "(1 - \\epsilon) + \\frac{\\epsilon}{n} = (1 - 0.5) + \\frac{0.5}{2} = 0.5 + 0.25 = 0.75 = 75\\%\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: *Bandit example*\n",
    "\n",
    "**Q**\n",
    "\n",
    "Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using $\\epsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1(a) = 0$, for all $a$. Suppose the initial sequence of actions and rewards is $A_1 = 1, R_1 = -1, A_2 = 2, R_2 = 1, A_3 = 2, R_3 = -2, A_4 = 2, R_4 = 2, A_5 = 3, R_5 = 0$. On some of these time steps the $\\epsilon$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?\n",
    "\n",
    "**A**\n",
    "\n",
    "It definitely occured when choosing $A_4 = 2$, because at this point, $Q(2) = \\frac{1-2}{2} = -0.5$, and the value of actions 3 and 4 were 0 (and $Q(1) = -1$), so the greedy action would have chosen the action 3 or 4. \n",
    "\n",
    "It also definitely occured when choosing $A_5 = 3$, because at this point $Q(2) = \\frac{1-2+2}{3} = \\frac{1}{3}$ was the highest action value ($Q(3) = 0$ at this point).\n",
    "\n",
    "The $\\epsilon$ case could possibly have occurred at any time-step (because every action has a positive probability of $\\frac{\\epsilon}{4}$ of being chosen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "**Q**\n",
    "\n",
    "In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.\n",
    "\n",
    "**A**\n",
    "\n",
    "The best case will be the smallest non-zero $\\epsilon$ case, which is $\\epsilon = 0.01$. It will take more time to give good results, but it will reach a point in which it is better than $\\epsilon = 0.1$, because the later will choose a wrong action about 9% of the time (the probability of exploration is 10% divided among the 10 actions, with the best action having probability of 1% in this case, and 90% in the exploitation case). Similarly, $\\epsilon = 0.01$ will choose a wrong action about 0.9% of the time, which means that it's 10 times better asymptotically than $\\epsilon = 0.1$.\n",
    "\n",
    "The greedy case would always win if the first action chosen was the best action (considering the best action as the one with the biggest mean), but on average it could end up very bad (like the graphs show), because the environment is non-deterministic, and the deviation may cause a suboptimal action seem as the best action in a specific step, even tough it isn't. In a deterministic environment, with optimistic initial values, the greedy case would always choose the best action, but that's not the proposed scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "\n",
    "**Q**\n",
    "\n",
    "If the step-size parameters, $\\alpha_n$, are not constant, then the estimate $Q_n$ is a weighted average of previously received rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters?\n",
    "\n",
    "**A**\n",
    "\n",
    "For the case of all $\\alpha$ being the same (2.6) we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q_{n+1} &= Q_n + \\alpha [R_n - Q_n]\n",
    "\\\\\n",
    "&= \\alpha R_n + (1 - \\alpha) Q_n\n",
    "\\\\\n",
    "&= \\alpha R_n + (1 - \\alpha) [\\alpha R_{n-1} + (1 - \\alpha) Q_{n-1}]\n",
    "\\\\\n",
    "&= \\alpha R_n + (1 - \\alpha) \\alpha R_{n-1} + (1 - \\alpha)^2 \\alpha R_{n-2} + ... + (1 - \\alpha)^{n-1} \\alpha R_1 + (1 - \\alpha)^n Q_1\n",
    "\\\\\n",
    "&= (1 - \\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1 - \\alpha)^{n-i} R_i \\tag{2.6} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If the step-size parameters, $\\alpha_n$, are not constant, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q_{n+1} &= Q_n + \\alpha_n [R_n - Q_n]\n",
    "\\\\\n",
    "&= \\alpha_n R_n + (1 - \\alpha_n) Q_n\n",
    "\\\\\n",
    "&= \\alpha_n R_n + (1 - \\alpha_n) [\\alpha_{n-1} R_{n-1} + (1 - \\alpha_{n-1}) Q_{n-1}]\n",
    "\\\\\n",
    "&= \\alpha_n R_n + (1 - \\alpha_n) \\alpha_{n-1} R_{n-1} + (1 - \\alpha_n)(1 - \\alpha_{n-1}) \\alpha_{n-2} R_{n-2} + ... + \\left[ \\prod_{k=2}^n (1 - \\alpha_k) \\right] \\alpha_1 R_1 + \\left[ \\prod_{k=1}^n (1 - \\alpha_k) \\right] Q_1\n",
    "\\\\\n",
    "&= \\left[ \\prod_{k=1}^n (1 - \\alpha_k) \\right] Q_1 + \\sum_{i=1}^n \\alpha_i \\left[ \\prod_{k=i+1}^n (1 - \\alpha_k) \\right] R_i \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "assuming the empty product convention $\\prod_{k=i}^j f(k) = 1$ if $i > j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5 (programming)\n",
    "\n",
    "**Q**\n",
    "\n",
    "Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the $q_*(a)$ start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard deviation 0.01 to all the $q_*(a)$ on each step). Prepare plots like Figure 2.2 for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, $\\alpha = 0.1$. Use $\\epsilon = 0.1$ and longer runs, say of 10,000 steps.\n",
    "\n",
    "**A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = 10\n",
    "\n",
    "def default_bandit():\n",
    "    def bandit(action: int, step: int) -> tuple[float, list[int]]:\n",
    "        if action == 1:\n",
    "            return 1.0, [1]\n",
    "        else:\n",
    "            return 0.0, [1]\n",
    "\n",
    "    return bandit\n",
    "\n",
    "def distributed_bandit(mean=0, deviation=1, mean_per_action=0, dev_per_action=1):\n",
    "    distributed_avgs = [np.random.normal(mean, deviation) for _ in range(k)]\n",
    "\n",
    "    def bandit(action: int, step: int) -> float:\n",
    "        max_mean = max(distributed_avgs)\n",
    "        delta = 1e-6\n",
    "        best_actions = [\n",
    "            i+1\n",
    "            for i, mean in enumerate(distributed_avgs)\n",
    "            if (max_mean - delta) <= mean <= (max_mean + delta)\n",
    "        ]\n",
    "        return distributed_avgs[action-1] + np.random.normal(\n",
    "            mean_per_action,\n",
    "            dev_per_action), best_actions\n",
    "\n",
    "    def get_bandit():\n",
    "        return bandit\n",
    "\n",
    "    return get_bandit\n",
    "\n",
    "def non_stationary_bandit(initial=0, deviation=0.01):\n",
    "    def get_bandit():\n",
    "        true_qs = [initial] * k\n",
    "        last_step = 0\n",
    "\n",
    "        def bandit(action: int, step: int) -> float:\n",
    "            nonlocal last_step\n",
    "            value = true_qs[action-1]\n",
    "            max_value = max(true_qs)\n",
    "            delta = 1e-6\n",
    "            best_actions = [\n",
    "                i+1\n",
    "                for i, q in enumerate(true_qs)\n",
    "                if (max_value - delta) <= q <= (max_value + delta)\n",
    "            ]\n",
    "\n",
    "            if step > last_step:\n",
    "                last_step = step\n",
    "                for i in range(k):\n",
    "                    true_qs[i] += np.random.normal(0, deviation)\n",
    "\n",
    "            return value, best_actions\n",
    "\n",
    "        return bandit\n",
    "\n",
    "    return get_bandit\n",
    "\n",
    "def no_epsilon(step: int) -> float:\n",
    "    return 0\n",
    "\n",
    "def default_epsilon(step: int) -> float:\n",
    "    return 0.1\n",
    "\n",
    "def fixed_epsilon(epsilon: float) -> float:\n",
    "    def get_epsilon(step: int) -> float:\n",
    "        return epsilon\n",
    "    return get_epsilon\n",
    "\n",
    "def default_alpha(step: int, n: int) -> float:\n",
    "    return 1/n\n",
    "\n",
    "def fixed_alpha(alpha: float) -> float:\n",
    "    def get_alpha(step: int, n: int) -> float:\n",
    "        return alpha\n",
    "    return get_alpha\n",
    "\n",
    "def default_beta(alpha: float, prev_o: float):\n",
    "    return prev_o + alpha * (1 - prev_o)\n",
    "\n",
    "def prob_from_preferences(action: int, h: list[float]) -> float:\n",
    "    return np.exp(h[action-1]) / sum(np.exp(h))\n",
    "\n",
    "class Params:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name='default',\n",
    "        initial_q=0.0,\n",
    "        get_epsilon: typing.Callable[[int], float] | None = None,\n",
    "        get_alpha: typing.Callable[[int, int], float] | None = None,\n",
    "        get_beta: typing.Callable[[float, float], float] | None = None,\n",
    "        ucb_c=0,\n",
    "        ucb_maximize=0.01,\n",
    "        preference=False,\n",
    "        no_pref_baseline=False,\n",
    "    ):\n",
    "        _get_epsilon = get_epsilon or fixed_epsilon(0)\n",
    "        _get_alpha = get_alpha or (\n",
    "            default_alpha\n",
    "            if (not get_beta) and (not ucb_c) and (not preference)\n",
    "            else fixed_alpha(0.1))\n",
    "\n",
    "        self.name = name\n",
    "        self.initial_q = initial_q\n",
    "        self.get_epsilon = _get_epsilon\n",
    "        self.get_alpha = _get_alpha\n",
    "        self.get_beta = get_beta\n",
    "        self.ucb_c = ucb_c\n",
    "        self.ucb_maximize = ucb_maximize\n",
    "        self.preference = preference\n",
    "        self.no_pref_baseline = no_pref_baseline\n",
    "\n",
    "class RunResult:\n",
    "    def __init__(\n",
    "        self,\n",
    "        step: int,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        avg_reward: float,\n",
    "        opt_percent: float,\n",
    "    ):\n",
    "        self.step = step\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.avg_reward = avg_reward\n",
    "        self.opt_percent = opt_percent\n",
    "\n",
    "def plot(results: list[tuple[str, list[RunResult]]]) -> None:\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "    for name, result in results:\n",
    "        steps = [r.step for r in result]\n",
    "        avg_r = [r.reward for r in result]\n",
    "        opt_a = [r.opt_percent for r in result]\n",
    "        ax1.plot(steps, avg_r, '-', label=name)\n",
    "        ax2.plot(steps, opt_a, '-', label=name)\n",
    "\n",
    "    ax1.set_xlabel('Steps')\n",
    "    ax1.set_ylabel('Avg Reward')\n",
    "    ax2.set_ylabel('Optimal Action (%)')\n",
    "    ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def run(\n",
    "    all_rewards: list[list[float]],\n",
    "    all_best_actions: list[list[int]],\n",
    "    params: Params,\n",
    "    best_action_per_step: bool, seed: int | None = None,\n",
    "):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    name = params.name\n",
    "    initial_q = params.initial_q\n",
    "    get_epsilon = params.get_epsilon\n",
    "    get_alpha = params.get_alpha\n",
    "    get_beta = params.get_beta\n",
    "    ucb_c = params.ucb_c\n",
    "    ucb_maximize = params.ucb_maximize\n",
    "    preference = params.preference\n",
    "    no_pref_baseline = params.no_pref_baseline\n",
    "\n",
    "    q = [initial_q] * k\n",
    "    n = [0] * k\n",
    "    h = [0] * k\n",
    "    prev_o = [0] * k\n",
    "    r_total = 0\n",
    "    r_avg = 0.0\n",
    "    opt_total = 0\n",
    "\n",
    "    # step, action, avg reward, optimal action percentage\n",
    "    result: list[RunResult] = []\n",
    "\n",
    "    for i, rewards in enumerate(all_rewards):\n",
    "        step = i + 1\n",
    "        epsilon = get_epsilon(step)\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            a: int = random.randint(1, k)\n",
    "        elif ucb_c:\n",
    "            # Upper-Confidence-Bound Action Selection\n",
    "            val_ucb = [\n",
    "                (q_i + ucb_c * np.sqrt(np.log(step) / (n_i or ucb_maximize)))\n",
    "                for q_i, n_i in zip(q, n)\n",
    "            ]\n",
    "            a: int = np.argmax(val_ucb) + 1\n",
    "        elif preference:\n",
    "            probs = [prob_from_preferences(i+1, h) for i in range(k)]\n",
    "            a = np.random.choice(range(1, k+1), p=probs)\n",
    "        else:\n",
    "            a: int = np.argmax(q) + 1\n",
    "\n",
    "        r = rewards[a-1]\n",
    "        n[a-1] += 1\n",
    "\n",
    "        alpha = get_alpha(step, n[a-1])\n",
    "        weight = alpha\n",
    "\n",
    "        if preference:\n",
    "            probs = [prob_from_preferences(i+1, h) for i in range(k)]\n",
    "            baseline = 0 if no_pref_baseline else r_avg\n",
    "            h = [\n",
    "                (\n",
    "                    (h_i + alpha * (r - baseline) * (1 - probs[i]))\n",
    "                    if i == a-1\n",
    "                    else (h_i - alpha * (r - baseline) * probs[i])\n",
    "                )\n",
    "                for i, h_i in enumerate(h)\n",
    "            ]\n",
    "\n",
    "        if get_beta:\n",
    "            beta = get_beta(alpha=alpha, prev_o=prev_o[a-1])\n",
    "            weight = alpha / beta\n",
    "\n",
    "        q[a-1] += weight * (r - q[a-1])\n",
    "\n",
    "        r_total += r\n",
    "        r_avg = r_total / step\n",
    "        best_r = max(rewards)\n",
    "        delta = 1e-6\n",
    "        is_best_a = (\n",
    "            (best_r - delta <= r <= best_r + delta)\n",
    "            if best_action_per_step\n",
    "            else (a in all_best_actions[i]))\n",
    "        opt_total += 1 if is_best_a else 0\n",
    "        opt_percent = 100.0 * opt_total / step\n",
    "        result.append(RunResult(\n",
    "            step=step,\n",
    "            action=a,\n",
    "            reward=r,\n",
    "            avg_reward=r_avg,\n",
    "            opt_percent=opt_percent))\n",
    "\n",
    "    return name, result\n",
    "\n",
    "def parallel_run(\n",
    "    amount: int,\n",
    "    params: Params,\n",
    "    best_action_per_step: bool,\n",
    "    steps=1000,\n",
    "    get_bandit=default_bandit,\n",
    "    seed: int | None = None,\n",
    ") -> tuple[str, list[RunResult], list[list[list[float]]]]:\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    parallel_seeds = [random.randint(0, 1000000) for _ in range(amount)]\n",
    "\n",
    "    for _ in range(amount):\n",
    "        bandit = get_bandit()\n",
    "        all_parallel_info = [\n",
    "            [[bandit(action=a, step=step) for a in range(1, k+1)] for step in range(1, steps+1)]\n",
    "            for _ in range(amount)\n",
    "        ]\n",
    "\n",
    "    results = [\n",
    "        run(\n",
    "            all_rewards=[[r for r, _ in all_info] for all_info in all_parallel_info[i]],\n",
    "            # all_info[0] will give the best actions if the first action is chosen\n",
    "            # (because it's the same for all actions)\n",
    "            all_best_actions=[all_info[0][1] for all_info in all_parallel_info[i]],\n",
    "            params=Params(\n",
    "                name=f'{params.name} {i}',\n",
    "                initial_q=params.initial_q,\n",
    "                get_epsilon=params.get_epsilon,\n",
    "                get_alpha=params.get_alpha,\n",
    "                get_beta=params.get_beta,\n",
    "                ucb_c=params.ucb_c,\n",
    "                ucb_maximize=params.ucb_maximize,\n",
    "                preference=params.preference,\n",
    "                no_pref_baseline=params.no_pref_baseline,\n",
    "            ),\n",
    "            best_action_per_step=best_action_per_step,\n",
    "            seed=parallel_seeds[i],\n",
    "        )\n",
    "        for i in range(amount)]\n",
    "    avg_results: list[RunResult] = []\n",
    "\n",
    "    for i in range(steps):\n",
    "        # the result of the ith step of each (parallel) run\n",
    "        run_results = [r[i] for _, r in results]\n",
    "        actions = [r.action for r in run_results]\n",
    "        # the most common action in the ith step of all runs\n",
    "        action = max(set(actions), key=actions.count)\n",
    "        avg_results.append(\n",
    "            RunResult(\n",
    "                step=i+1,\n",
    "                action=action,\n",
    "                reward=sum(r.reward for r in run_results) / amount,\n",
    "                avg_reward=sum(r.avg_reward for r in run_results) / amount,\n",
    "                opt_percent=sum(r.opt_percent for r in run_results) / amount,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return params.name, avg_results\n",
    "\n",
    "\n",
    "def main(\n",
    "    parallel=10,\n",
    "    steps=1000,\n",
    "    get_bandit=default_bandit,\n",
    "    best_action_per_step=False,\n",
    "    plot_rewards=False,\n",
    "    cases: list[Params] = [],\n",
    "    seed=1,\n",
    "):\n",
    "    cases = cases or [Params()]\n",
    "\n",
    "    if plot_rewards:\n",
    "        bandit = get_bandit()\n",
    "        all_rewards = [\n",
    "            [bandit(action=a, step=step)[0] for a in range(1, k+1)]\n",
    "            for step in range(1, steps+1)\n",
    "        ]\n",
    "        r_per_a = list(zip(*all_rewards))\n",
    "        plt.boxplot(r_per_a)\n",
    "        plt.title('Reward distribution per action (Example)')\n",
    "        plt.show()\n",
    "\n",
    "    full_results = [\n",
    "        parallel_run(\n",
    "            amount=parallel,\n",
    "            steps=steps,\n",
    "            get_bandit=get_bandit,\n",
    "            params=case,\n",
    "            best_action_per_step=best_action_per_step,\n",
    "            seed=seed)\n",
    "        for case in cases\n",
    "    ]\n",
    "\n",
    "    plot(full_results)\n",
    "\n",
    "class CaseParams:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        params_list: list[float],\n",
    "        get_case: typing.Callable[[float], Params],\n",
    "        color: str | None = None,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.params_list = params_list\n",
    "        self.get_case = get_case\n",
    "        self.color = color\n",
    "\n",
    "class CaseParamsResult:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        params_list: list[float],\n",
    "        case_results: list[float],\n",
    "        color: str | None,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.params_list = params_list\n",
    "        self.case_results = case_results\n",
    "        self.color = color\n",
    "\n",
    "def exponential_params(initial: int, final: int):\n",
    "    return [2**i for i in range(initial, final+1)]\n",
    "\n",
    "def verify_params(\n",
    "    cases_params: list[CaseParams],\n",
    "    parallel=10,\n",
    "    steps=1000,\n",
    "    get_bandit=default_bandit,\n",
    "    best_action_per_step=False,\n",
    "    plot_rewards=False,\n",
    "    plot_cases=False,\n",
    "    consider_last=0,\n",
    "    seed=1,\n",
    "):\n",
    "    results: list[CaseParamsResult] = []\n",
    "\n",
    "    if plot_rewards:\n",
    "        bandit = get_bandit()\n",
    "        all_rewards = [\n",
    "            [bandit(action=a, step=step)[0] for a in range(1, k+1)]\n",
    "            for step in range(1, steps+1)\n",
    "        ]\n",
    "        r_per_a = list(zip(*all_rewards))\n",
    "        plt.boxplot(r_per_a)\n",
    "        plt.title('Reward distribution per action (Example)')\n",
    "        plt.show()\n",
    "\n",
    "    for case_params in cases_params:\n",
    "        name = case_params.name\n",
    "        params_list = case_params.params_list\n",
    "        get_case = case_params.get_case\n",
    "        color = case_params.color\n",
    "\n",
    "        full_results = [\n",
    "            parallel_run(\n",
    "                amount=parallel,\n",
    "                steps=steps,\n",
    "                get_bandit=get_bandit,\n",
    "                params=get_case(param),\n",
    "                best_action_per_step=best_action_per_step,\n",
    "                seed=seed)\n",
    "            for param in params_list\n",
    "        ]\n",
    "        # get all rewards for each parameter case\n",
    "        params_all_rewards = [\n",
    "            [r.reward for r in inner_results]\n",
    "            for name, inner_results in full_results\n",
    "        ]\n",
    "\n",
    "        if consider_last > 0:\n",
    "            params_all_rewards = params_all_rewards[-consider_last:]\n",
    "\n",
    "        avg_rewards = [sum(rewards) / len(rewards) for rewards in params_all_rewards]\n",
    "\n",
    "        results.append(CaseParamsResult(\n",
    "            name=name,\n",
    "            params_list=params_list,\n",
    "            case_results=avg_rewards,\n",
    "            color=color))\n",
    "\n",
    "        if plot_cases:\n",
    "            plot(full_results)\n",
    "\n",
    "    # logarithmic plot in a single graph the average reward of the last step of each parameter of each case\n",
    "    # in which each case (parameter type) is a different line\n",
    "    fig, ax = plt.subplots()\n",
    "    for case in results:\n",
    "        x_labels = [\n",
    "            f'2^{int(param)}'\n",
    "            for param in np.log2(case.params_list)\n",
    "        ]\n",
    "        ax.plot(x_labels, case.case_results, '-', label=case.name, color=case.color)\n",
    "\n",
    "    ax.set_xlabel('Parameter')\n",
    "    ax.set_ylabel('Avg Reward')\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(get_bandit=distributed_bandit(), parallel=2000, steps=1000, plot_rewards=True, cases=[\n",
    "    Params(name='random', get_epsilon=lambda step: 1),\n",
    "    Params(name='greedy', get_epsilon=lambda step: 0),\n",
    "    Params(name='ε-greedy (0.1)', get_epsilon=lambda step: 0.1),\n",
    "    Params(name='ε-greedy (0.01)', get_epsilon=lambda step: 0.01),\n",
    "    Params(name='constant_alpha', get_epsilon=lambda step: 0.1, get_alpha=lambda step, n: 0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(get_bandit=default_bandit, parallel=100, steps=1000, plot_rewards=True, cases=[\n",
    "    Params(name='random', get_epsilon=lambda step: 1),\n",
    "    Params(name='greedy', get_epsilon=lambda step: 0),\n",
    "    Params(name='e-greedy', get_epsilon=lambda step: 0.1),\n",
    "    Params(name='constant_alpha', get_epsilon=lambda step: 0.1, get_alpha=lambda step, n: 0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the best action in each step, instead of a single best action overall\n",
    "# defined with a lesser deviation per action, to more easily define the best action\n",
    "main(get_bandit=distributed_bandit(dev_per_action=0.2), parallel=2000, steps=1000, best_action_per_step=True, plot_rewards=True, cases=[\n",
    "    Params(name='random', get_epsilon=lambda step: 1),\n",
    "    Params(name='greedy', get_epsilon=lambda step: 0),\n",
    "    Params(name='e-greedy (0.1)', get_epsilon=lambda step: 0.1),\n",
    "    Params(name='ε-greedy (0.01)', get_epsilon=lambda step: 0.01),\n",
    "    Params(name='constant_alpha', get_epsilon=lambda step: 0.1, get_alpha=lambda step, n: 0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.5 (pg. 55)\n",
    "main(get_bandit=non_stationary_bandit(), parallel=2000, steps=10000, plot_rewards=True, cases=[\n",
    "    Params(name='sample_averages', get_epsilon=lambda step: 0.1),\n",
    "    Params(name='constant_alpha', get_epsilon=lambda step: 0.1, get_alpha=lambda step, n: 0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_q=5\n",
    "main(get_bandit=distributed_bandit(), parallel=2000, steps=1000, plot_rewards=True, cases=[\n",
    "    Params(name='optimistic_greedy', get_epsilon=lambda step: 0, initial_q=5),\n",
    "    Params(name='e-greedy', get_epsilon=lambda step: 0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6: *Mysterious Spikes*\n",
    "\n",
    "**Q**\n",
    "\n",
    "The results shown in Figure 2.3 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?\n",
    "\n",
    "**A**\n",
    "\n",
    "Initially, it will explore all actions until all of them be more or less on the higher side of the best action, and from this point on it will choose the best action, but due to the stochasticity of the environment, the best (mean) action may give a bad reward in some time-step (which is almost guaranteed after several time-steps), which will decrease its value below the point where the others are, and the time it takes to reduce the value of the other actions may not be small, because at this point many actions may give rewards above the point in which the value of the best action is, and the agent is following a greedy policy, so while there is at least 1 action with higher value than the best action, it will continue to choose it. \n",
    "\n",
    "After the value of the best action becomes the highest again, as the steps go on, the value stabilizes at about its mean value (even if it receives a bad reward sometimes, the other actions were also chosen several times, so they will all be closer to their true values, and the updates will have less impact in the final value of each action).\n",
    "\n",
    "This method perform particularly better when the overlap in the possible rewards of each action is small (being optimal if there's no overlap, in which case the best action will be chosen after each action is visited once, and will always be chosen from then on). It might be worse when there are good chances of overlap in the rewards (with good chances of making suboptimal actions give better rewards than the optimal action) due to its greedy nature, that will make the agent take more time to fix the mistake. In other words, the agent may temporarily prioritize suboptimal actions due to random reward fluctuations until the values stabilize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.7\n",
    "\n",
    "In most of this chapter we have used sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis in (2.6)). However, sample averages are not a completely satisfactory solution because they may perform poorly on non-stationary problems. Is it possible to avoid the bias of constant step sizes while retaining their advantages on non-stationary problems? One way is to use a step size of\n",
    "\n",
    "$$\n",
    "\\beta_t \\doteq \\alpha / \\overline{o}_t, \\tag{2}\n",
    "$$\n",
    "\n",
    "where α > 0 is a conventional constant step size and $\\overline{o}_t$ is a trace of one that starts at 0:\n",
    "\n",
    "$$\n",
    "\\overline{o}_{t+1} = \\overline{o}_t + \\alpha(1 - \\overline{o}_t), \\tag{3}\n",
    "$$\n",
    "\n",
    "for t ≥ 1 and with $\\overline{o}_1 = \\alpha$.\n",
    "\n",
    "Carry out an analysis like that in (2.6) to show that $\\beta_t$ is an exponential recency-weighted average without initial bias.\n",
    "\n",
    "\\begin{align*}\n",
    "\\overline{o}_{n} &= \\overline{o}_{n-1} + \\alpha \\cdot (1 - \\overline{o}_{n-1}) \\\\\n",
    "&=  \\overline{o}_{n-1} + \\alpha - \\alpha \\cdot \\overline{o}_{n-1} \\\\\n",
    "&=  \\alpha + (1 - \\alpha) \\cdot \\overline{o}_{n-1} \\\\\n",
    "&=  \\alpha + (1 - \\alpha) \\left( \\alpha + (1 - \\alpha) \\cdot \\overline{o}_{n-2} \\right) \\\\\n",
    "&=  \\alpha + \\alpha \\cdot (1 - \\alpha) + (1 - \\alpha)^2 \\cdot \\overline{o}_{n-2} \\\\\n",
    "&=  \\alpha + \\alpha \\cdot (1 - \\alpha) + (1 - \\alpha) \\left( (1 - \\alpha) \\cdot \\overline{o}_{n-2} \\right) \\\\\n",
    "&...* \\\\\n",
    "&= \\alpha + \\alpha \\cdot (1 - \\alpha) + \\alpha \\cdot (1 - \\alpha)^2 + (1 - \\alpha)^3 \\cdot \\overline{o}_{n-3} \\\\\n",
    "&... \\\\\n",
    "&= \\alpha + \\alpha \\cdot (1 - \\alpha) + \\alpha \\cdot (1 - \\alpha)^2 + \\alpha \\cdot (1 - \\alpha)^3 + \\ldots + \\alpha \\cdot (1 - \\alpha)^{n-1} + (1 - \\alpha)^n \\cdot \\overline{o}_0 \\\\\n",
    "&= \\alpha + \\alpha \\cdot (1 - \\alpha) + \\alpha \\cdot (1 - \\alpha)^2 + \\alpha \\cdot (1 - \\alpha)^3 + \\ldots + \\alpha \\cdot (1 - \\alpha)^{n-1} \\\\\n",
    "&= \\sum_{i=0}^{n-1} \\alpha \\cdot (1 - \\alpha)^i\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "*Obs: (1 - \\alpha) \\cdot \\overline{o}_{n-1} \\text{ became } \\alpha \\cdot (1 - \\alpha) + (1 - \\alpha)^2 \\cdot \\overline{o}_{n-2}, \\\\\n",
    "\\text{ so } (1 - \\alpha) \\cdot \\overline{o}_{n-2} \\text{ will become } \\alpha \\cdot (1 - \\alpha) + (1 - \\alpha)^2 \\cdot \\overline{o}_{n-3}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{o}_{n} = \\sum_{i=0}^{n-1} \\alpha \\cdot (1 - \\alpha)^i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align*}\n",
    "or\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{o}_{n} = \\alpha \\cdot (1 - \\alpha)^{n-1} + \\overline{o}_{n-1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{this means that the most recent reward will have a slightly higher weight than the previous, and so on, }\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{but the increase of weight reduces exponentially with the number of steps}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{so, } \\beta_{n} = \\alpha/\\overline{o}_{n} \\text{ will decrease with n, but very little due to the exponential nature of } \\overline{o}_{n}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Also, } \\overline{o}_{n} \\text{ converges to 1 as n increases, according to the following:}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\overline{o}_{n} &= \\sum_{i=0}^{n-1} \\alpha \\cdot (1 - \\alpha)^i \\\\\n",
    "&= \\alpha + \\alpha \\cdot (1 - \\alpha) + \\alpha \\cdot (1 - \\alpha)^2 + ... + \\alpha \\cdot (1 - \\alpha)^{n-1}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{so}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\overline{o}_{n} - (1 - \\alpha) \\cdot \\overline{o}_{n} = \\alpha - \\alpha \\cdot (1 - \\alpha)^n \\\\\n",
    "\\alpha \\cdot \\overline{o}_{n} = \\alpha \\cdot (1 - (1 - \\alpha)^n) \\\\\n",
    "\\overline{o}_{n} = 1 - (1 - \\alpha)^n\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\overline{o}_{n} \\text{ increases each time less as n increases, converging to 1 as n goes to infinity, because } |1 - \\alpha| < 1\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Which means that } \\beta_{n} = \\alpha / \\overline{o}_{n} \\text{ starts with value } \\beta_1 = 1 \\text{ (} \\overline{o}_{1} = \\alpha \\text{)}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{and converges to } \\alpha \\text{ (} \\overline{o}_{n} \\text{ converges to 1, as explained above) as n goes to infinity, }\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{while sample averages starts with 1 and converges to 0 (because 1/n converges to 0)}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{This means that using sample averages, the reward in later steps has almost no impact on } Q_n \\text{,}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{while using } \\beta_n \\text{, no matter how big is n, the current reward will still have a weight of at least } \\alpha\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Applying a reasoning similar to 2.6, we have:}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "Q_{n+1} &= Q_n + \\beta_n \\cdot [R_n - Q_n] \\\\\n",
    "&= \\beta_n \\cdot R_n + (1 - \\beta_n) \\cdot Q_n \\\\\n",
    "&= \\beta_n \\cdot R_n + (1 - \\beta_n) \\cdot [\\beta_{n-1} \\cdot R_{n-1} + (1 - \\beta_{n-1}) \\cdot Q_{n-1}] \\\\\n",
    "&= \\beta_n \\cdot R_n + (1 - \\beta_n) \\cdot \\beta_{n-1} \\cdot R_{n-1} + (1 - \\beta_n)(1 - \\beta_{n-1}) \\cdot Q_{n-1} \\\\\n",
    "&= \\beta_n \\cdot R_n + (1 - \\beta_n) \\cdot \\beta_{n-1} \\cdot R_{n-1} + (1 - \\beta_n)(1 - \\beta_{n-1}) \\cdot \\beta_{n-2} \\cdot R_{n-2} + \\\\\n",
    "&... + [\\prod_{i=2}^n (1 - \\beta_i) ] \\cdot \\beta_1 \\cdot R_1 + [\\prod_{i=1}^n (1 - \\beta_i) ] \\cdot Q_1 \\\\\n",
    "&= [\\prod_{i=1}^n (1 - \\beta_i) ] \\cdot Q_1 + \\sum_{i=1}^n [\\prod_{j=i+1}^n (1 - \\beta_j) ] \\cdot \\beta_i \\cdot R_i\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{with } \\prod_{j=k}^n (1 - \\beta_j) = 1 \\text{ if } k > n \\text{ (Empty Product Convention)}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{but } \\beta_1 = 1 \\text{ (as said above) and, consequently, } 1 - \\beta_1 = 0 \\text{, so:}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "Q_{n+1} = \\sum_{i=1}^n [\\prod_{j=i+1}^n (1 - \\beta_j) ] \\cdot \\beta_i \\cdot R_i\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{which does not have an initial bias (does not depend on } Q_1 \\text{)}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{It's important to note that the sum of the weights is 1, that is:}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "f(n) = \\sum_{i=1}^n [\\prod_{j=i+1}^n (1 - \\beta_j) ] \\cdot \\beta_i = 1\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{because for a value of n, the next function value is the previous multiplied by }(1 - \\beta_{n+1})\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{ and added the new weight, the last, which is }\\beta_{n+1}\\text{:}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "f(n+1) &= \\sum_{i=1}^{n+1} [\\prod_{j=i+1}^{n+1} (1 - \\beta_j) ] \\cdot \\beta_i \\\\\n",
    "&= \\left[ \\left[\\prod_{j=n+1+1}^{n+1} (1 - \\beta_j) \\right] \\cdot \\beta_{n+1} \\right] + \\left[\\sum_{i=1}^n \\left[\\prod_{j=i+1}^{n+1} (1 - \\beta_j) \\right] \\cdot \\beta_i \\right] \\\\\n",
    "&= \\left[ 1 \\cdot \\beta_{n+1} \\right] + \\left[\\sum_{i=1}^n (1 - \\beta_{n+1}) \\left[\\prod_{j=i+1}^n (1 - \\beta_j) \\right] \\cdot \\beta_i \\right] \\\\\n",
    "&= \\beta_{n+1} + (1 - \\beta_{n+1}) \\left[\\sum_{i=1}^n \\left[\\prod_{j=i+1}^n (1 - \\beta_j) \\right] \\cdot \\beta_i \\right] \\\\\n",
    "&= \\beta_{n+1} + (1 - \\beta_{n+1}) f(n) \\\\\n",
    "&= 1 + \\beta_{n+1} (1 - f(n))\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{For a value of n such as f(n) = 1, the next term is 1:}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "f(n+1) = 1 + \\beta_{n+1} (1 - f(n)) = 1 + \\beta_{n+1} (1 - 1) = 1, \\quad \\text{given f(n) = 1}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Also, f(1) = 1:}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "f(1) = \\sum_{i=1}^1 [\\prod_{j=i+1}^1 (1 - \\beta_j) ] \\cdot \\beta_i = [\\prod_{j=1+1}^1 (1 - \\beta_j) ] \\cdot \\beta_1 = 1 \\cdot \\beta_1 = \\beta_1 = 1\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{And taking into account that f(n+1) = 1 if f(n) = 1, then f(n) = 1 for all n > 1}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{(f(1) = 1 implies f(2) = 1, which implies f(3) = 1, and so on)}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{so the sum of the weights are 1 for any number of terms n in the sum.}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [section 2.7] Upper-Confidence-Bound Action Selection\n",
    "main(get_bandit=distributed_bandit(), parallel=20, steps=1000, plot_rewards=True, cases=[\n",
    "    Params(name='optimistic_greedy', get_epsilon=lambda step: 0, initial_q=5),\n",
    "    Params(name='e-greedy', get_epsilon=lambda step: 0.1),\n",
    "    Params(name='e-greedy-beta', get_epsilon=lambda step: 0.1, get_beta=default_beta),\n",
    "    Params(name='ucb', ucb_c=2),\n",
    "    Params(name='ucb-beta', ucb_c=2, get_beta=default_beta),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.8: *UCB Spikes*\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Figure 2.4 the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward increases on the 11th step and why it decreases on the subsequent steps. Hint: If c = 1, then the spike is less prominent. \n",
    "\n",
    "**A**\n",
    "\n",
    "There are 10 different actions that the agent can choose, so the first 10 time-steps will each have a different action chosen because there will be an action $a$ in which $N(a) = 0$ in each of these time-steps. In the 11th step, the term $c\\sqrt{\\frac{ln{t}}{N_t(a)}}$, that corresponds to the confidence in the precision of its value due to the amount of times the action was chosen according to UCB (less confidence will give higher values to explore more), is the same for all actions ($t$ is $11$ in this case, $N_{11}(a) = 1$ for every action). This mean that the action that gave the best reward in the first 10 time-steps will be chosen.\n",
    "\n",
    "The graph shows the average performance of UCB action selection among several executions in parallel. The action with best reward in those executions is found between the 1st and 10th time-steps in each run, but it will not be the same across a sufficient number of runs with different seeds, so the average performance should be about 0 (because the mean reward of the actions is 0). On the 11th time-step, tough, the action with the best reward will be chosen in each parallel execution, due to the reason explained previously, so there will be a significant increase in value between the 10th and 11th average performance. At this point, the value may not still be the optimal value: some executions will choose non-optimal actions because the action that have given the best reward in the first 10 time-steps may not be the best action due to the stochasticity of the environment (if the standard deviation of each action was 0, then the environment would be deterministic, but this is the case). As new actions are chosen across many time-steps, the agent will start choosing the actual best actions (the ones with the best mean). This can be seen as the spike in the 11th time-step has an average reward of about 1, but in later time-steps it approaches 1.5. That said, the action chosen by each agent for the 11th time-step should be a good action in most cases, so the average reward at this time-step is not so bad.\n",
    "\n",
    "The reason for the fast decrease in value right after the 11th time-step is that at this point the term $c\\sqrt{\\frac{ln{t}}{N_t(a)}}$ for the best action will be less than the other actions by a factor of $\\frac{1}{\\sqrt{2}}$, with the possibility of choosing the other actions depending on how big this value is when compared to the current values of those actions. For example, at time-step 16, considering that the best action of the first 10 time-steps was not chosen gain after time-step 11, the value of this term will be:\n",
    "\n",
    "For the best action: \n",
    "\n",
    "$$\n",
    "c\\sqrt{\\frac{ln{t}}{N_t(a)}} = 2\\sqrt{\\frac{16}{2}} = 2\\sqrt{8} = 4\\sqrt{2} \\approx 5.6\n",
    "$$\n",
    "\n",
    "For all actions that were chosen only once:\n",
    "\n",
    "$$\n",
    "c\\sqrt{\\frac{ln{t}}{N_t(a)}} = 2\\sqrt{\\frac{16}{1}} = 2 \\cdot 4 = 8\n",
    "$$\n",
    "\n",
    "The mean among all actions is 0, the standard deviation to define the mean of each action is 1, and for each action, the standard deviation of the actual value is also 1, so it's expected that the best action will not have a value higher than 1.5, and the worse action probably will not have a value lower than -1.5, so their difference in values will probably be less than 3, with many actions with much less difference. The difference of the 2nd term used to choose the action is 2.4 (8 - 5.6), so it's very likely that the algorithm will iterate all over the actions again, or at least iterate among most of them, after step 11, causing the decrease in reward. From now on, the 2nd term will have each time less impact on the overall UCB value to choose the action, because t is in a logarithm and will not increase so much for larger values of t.\n",
    "\n",
    "A value of $c = 1$ just makes the process of making the 2nd term have less impact faster:\n",
    "\n",
    "For the best action: \n",
    "\n",
    "$$\n",
    "c\\sqrt{\\frac{ln{t}}{N_t(a)}} = \\sqrt{\\frac{16}{2}} = \\sqrt{8} = 2\\sqrt{2} \\approx 2.8\n",
    "$$\n",
    "\n",
    "For all actions that were chosen only once:\n",
    "\n",
    "$$\n",
    "c\\sqrt{\\frac{ln{t}}{N_t(a)}} = \\sqrt{\\frac{16}{1}} = 4\n",
    "$$\n",
    "\n",
    "The difference is 1.2 (4 - 2.8), so it's very likely that only a few other actions with good rewards will be chosen after the 11th step, while the worst actions (that probably won't be the action with the actual best mean) will take some time to be chosen again, making the spike less prominent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.9\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "σ(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{align*}\n",
    "\n",
    "The softmax distribution of action probabilities is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "Pr\\{A_t=a\\} = \\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}}\n",
    "\\end{align*}\n",
    "\n",
    "In the case of 2 actions `a=1` and `b=2`, we have the softmax function:\n",
    "\n",
    "\\begin{align*}\n",
    "Pr\\{A_t=a\\} = \\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{i=1}^2 e^{H_t(i)}} = \\frac{e^{H_t(a)}}{e^{H_t(a)} + e^{H_t(b)}} = \\frac{1}{1 + \\frac{e^{H_t(b)}}{e^{H_t(a)}}} = \\frac{1}{1 + e^{{H_t(b)} - {H_t(a)}}}\n",
    "\\end{align*}\n",
    "\n",
    "So we have: \n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_t(a) = \\frac{1}{1 + e^{- ({H_t(a)} - {H_t(b)})}}\n",
    "\\end{align*}\n",
    "\n",
    "Which means that the softmax function with 2 actions, `a` and `b`, is the same as the sigmoid function:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_t(a) = σ({H_t(a)} - {H_t(b)})\n",
    "\\end{align*}\n",
    "\n",
    "Where ${H_t(a)} - {H_t(b)}$ is the relative preference of `a` over `b`.\n",
    "\n",
    "And correspondingly:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_t(b) = σ({H_t(b)} - {H_t(a)})\n",
    "\\end{align*}\n",
    "\n",
    "It's also important to note that the sum of the probabilities for all actions is always 1 (for any number of actions, not only for the specific case of 2 actions):\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{a=1}^k \\pi_t(a) = \\sum_{a=1}^k \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}} = \\frac{\\sum_{a=1}^k e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}} = 1\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [section 2.8] Gradient Bandit Algorithms\n",
    "main(get_bandit=distributed_bandit(mean=4, deviation=1), parallel=200, steps=10000, plot_rewards=True, cases=[\n",
    "    Params(name='a_0.1_with_baseline', get_epsilon=no_epsilon, get_alpha=fixed_alpha(0.1), preference=True),\n",
    "    Params(name='a_0.4_with_baseline', get_epsilon=no_epsilon, get_alpha=fixed_alpha(0.4), preference=True),\n",
    "    Params(name='a_0.1_without_baseline', get_epsilon=no_epsilon, get_alpha=fixed_alpha(0.1), preference=True, no_pref_baseline=True),\n",
    "    Params(name='a_0.4_without_baseline', get_epsilon=no_epsilon, get_alpha=fixed_alpha(0.4), preference=True, no_pref_baseline=True),\n",
    "    Params(name='a_0.1_e-greed', get_epsilon=fixed_epsilon(0.1), get_alpha=fixed_alpha(0.1)),\n",
    "    Params(name='a_0.4_e-greed', get_epsilon=fixed_epsilon(0.1), get_alpha=fixed_alpha(0.4)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [section 2.8] Gradient Bandit Algorithms (considering the best action per step, instead of the best average for all steps)\n",
    "main(get_bandit=distributed_bandit(mean=4, deviation=1), parallel=200, steps=10000, best_action_per_step=True, plot_rewards=True, cases=[\n",
    "    Params(name='a_0.1_with_baseline', get_epsilon=no_epsilon, get_alpha=fixed_alpha(0.1), preference=True),\n",
    "    Params(name='a_0.4_with_baseline', get_epsilon=no_epsilon, get_alpha=fixed_alpha(0.4), preference=True),\n",
    "    Params(name='a_0.1_without_baseline', get_epsilon=no_epsilon, get_alpha=fixed_alpha(0.1), preference=True, no_pref_baseline=True),\n",
    "    Params(name='a_0.4_without_baseline', get_epsilon=no_epsilon, get_alpha=fixed_alpha(0.4), preference=True, no_pref_baseline=True),\n",
    "    Params(name='a_0.1_e-greed', get_epsilon=fixed_epsilon(0.1), get_alpha=fixed_alpha(0.1)),\n",
    "    Params(name='a_0.4_e-greed', get_epsilon=fixed_epsilon(0.1), get_alpha=fixed_alpha(0.4)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.10\n",
    "\n",
    "**Q**\n",
    "\n",
    "Suppose you face a 2-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions 1 and 2 are respectively 0.1 and 0.2 with probability 0.5 (case A), and 0.9 and 0.8 with probability 0.5 (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?\n",
    "\n",
    "**A**\n",
    "\n",
    "If you don't know the case, the expectation of choosing the same case is:\n",
    "\n",
    "For action 1: $0.5 \\cdot 0.1 + 0.5 \\cdot 0.9 = 0.5$ (0.5 probability of case A and 0.5 probability of case B)\n",
    "\n",
    "For action 2: $0.5 \\cdot 0.2 + 0.5 \\cdot 0.8 = 0.5$ (0.5 probability of case A and 0.5 probability of case B)\n",
    "\n",
    "This means that for each action, if you don't know which case is, the expectation is 0.5, no matter which action you choose.\n",
    "\n",
    "In the case in which you know which case it is (A or B), then:\n",
    "\n",
    "If the case is A, choose the best action: action 2, that has true value 0.2\n",
    "\n",
    "If the case is B, choose the best action: action 1, that has true value 0.9\n",
    "\n",
    "As each of the 2 cases have a probability of 0.5 of happening, the expectation (mean value across a large number of steps) will be: $0.5 \\cdot 0.2 + 0.5 \\cdot 0.9 = 0.55$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.11 (programming)\n",
    "\n",
    "**Q**\n",
    "\n",
    "Make a figure analogous to Figure 2.6 for the nonstationary case outlined in Exercise 2.5. Include the constant-step-size $\\epsilon$-greedy algorithm with $\\alpha$=0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average reward over the last 100,000 steps.\n",
    "\n",
    "**A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.11 (pg. 66)\n",
    "verify_params(\n",
    "    parallel=200,\n",
    "    steps=200 * 10**3,\n",
    "    consider_last=100 * 10**3,\n",
    "    get_bandit=non_stationary_bandit(),\n",
    "    plot_rewards=True,\n",
    "    plot_cases=True,\n",
    "    cases_params=[\n",
    "        CaseParams(\n",
    "            name='epsilon',\n",
    "            params_list=exponential_params(-7, -2),\n",
    "            get_case=lambda epsilon: Params(name=f'epsilon={epsilon}', get_epsilon=fixed_epsilon(epsilon)),\n",
    "            color='red',\n",
    "        ),\n",
    "        CaseParams(\n",
    "            name='gradient',\n",
    "            params_list=exponential_params(-5, 2),\n",
    "            get_case=lambda alpha: Params(name=f'gradient={alpha}', preference=True, get_alpha=fixed_alpha(alpha)),\n",
    "            color='green',\n",
    "        ),\n",
    "        CaseParams(\n",
    "            name='ucb',\n",
    "            params_list=exponential_params(-4, 2),\n",
    "            get_case=lambda ucb_c: Params(name=f'ucb={ucb_c}', ucb_c=ucb_c),\n",
    "            color='blue',\n",
    "        ),\n",
    "        CaseParams(\n",
    "            name='optimistic',\n",
    "            params_list=exponential_params(-2, 2),\n",
    "            get_case=lambda initial_q: Params(name=f'optimistic={initial_q}', get_alpha=fixed_alpha(0.1), initial_q=initial_q),\n",
    "            color='black',\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.11 (pg. 66) (with best action per step, instead of the best average reward per step)\n",
    "verify_params(\n",
    "    parallel=200,\n",
    "    steps=200 * 10**3,\n",
    "    consider_last=100 * 10**3,\n",
    "    get_bandit=non_stationary_bandit(),\n",
    "    best_action_per_step=True,\n",
    "    plot_rewards=True,\n",
    "    plot_cases=True,\n",
    "    cases_params=[\n",
    "        CaseParams(\n",
    "            name='epsilon',\n",
    "            params_list=exponential_params(-7, -2),\n",
    "            get_case=lambda epsilon: Params(name=f'epsilon={epsilon}', get_epsilon=fixed_epsilon(epsilon)),\n",
    "            color='red',\n",
    "        ),\n",
    "        CaseParams(\n",
    "            name='gradient',\n",
    "            params_list=exponential_params(-5, 2),\n",
    "            get_case=lambda alpha: Params(name=f'gradient={alpha}', preference=True, get_alpha=fixed_alpha(alpha)),\n",
    "            color='green',\n",
    "        ),\n",
    "        CaseParams(\n",
    "            name='ucb',\n",
    "            params_list=exponential_params(-4, 2),\n",
    "            get_case=lambda ucb_c: Params(name=f'ucb={ucb_c}', ucb_c=ucb_c),\n",
    "            color='blue',\n",
    "        ),\n",
    "        CaseParams(\n",
    "            name='optimistic',\n",
    "            params_list=exponential_params(-2, 2),\n",
    "            get_case=lambda initial_q: Params(name=f'optimistic={initial_q}', get_alpha=fixed_alpha(0.1), initial_q=initial_q),\n",
    "            color='black',\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_params(\n",
    "    parallel=200,\n",
    "    steps=200 * 10**3,\n",
    "    consider_last=100 * 10**3,\n",
    "    get_bandit=distributed_bandit(mean=0, deviation=2),\n",
    "    plot_rewards=True,\n",
    "    plot_cases=True,\n",
    "    cases_params=[\n",
    "        CaseParams(\n",
    "            name='epsilon',\n",
    "            params_list=exponential_params(-7, -2),\n",
    "            get_case=lambda epsilon: Params(name=f'epsilon={epsilon}', get_epsilon=fixed_epsilon(epsilon)),\n",
    "            color='red',\n",
    "        ),\n",
    "        CaseParams(\n",
    "            name='gradient',\n",
    "            params_list=exponential_params(-5, 2),\n",
    "            get_case=lambda alpha: Params(name=f'gradient={alpha}', preference=True, get_alpha=fixed_alpha(alpha)),\n",
    "            color='green',\n",
    "        ),\n",
    "        CaseParams(\n",
    "            name='ucb',\n",
    "            params_list=exponential_params(-4, 2),\n",
    "            get_case=lambda ucb_c: Params(name=f'ucb={ucb_c}', ucb_c=ucb_c),\n",
    "            color='blue',\n",
    "        ),\n",
    "        CaseParams(\n",
    "            name='optimistic',\n",
    "            params_list=exponential_params(-2, 2),\n",
    "            get_case=lambda initial_q: Params(name=f'optimistic={initial_q}', get_alpha=fixed_alpha(0.1), initial_q=initial_q),\n",
    "            color='black',\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
