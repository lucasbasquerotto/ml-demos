{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 06 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "If V changes during the episode, then (6.6) only holds approximately; what would the difference be between the two sides? Let $V_t$ denote the array of state values used at time t in the TD error (6.5) and in the TD update (6.2). Redo the derivation above to determine the additional amount that must be added to the sum of TD errors in order to equal the Monte Carlo error.\n",
    "\n",
    "**A**\n",
    "\n",
    "The equation 6.2, that defines how the values are updated using the TD method, is:\n",
    "\n",
    "\\begin{align*}\n",
    "V(S_t) \\gets V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)] \\tag{6.2}\n",
    "\\end{align*}\n",
    "\n",
    "The TD error (equation 6.5) is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_t \\stackrel{.}{=} R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\tag{6.5}\n",
    "\\end{align*}\n",
    "\n",
    "The equation 6.6 is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1}) \\tag{from (3.9)} \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V(S_{t+2})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V(S_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\tag{6.6}\n",
    "\\end{align*}\n",
    "\n",
    "If V changes during the episode, then at the start of time t+1 the new V would be (for TD(0)):\n",
    "\n",
    "\\begin{align*}\n",
    "V_{t+1}(S_t) \\gets V_t(S_t) + \\alpha [R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)]\n",
    "\\end{align*}\n",
    "\n",
    "where $V_t$ hold the state values at time t.\n",
    "\n",
    "The TD error at time t is now:\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_t \\stackrel{.}{=} R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)\n",
    "\\end{align*}\n",
    "\n",
    "The equation 6.6 considering the initial $V_t$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V_t(S_t) &= R_{t+1} + \\gamma G_{t+1} - V_t(S_t) + \\gamma V_t(S_{t+1}) - \\gamma V_t(S_{t+1})\\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V_t(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V_t(S_{t+1}) + V_{t+1}(S_{t+1}) - V_{t+1}(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V_{t+1}(S_{t+1})) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V_{t+2}(S_{t+2})) + \\gamma^2 (V_{t+2}(S_{t+2}) - V_{t+1}(S_{t+2})) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V_T(S_T)) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) + \\gamma^2 (V_{t+2}(S_{t+2}) - V_{t+1}(S_{t+2})) + ... + \\gamma^{T-t} (V_T(S_T) - V_{T-1}(S_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) + \\gamma^2 (V_{t+2}(S_{t+2}) - V_{t+1}(S_{t+2})) + ... + \\gamma^{T-t} (V_T(S_T) - V_{T-1}(S_T)) \\\\\n",
    "&= \\left[ \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\right] + \\left[ \\sum_{k=t+1}^{T} \\gamma^{k-t} (V_k(S_k) - V_{k-1}(S_k)) \\right] \\\\\n",
    "&= \\left[ \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\right] + \\left[ \\sum_{k=t}^{T-1} \\gamma^{k-t+1} (V_{k+1}(S_{k+1}) - V_k(S_{k+1})) \\right] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} (V_{k+1}(S_{k+1}) - V_k(S_{k+1})) ]\n",
    "\\end{align*}\n",
    "\n",
    "We have $V_{k+1}(S_{k+1}) = V_k(S_{k+1})$, unless $S_{k+1} = S_k$, in which case there would be a change in the state-value for the state $S_{k+1} = S_k$ in the transition from the time-step k to k+1, and we would have $V_{k+1}(S_{k+1}) = V_{k+1}(S_k)$, because $S_k = S_{k+1}$. The equation $V_{k+1}(S_{k+1}) = V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_{k+1}(S_k) - V_k(S_{k+1}))$ corresponds to these 2 cases. So:\n",
    "\n",
    "\\begin{align*}\n",
    "V_{k+1}(S_{k+1}) &= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_{k+1}(S_k) - V_k(S_{k+1})) \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_k(S_k) + \\alpha [R_{k+1} + \\gamma V_k(S_{k+1}) - V_k(S_k)] - V_k(S_{k+1})) \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_k(S_k) + \\alpha [R_{k+1} + \\gamma V_k(S_{k+1}) - V_k(S_k)] - V_k(S_k)) \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} \\alpha [R_{k+1} + \\gamma V_k(S_{k+1}) - V_k(S_k)] \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} \\alpha \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V_t(S_t) &= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} (V_{k+1}(S_{k+1}) - V_k(S_{k+1})) ] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} (V_k(S_{k+1}) + 1_{S_{k+1} = S_k} \\alpha \\delta_k - V_k(S_{k+1})) ] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + 1_{S_{k+1} = S_k} \\gamma^{k-t+1} \\alpha \\delta_k ] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ (\\gamma^{k-t} + 1_{S_{k+1} = S_k} \\gamma^{k-t+1} \\alpha) \\delta_k ]\n",
    "\\end{align*}\n",
    "\n",
    "The above equation is the answer of this question. We can also consider the error using the values after the episode ended (after the terminal state $S_T$ is reached).\n",
    "\n",
    "The final value at the end of the episode, $V_T$, is:\n",
    "\n",
    "\\begin{align*}\n",
    "V(S_t) = V_T(S_t) &= V_{T-1}(S_t) + 1_{S_{T-1} = S_t} \\alpha [R_T + \\gamma V_{T-1}(S_T) - V_{T-1}(S_{T-1})] \\\\\n",
    "&= V_{T-1}(S_t) + 1_{S_{T-1} = S_t} \\alpha \\delta_{T-1} \\\\\n",
    "&= [V_{T-2}(S_t) + 1_{S_{T-2} = S_t} \\alpha \\delta_{T-2}] + 1_{S_{T-1} = S_t} \\alpha \\delta_{T-1} \\\\\n",
    "&= V_t(S_t) + \\alpha \\sum_{k=t}^{T-1} 1_{S_k = S_t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "in which $1_{S_k = S_t}$ means that the value will be 1 if $S_k = S_t$ and 0 otherwise, because $V(S_t)$ changes after the time-step k only if the state at that time-step, $S_k$, is the same state at the time-step t, $S_t$. \n",
    "\n",
    "It's important to keep in mind that the same state may be visited several times depending on the choosen actions and the environment dynamics. For the special case in which we have the state $S_t$ only visited at time t, then $V(S_t) = V_t(S_t) + \\alpha \\delta_t$ (there was only one change: it happened in the transition from t to t+1). In the other extreme, if the state $S_t$ was the only state for all time-steps before the terminal state, then $V(S_t) = V_t(S_t) + \\alpha \\sum_{k=t}^{T-1} \\delta_k$, which is the initial value at time t plus all the changes after each time-step.\n",
    "\n",
    "The new derivation (6.6) is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) &= G_t - V_t(S_t) - \\alpha \\sum_{k=t}^{T-1} 1_{S_k = S_t} \\delta_k \\\\\n",
    "&= \\left[ \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} 1_{S_{k+1} = S_k} \\alpha \\delta_k ] \\right] - \\alpha \\left[ \\sum_{k=t}^{T-1} 1_{S_k = S_t} \\delta_k \\right] \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\left[ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} 1_{S_{k+1} = S_k} \\alpha \\delta_k - 1_{S_k = S_t} \\alpha \\delta_k \\right] \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\left[ (\\gamma^{k-t} + \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right]\n",
    "\\end{align*}\n",
    "\n",
    "When $\\alpha$ is infinitesimally small, the above error becomes the Monte Carlo error:\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{lim}_{\\alpha \\to 0} \\sum_{k=t}^{T-1} \\left[ (\\gamma^{k-t} + \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right] = \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "The above cases were considered for TD(0). For n-step TD, the state-values change only in time-steps multiple of n, so we would have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) = \\sum_{k=t}^{T-1} \\left[ (\\gamma^{k-t} + 1_{(k+1) \\equiv 0 \\pmod{n}} \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right]\n",
    "\\end{align*}\n",
    "\n",
    "In the case in which $n \\gt T$, the condition $(k+1) \\equiv 0 \\pmod{n}$ would always be false, and the error above would become the Monte Carlo error, which is expected, because Monte Carlo is equivalent to TD(T+1), as there are T+1 time-steps in t = 0, 1, 2, ..., T-1, T.\n",
    "\n",
    "Actually, the above equation can be generalized to find the error for the state-value $V(S_t)$ after having calculated the values until the time-step $t_{target} > t$, for a n-step TD:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V_{t_{target}}(S_t) = \\sum_{k=t}^{t_{target}-1} \\left[ (\\gamma^{k-t} + 1_{(k+1) \\equiv 0 \\pmod{n}} \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right]\n",
    "\\end{align*}\n",
    "\n",
    "The above gives the current error for the state-value $V(S_t)$ (with $V_{t_{target}}(S_t)$ probably closer to the true value, $v_{\\pi} (S_t)$, than $V_t(S_t)$, because it may have been updated more times after t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2\n",
    "\n",
    "**Q**\n",
    "\n",
    "This is an exercise to help develop your intuition about why TD methods are often more efficient than Monte Carlo methods. Consider the driving home example and how it is addressed by TD and Monte Carlo methods. Can you imagine a scenario in which a TD update would be better on average than a Monte Carlo update? Give an example scenario—a description of past experience and a current state—in which you would expect the TD update to be better. Here’s a hint: Suppose you have lots of experience driving home from work. Then you move to a new building and a new parking lot (but you still enter the highway at the same place). Now you are starting to learn predictions for the new building. Can you see why TD updates are likely to be much better, at least initially, in this case? Might the same sort of thing happen in the original scenario?\n",
    "\n",
    "**A**\n",
    "\n",
    "One important advantage of TD over Monte Carlo is that TD adjusts itself during the episode. If in the given example each situation (leaving office, reach car, etc...) along with the elapsed time is considered to be a state, and the transitions between them as the single possible action, with the next state deterministic and the reward (the elapsed time for the step) depending on the environment dynamics, then the Monte Carlo method can be seen as an average over all the randomness in all possible steps, while TD adjusts for the randomness in each step (for TD(0)).\n",
    "\n",
    "The graphs show the total elapsed time, but policy evaluation for Monte Carlo and TD define the expected rewards (state values or state-action values) for the states, from that state onwards (not from the initial state). For $\\gamma = 1$ and $\\alpha = 1$, The Monte Carlo would change the expected reward of the state \"exiting the highway\" from 15 to 23, while TD(0) would change from 15 to 20 ($R + \\gamma G_{next} = 10 + 1 \\cdot 10 = 20$, with $G_{next}$ the expected reward of the next state, which is the remaining expected time when reaching the state \"2ndary road\", which in this case, because there's a single action in each state, corresponds to $V_{next}$).\n",
    "\n",
    "In the proposed case, when moving to a new building and a new parking lot, that trajectory would initially give wrong times (predicted rewards), which would affect the expected reward for the entire episode in the case of Monte Carlo, but TD mainly adjust for the new subpaths, while the known subpaths should give the same expected reward. This means that Monte Carlo expected reward should have a higher variance initially because the new subpath will end up affecting the entire expected reward, while TD would be more precise adjusting mainly the subpaths expected rewards (which will affect the total expected reward, but as a consequence). After lots episodes, the new subpaths will have the average expected reward and will be the same for Monte Carlo and TD. An interesting point is that if there's a need to change the environment to use the proposed scenario, then the values from TD could be bootstrapped to be used for the existing states (the new states should be defined arbitrarily), giving a faster convergence to the expected values.\n",
    "\n",
    "This also happen in the original scenario, but the error should reduce in both cases after a large amount of episodes. Considering that it's a real scenario with a huge amount of possible rewards (it has a high entropy due to rain, traffic, and a lot of other situations that can happen), TD should converge faster to more precise rewards because it can adapt better for each step, instead of just having a global view. There should be exceptions to this, especially in scenarios in which a change in the reward of a state affect the reward of another, like choosing a different highway that takes a bit more time, but ends at the end of the secondary road, reducing the total time (this wouldn't be an MDP environment tough; to turn it in an MDP the states should be divided in more states to preserve the MDP property; in this case, removing the state \"2ndary road\" and adding the states \"2ndary road - part 1\" and \"2ndary road - part 2\" that corresponds the parts of the secondary road before and after the intersection with the new highway, respectively, should turn it back into an MDP).\n",
    "\n",
    "It's important to note that TD only adjusts the expected reward (value) for the previous state (after transitioning from $S_1$ to $S_2$ due to action $A_1$ and receiving the reward $R_2$, it will update the state value V($S_1$), or the state-action value Q($S_1$, $A_1$)). In the proposed scenario, the states do not repeat in an episode, so the changes will only benefit the next episode, even for TD(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.3\n",
    "\n",
    "**Q**\n",
    "\n",
    "From the results shown in the left graph of the random walk example it appears that the first episode results in a change in only V(A). What does this tell you about what happened on the first episode? Why was only the estimate for this one state changed? By exactly how much was it changed?\n",
    "\n",
    "**A**\n",
    "\n",
    "The first episoded ended in the terminal state that returns a reward of 0 (the left terminal state). \n",
    "\n",
    "Initially, all states have the same state value (0.5), and because all the rewards are 0, there was no change in the state-value before the terminal state (because the task is undiscounted, that is, $\\gamma = 1$):\n",
    "\n",
    "\\begin{align*}\n",
    "V(S_t) \\gets V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)] = 0.5 + \\alpha [0 + 1 \\cdot 0.5 - 0.5] = 0.5 + \\alpha \\cdot 0 = 0.5\n",
    "\\end{align*}\n",
    "\n",
    "The only exception is for the last time-step in the episode, because $V(S_T) = 0$, and the reward received going from A to the terminal state at the left ($R_{T_{left}}$) is 0:\n",
    "\n",
    "\\begin{align*}\n",
    "V(A) \\gets V(A) + \\alpha [R_{T_{left}} + \\gamma V(S_T) - V(A)] = V(A) + \\alpha \\cdot [0 + 1 \\cdot 0 - V(A)] = V(A) - \\alpha V(A) = (1 - \\alpha) V(A)\n",
    "\\end{align*}\n",
    "\n",
    "The case shown in the left graph used $\\alpha = 0.1$, so:\n",
    "\n",
    "\\begin{align*}\n",
    "V(A) \\gets (1 - \\alpha) V(A) = (1 - 0.1) \\cdot 0.5 = 0.9 \\cdot 0.5 = 0.45\n",
    "\\end{align*}\n",
    "\n",
    "The state-value of A decreased by 0.05 (from 0.5 to 0.45)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.4\n",
    "\n",
    "**Q**\n",
    "\n",
    "The specific results shown in the right graph of the random walk example are dependent on the value of the step-size parameter, $\\alpha$. Do you think the conclusions\n",
    "about which algorithm is better would be affected if a wider range of $\\alpha$ values were used? Is there a different, fixed value of $\\alpha$ at which either algorithm would have performed significantly better than shown? Why or why not?\n",
    "\n",
    "**A**\n",
    "\n",
    "If a wider range of $\\alpha$ values were used, the TD algorithm should still have better results, on average. \n",
    "\n",
    "The general rule is that the lesser the value of $\\alpha$, the smaller the error will be after a lot of episodes, with less variance between episodes (similar to how derivation finds the direction of a curve after an infinitesimal distance, with greater error the larger the distance considered is). \n",
    "\n",
    "Although very small values of $\\alpha$ end up with less variance, they take longer start giving better values (which can be seen for the cases of $\\alpha = 0.05$ for TD and $\\alpha = 0.01$ for MC). On the other hand, large values of $\\alpha$ will change faster, but with greater variance, ending up worse than the cases shown for smaller $\\alpha$ after a sufficient amount of episodes (like $\\alpha = 0.15$ for TD and $\\alpha = 0.04$ for MC).\n",
    "\n",
    "Smaller values of $\\alpha$ should give better results after a large amount of episodes, but for the 100 episodes of the proposed scenario, the algorithm would not have performed significantly better than shown (it could end up being worse because it would take too long to give good results, giving a worse result after 100 episodes than a larger $\\alpha$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.5\n",
    "\n",
    "**Q**\n",
    "\n",
    "In the right graph of the random walk example, the RMS error of the TD method seems to go down and then up again, particularly at high $\\alpha$’s. What could have caused this? Do you think this always occurs, or might it be a function of how the approximate value function was initialized?\n",
    "\n",
    "**A**\n",
    "\n",
    "As the predicted state-values approach the true state-values, the changes to the values will have more impact in the error, causing the error to increase sometimes, although it might decrease again after more episodes. Smaller values of $\\alpha$ decrease this variance, with an infinitesimally small $\\alpha$ giving a smooth graph (see the previous exercise). This situation should generally occur, epecially at high $\\alpha$’s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.6\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Example 6.2 we stated that the true values for the random walk example are $\\frac{1}{6}$, $\\frac{2}{6}$, $\\frac{3}{6}$, $\\frac{4}{6}$ and $\\frac{5}{6}$, for states A through E. Describe at least two different ways that these could have been computed. Which would you guess we actually used? Why?\n",
    "\n",
    "**A**\n",
    "\n",
    "One and more general way to solve it is to solve the system of equations for every state value ($\\gamma = 1$ because the task is undiscounted):\n",
    "\n",
    "\\begin{align*}\n",
    "V(s) = \\mathbb{E} [R_{t+1} + \\gamma V(S_{t+1}), S_t=s] = \\mathbb{E} [R_{t+1} + V(S_{t+1}), S_t=s] = 0.5 \\cdot [R_{t_{left}} + V(S_{t_{left}})] + 0.5 \\cdot [R_{t_{right}} + V(S_{t_{right}})]\n",
    "\\end{align*}\n",
    "\n",
    "Each state has 50% chance of going either left or right, so each case is multiplied (weighted) by 0.5.\n",
    "\n",
    "For all states A through E:\n",
    "\n",
    "\\begin{align*}\n",
    "V(A) &= 0.5 \\cdot [R_{T_{left}} + V(S_{T_{left}})] + 0.5 \\cdot [R_B + V(B)] = 0.5 \\cdot [0 + 0] + 0.5 \\cdot [0 + V(B)] = 0.5 \\cdot V(B) \\\\\n",
    "V(B) &= 0.5 \\cdot [R_A + V(A)] + 0.5 \\cdot [R_C + V(C)] = 0.5 \\cdot [0 + V(A)] + 0.5 \\cdot [0 + V(C)] = 0.5 \\cdot [V(A) + V(C)] \\\\\n",
    "V(C) &= 0.5 \\cdot [R_B + V(B)] + 0.5 \\cdot [R_D + V(D)] = 0.5 \\cdot [0 + V(B)] + 0.5 \\cdot [0 + V(D)] = 0.5 \\cdot [V(B) + V(D)] \\\\\n",
    "V(D) &= 0.5 \\cdot [R_C + V(C)] + 0.5 \\cdot [R_E + V(E)] = 0.5 \\cdot [0 + V(C)] + 0.5 \\cdot [0 + V(E)] = 0.5 \\cdot [V(C) + V(E)] \\\\\n",
    "V(E) &= 0.5 \\cdot [R_D + V(D)] + 0.5 \\cdot [R_{T_{right}} + V(S_{T_{right}})] = 0.5 \\cdot [0 + V(D)] + 0.5 \\cdot [1 + 0] = 0.5 \\cdot V(D) + 0.5\n",
    "\\end{align*}\n",
    "\n",
    "V(B) in terms of V(A):\n",
    "\n",
    "\\begin{align*}\n",
    "V(A) &= 0.5 \\cdot V(B) \\\\\n",
    "V(B) &= 2 \\cdot V(A)\n",
    "\\end{align*}\n",
    "\n",
    "V(C) in terms of V(A):\n",
    "\n",
    "\\begin{align*}\n",
    "V(B) &= 0.5 \\cdot [V(A) + V(C)] \\\\\n",
    "2 \\cdot V(A) &= 0.5 \\cdot V(A) + 0.5 \\cdot V(C) \\\\\n",
    "1.5 \\cdot V(A) &= 0.5 \\cdot V(C) \\\\\n",
    "V(C) &= 3 \\cdot V(A) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "V(D) in terms of V(A):\n",
    "\n",
    "\\begin{align*}\n",
    "V(C) &= 0.5 \\cdot [V(B) + V(D)] \\\\\n",
    "3 \\cdot V(A) V(B) &= 0.5 (2 \\cdot V(A)) + 0.5 \\cdot V(D) \\\\\n",
    "3 \\cdot V(A) V(B) &= V(A) + 0.5 \\cdot V(D) \\\\\n",
    "2 \\cdot V(A) V(B) &= 0.5 \\cdot V(D) \\\\\n",
    "V(D) &= 4 \\cdot V(A)\n",
    "\\end{align*}\n",
    "\n",
    "V(E) in terms of V(A):\n",
    "\n",
    "\\begin{align*}\n",
    "V(D) &= 0.5 \\cdot [V(C) + V(E)] \\\\\n",
    "4 \\cdot V(A) &= 0.5 (3 \\cdot V(A)) + 0.5 \\cdot V(E) \\\\\n",
    "8 \\cdot V(A) &= 3 \\cdot V(A) + V(E) \\\\\n",
    "V(E) &= 5 \\cdot V(A)\n",
    "\\end{align*}\n",
    "\n",
    "Solving for V(A):\n",
    "\n",
    "\\begin{align*}\n",
    "V(E) &= 0.5 \\cdot V(D) + 0.5 \\\\\n",
    "2 \\cdot V(E) &= V(D) + 1 \\\\\n",
    "10 \\cdot V(A) &= 4 \\cdot V(A) + 1 \\\\\n",
    "6 \\cdot V(A) &= 1 \\\\\n",
    "V(A) &= \\frac{1}{6}\n",
    "\\end{align*}\n",
    "\n",
    "Solving for V(B), V(C), V(D) and V(E):\n",
    "\n",
    "\\begin{align*}\n",
    "V(B) &= 2 \\cdot V(A) = 2 \\cdot \\frac{1}{6} = \\frac{2}{6} \\\\\n",
    "V(C) &= 3 \\cdot V(A) = 3 \\cdot \\frac{1}{6} = \\frac{3}{6} \\\\\n",
    "V(D) &= 4 \\cdot V(A) = 4 \\cdot \\frac{1}{6} = \\frac{4}{6} \\\\\n",
    "V(E) &= 5 \\cdot V(A) = 5 \\cdot \\frac{1}{6} = \\frac{5}{6}\n",
    "\\end{align*}\n",
    "\n",
    "Due to the symmetry of the environment, the reward of going to the leftmost terminal state being 0 and the rightmost 1, with all other rewards 0, and being undiscounted, with an odd number of states with equally random chances of going left or right, the state at the center, C, could already be considered having a return of 0.5, so $V(C) = 0.5$, and then the values of the other states could be found faster.\n",
    "\n",
    "Actually, due to the symmetry, a convenient way to define the returns (and state-values), is to attribute weights from the leftmost state (terminal) to the rightmost (terminal), from 0 to 6 ($weight_{left} = 0$, $weight_A = 1$, ..., $weight_E = 5$, $weight_{right} = 6$, that is, 7 weights for 7 states, with 5 inner weights corresponding to the non-terminal states, the worst/leftmost state with minimum weight and the best/rightmost state with maximum weight), and divide by the maximum weight (6) to define the values of the states, $V(left) = \\frac{0}{6} = 0$, $V(A) = \\frac{1}{6}$, ..., $V(E) = \\frac{5}{6}$, $V(right) = \\frac{6}{6} = 1$, and then ignore the first and last states (terminal states have value 0; they were given other values just for the sake of continuity, with weights to define a continuous sequence from 0 to 1). This is probably the way that the values were computed due to its simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.7\n",
    "\n",
    "**Q**\n",
    "\n",
    "Design an off-policy version of the TD(0) update that can be used with arbitrary target policy $\\pi$ and covering behavior policy b, using at each step t the importance sampling ratio $\\rho_{t:t}$ (5.3).\n",
    "\n",
    "**A**\n",
    "\n",
    "The equation 5.3 is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\rho_{t:T-1} \\stackrel{.}{=} \\frac{\\prod_{k=t}^{T-1} \\pi(A_k | S_k) p(S_{k+1} | S_k, A_k)}{\\prod_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k | S_k)}{b(A_k | S_k)}\n",
    "\\end{align*}\n",
    "\n",
    "For TD(0), the policy evaluation is done after each time-step, so it's only considered the transition from the current to the next state:\n",
    "\n",
    "\\begin{align*}\n",
    "\\rho_{t:t} \\stackrel{.}{=} \\prod_{k=t}^{t} \\frac{\\pi(A_k | S_k)}{b(A_k | S_k)} = \\frac{\\pi(A_t | S_t)}{b(A_t | S_t)}\n",
    "\\end{align*}\n",
    "\n",
    "which is the relation of the probabilities of choosing the action $A_t$ in the state $S_t$ under the target policy $\\pi$ compared to the probability of choosing it under the behavior policy $b$. For the cases in which the policy $\\pi$ has no probability of choosing that action, $\\rho = 0$, so these cases will not be used during the target policy improvement (the probability of $b$ choosing that action is always higher than 0, because it was the actual policy used during the evaluation, which means that the action $A_t$ was choosen by $b$ at the time-step t).\n",
    "\n",
    "The algorithm for off-policy TD(0) control is:\n",
    "\n",
    "> Input: an arbitrary target policy $\\pi$, discount factor $\\gamma$ and step-size parameter $\\alpha > 0$\n",
    ">\n",
    "> Initialize, for all $s \\in \\mathcal{S}$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$V(s) \\in \\mathbb{R}$ (arbitrarily)<br/>\n",
    ">\n",
    ">Loop forever (for each episode):<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$b \\gets$ any policy with coverage of $\\pi$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;For each time-step t in the episode, with $S_t$, $A_t$, $R_{t+1}$ and $S_{t+1}$ being the state, action, reward and next state, respectively:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$W \\gets \\frac{\\pi(A_t | S_t)}{b(A_t | S_t)}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(S_t) \\gets V(S_t) + W \\cdot \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$<br/>\n",
    "\n",
    "The above algorithm can be resumed in the following update at each time-step t:\n",
    "\n",
    "\\begin{align*}\n",
    "V(S_t) \\gets V(S_t) + \\rho_{t:t} \\cdot \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]\n",
    "\\end{align*}\n",
    "\n",
    "Note that the state-value is not updated when $\\pi(A_t | S_t) = 0$, because $\\rho_{t:t} = 0$ (the value should not be updated for the cases in which the action $A_t$ can not be choosen by the target policy $\\pi$ in the state $S_t$), and for the cases in which both policies have the same probability of choosing $A_t$ given $S_t$, then the update is the same as the update that would be done in an on-policy update over $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.8 \n",
    "\n",
    "**Q**\n",
    "\n",
    "Show that an action-value version of (6.6) holds for the action-value form of the TD error $\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$, again assuming that the values don’t change from step to step.\n",
    "\n",
    "**A**\n",
    "\n",
    "Equation 6.6 is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1}) \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V(S_{t+2})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V(S_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "The action-value version of it is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - Q(S_t, A_t) &= R_{t+1} + \\gamma G_{t+1} - Q(S_t, A_t) + \\gamma Q(S_{t+1}, A_{t+1}) - \\gamma Q(S_{t+1}, A_{t+1}) \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - Q(S_{t+1}, A_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - Q(S_{t+2}, A_{t+2})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - Q(S_T, A_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.9\n",
    "\n",
    "**Q**\n",
    "\n",
    "Windy Gridworld with King’s Moves (programming) Re-solve the windy gridworld assuming eight possible actions, including the diagonal moves, rather than the usual four. How much better can you do with the extra actions? Can you do even better by including a ninth action that causes no movement at all other than that caused by the wind?\n",
    "\n",
    "**A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy as np\n",
    "from numpy.random import MT19937, Generator\n",
    "\n",
    "S = typing.TypeVar('S')\n",
    "A = typing.TypeVar('A')\n",
    "\n",
    "def random_generator(seed: int | None = None):\n",
    "    bg = MT19937(seed)\n",
    "    rg = Generator(bg)\n",
    "    return rg\n",
    "\n",
    "class BaseEnv(typing.Generic[S, A]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        states: list[S],\n",
    "        actions: list[A],\n",
    "        terminal_states: list[int],\n",
    "    ):\n",
    "        num_states = len(states)\n",
    "        num_actions = len(actions)\n",
    "\n",
    "        assert num_states > 0\n",
    "        assert num_actions > 0\n",
    "\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.terminal_states = terminal_states\n",
    "\n",
    "    def reset(self, seed: int | None) -> S:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step(self, action: A) -> tuple[S, float, bool, bool]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class AgentData:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: list[list[float]],\n",
    "        q: list[float],\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.q = q\n",
    "\n",
    "class TrainResult:\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps: list[int],\n",
    "        returns: list[float],\n",
    "    ):\n",
    "        self.steps = steps\n",
    "        self.returns = returns\n",
    "        self.greedy_steps: int | None = None\n",
    "        self.greedy_returns: float | None = None\n",
    "\n",
    "class BaseAgent(typing.Generic[S, A]):\n",
    "\n",
    "    def action(self, state: S) -> A:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, episodes: int) -> TrainResult:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def reset(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class TDAgentOnPolicy(BaseAgent[S, A], typing.Generic[S, A]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: BaseEnv[S, A],\n",
    "        alpha: float = 0.5,\n",
    "        gamma: float = 1.0,\n",
    "        epsilon: float = 0.1,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        assert alpha > 0\n",
    "        assert alpha <= 1\n",
    "        assert gamma >= 0\n",
    "        assert gamma <= 1\n",
    "        assert epsilon > 0\n",
    "        assert epsilon <= 1\n",
    "\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        num_states = env.num_states\n",
    "        num_actions = env.num_actions\n",
    "\n",
    "        other_prob = epsilon/num_actions\n",
    "        main_prob = 1.0 - epsilon + other_prob\n",
    "\n",
    "        q: list[list[float]] = [[0.0 for _ in range(num_actions)] for _ in range(num_states)]\n",
    "\n",
    "        self.main_prob = main_prob\n",
    "        self.other_prob = other_prob\n",
    "        self.initial_q = q\n",
    "        self.q = q\n",
    "        self.rg = random_generator(seed)\n",
    "\n",
    "    def get_probs(self, s: int) -> list[float]:\n",
    "        main_prob = self.main_prob\n",
    "        other_prob = self.other_prob\n",
    "        qs = self.q[s]\n",
    "\n",
    "        best_a = np.argmax(qs)\n",
    "        probs = [main_prob if a == best_a else other_prob for a in range(len(qs))]\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def action(self, state: S, greedy=False) -> A:\n",
    "        env = self.env\n",
    "\n",
    "        s = env.states.index(state)\n",
    "        probs = self.get_probs(s)\n",
    "        a = np.argmax(probs) if greedy else self.rg.choice(len(probs), p=probs)\n",
    "        action = env.actions[a]\n",
    "\n",
    "        return action\n",
    "\n",
    "    def run_episode(self, greedy: bool, seed: int | None) -> TrainResult:\n",
    "        env = self.env\n",
    "        q = self.q\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "        state = env.reset(seed)\n",
    "        action = self.action(state, greedy=greedy)\n",
    "        done = False\n",
    "        steps = 0\n",
    "        ret: float = 0.0\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated = env.step(action)\n",
    "            steps += 1\n",
    "            ret += reward\n",
    "            next_action = self.action(next_state, greedy=greedy)\n",
    "            s = env.states.index(state)\n",
    "            a = env.actions.index(action)\n",
    "            ns = env.states.index(next_state)\n",
    "            na = env.actions.index(next_action)\n",
    "\n",
    "            if not greedy:\n",
    "                q[s][a] += alpha * (reward + gamma * q[ns][na] - q[s][a])\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            done = terminated or truncated\n",
    "\n",
    "        return steps, ret\n",
    "\n",
    "    def train(self, episodes: int, seed: int | None = None, run_greedy=True, print_results=True) -> TrainResult:\n",
    "        print_every = max((episodes // 10), 1)\n",
    "        result = TrainResult(steps=[], returns=[])\n",
    "\n",
    "        for i_episode in range(episodes):\n",
    "            episode_num = i_episode + 1\n",
    "\n",
    "            steps, ret = self.run_episode(greedy=False, seed=seed)\n",
    "\n",
    "            result.steps.append(steps)\n",
    "            result.returns.append(ret)\n",
    "\n",
    "            if print_results and ((episode_num % print_every == 0) or (episode_num == episodes)):\n",
    "                print(f'Episode #{episode_num}: steps={steps} return={ret:.0f}')\n",
    "\n",
    "        if run_greedy:\n",
    "            steps, ret = self.run_episode(greedy=True, seed=seed)\n",
    "            self.greedy_steps = steps\n",
    "            self.greedy_returns = ret\n",
    "\n",
    "            if print_results:\n",
    "                print(f'Greedy Policy: steps={steps} return={ret:.0f}')\n",
    "\n",
    "        return result\n",
    "\n",
    "    def reset(self) -> AgentData:\n",
    "        q = self.initial_q\n",
    "        self.initial_q = q\n",
    "        self.q = q\n",
    "\n",
    "class WindGridEnvBase(BaseEnv[tuple[int, int], tuple[int, int]]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        actions: list[tuple[int, int]],\n",
    "        stochastic=False,\n",
    "        max_steps=100,\n",
    "    ):\n",
    "        num_x = 10\n",
    "        num_y = 7\n",
    "        winds = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
    "        initial_state = (0, 3)\n",
    "        final_state = (7, 3)\n",
    "        states = [(x, y) for y in range(num_y) for x in range(num_x)]\n",
    "        terminal_states = [states.index(final_state)]\n",
    "\n",
    "        super().__init__(states=states, actions=actions, terminal_states=terminal_states)\n",
    "\n",
    "        self.stochastic = stochastic\n",
    "        self.max_steps = max_steps\n",
    "        self.initial_state = initial_state\n",
    "        self.final_state = final_state\n",
    "        self.num_x = num_x\n",
    "        self.num_y = num_y\n",
    "        self.winds = winds\n",
    "\n",
    "        self.steps = 0\n",
    "        self.rg = random_generator(None)\n",
    "        self.state: tuple[int, int] | None = None\n",
    "\n",
    "    def reset(self, seed: int | None) -> S:\n",
    "        self.steps = 0\n",
    "        self.state = self.initial_state\n",
    "        if seed is not None:\n",
    "            self.rg = random_generator(seed)\n",
    "        return self.state\n",
    "\n",
    "    def normalize_state(self, state: tuple[int, int]) -> tuple[S, float, bool, bool]:\n",
    "        num_x = self.num_x\n",
    "        num_y = self.num_y\n",
    "\n",
    "        s_x, s_y = state\n",
    "\n",
    "        x = max(min(s_x, num_x-1), 0)\n",
    "        y = max(min(s_y, num_y-1), 0)\n",
    "\n",
    "        return (x, y)\n",
    "\n",
    "    def apply_move(self, state: tuple[int, int], move: tuple[int, int]) -> tuple[int, int]:\n",
    "        tmp_state = (state[0] + move[0], state[1] + move[1])\n",
    "        return self.normalize_state(tmp_state)\n",
    "\n",
    "    def step(self, action: A) -> tuple[S, float, bool, bool]:\n",
    "        stochastic = self.stochastic\n",
    "        max_steps = self.max_steps\n",
    "        final_state = self.final_state\n",
    "        winds = self.winds\n",
    "        steps = self.steps\n",
    "        state = self.state\n",
    "        rg = self.rg\n",
    "\n",
    "        assert state is not None\n",
    "        assert state != final_state\n",
    "        assert steps < max_steps\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        tmp_state = self.apply_move(state, action)\n",
    "        wind = winds[state[0]]\n",
    "        wind += 0 if not stochastic else rg.choice([-1, 0, 1])\n",
    "        next_state = self.apply_move(tmp_state, (0, wind))\n",
    "\n",
    "        reward = -1\n",
    "        terminated = next_state == final_state\n",
    "        truncated = (not terminated) and (steps >= max_steps)\n",
    "\n",
    "        self.steps = steps\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, terminated, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot(env: WindGridEnvBase, agent: TDAgentOnPolicy, result: TrainResult):\n",
    "    steps = result.steps\n",
    "    q = agent.q\n",
    "    num_x = env.num_x\n",
    "    num_y = env.num_y\n",
    "    winds = env.winds\n",
    "\n",
    "    cumsteps = [0]\n",
    "    sum_steps = 0\n",
    "\n",
    "    for ep_steps in steps:\n",
    "        sum_steps += ep_steps\n",
    "        cumsteps += [sum_steps]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Episodes')\n",
    "    plt.plot(cumsteps, range(len(cumsteps)))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Time steps (Episode)')\n",
    "    plt.plot(range(len(steps) + 1), [0] + steps)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Last 100 episodes')\n",
    "    plt.ylabel('Time steps (Episode)')\n",
    "    plt.plot(range(len(steps) - len(steps[-100:]) + 1, len(steps) + 1), steps[-100:])\n",
    "\n",
    "    def idx(x: int, y: int) -> int:\n",
    "        return (num_y - y - 1) * num_x + x\n",
    "\n",
    "    v = [max(qs) for qs in q]\n",
    "    v_table = [[v[idx(x, y)] for x in range(num_x)] for y in range(num_y)]\n",
    "    annot = [[\n",
    "        f'{v[idx(x, y)]:.1f}\\n({x}, {num_y - y - 1})\\n{env.actions[np.argmax(q[idx(x, y)])]}'\n",
    "        for x in range(num_x)] for y in range(num_y)\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    sns.heatmap(\n",
    "        v_table,\n",
    "        fmt='',\n",
    "        cmap='viridis',\n",
    "        annot=annot,\n",
    "        xticklabels=winds,\n",
    "        yticklabels=[])\n",
    "    plt.title('Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Env\n",
    "seed = 4\n",
    "env = WindGridEnvBase(\n",
    "    actions=[(0, 1), (1, 0), (0, -1), (-1, 0)],\n",
    "    stochastic=False,\n",
    "    max_steps=2000)\n",
    "agent = TDAgentOnPolicy(env=env, seed=seed)\n",
    "result = agent.train(episodes=1000, seed=seed)\n",
    "print(f'Time Steps: {sum(result.steps)}')\n",
    "plot(env=env, agent=agent, result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer (8 actions)\n",
    "seed = 4\n",
    "env = WindGridEnvBase(\n",
    "    actions=[item for item in [(x, y) for x in [-1, 0, 1]  for y in [-1, 0, 1]] if item != (0, 0)],\n",
    "    stochastic=False,\n",
    "    max_steps=2000)\n",
    "agent = TDAgentOnPolicy(env=env, seed=seed)\n",
    "result = agent.train(episodes=1000, seed=seed)\n",
    "print(f'Time Steps: {sum(result.steps)}')\n",
    "plot(env=env, agent=agent, result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer (9 actions)\n",
    "seed = 6\n",
    "env = WindGridEnvBase(\n",
    "    actions=[(x, y) for x in [-1, 0, 1]  for y in [-1, 0, 1]],\n",
    "    stochastic=False,\n",
    "    max_steps=2000)\n",
    "agent = TDAgentOnPolicy(env=env, seed=seed)\n",
    "result = agent.train(episodes=5000, seed=seed)\n",
    "print(f'Time Steps: {sum(result.steps)}')\n",
    "plot(env=env, agent=agent, result=result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ninth action caused no difference in the results, which is expected, considering that it does nothing and decreases the reward due to the time-step spent; one exception would be the state exactly 2 cells below the result, which would benefit by doing nothing due to the wind, but the optimal policy don't go there, it can use 7 steps using other paths, like in the case with 8 actions, so this ninth action does not help in this case (like shown in the heatmap above the values in the cells, with the best action at the bottom; cells with value 0 can't be reached, except for the final state; 5000 episodes were iterated to define correctly the value -1 for the best action in the cell (7, 1), with action (0, 0), to exemplify a case in which it helps, even tough the minimum path with or without this action remains with 7 steps). The ninth action could help in a stochastic scenario, tough, like the next example (because the greedy agent over Q could end up in the position (7, 1), due to the randomness of the environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.10\n",
    "\n",
    "**Q**\n",
    "\n",
    "*Stochastic Wind (programming)* Re-solve the windy gridworld task with King’s moves, assuming that the effect of the wind, if there is any, is stochastic, sometimes varying by 1 from the mean values given for each column. That is, a third of the time you move exactly according to these values, as in the previous exercise, but also a third of the time you move one cell above that, and another third of the time you move one cell below that. For example, if you are one cell to the right of the goal and you move left, then one-third of the time you move one cell above the goal, one-third of the time you move two cells above the goal, and one-third of the time you move to the goal.\n",
    "\n",
    "**A**\n",
    "\n",
    "Due to the randomness of the possible next states given a state and action, the action-values are more distributed, with the states close to the terminal state with lower values than in the non-stochastic scenario. The greedy policy could solve in 8 steps in the example below, but changing the seeds give more variance than without the randomness (although the changing of seeds could cause differences in both cases due to the definition of the actions depending on $\\epsilon$ for the soft policy, which would give different actions and next states, as well as action-values, in both scenarios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Wind\n",
    "seed = 1\n",
    "env = WindGridEnvBase(\n",
    "    actions=[item for item in [(x, y) for x in [-1, 0, 1]  for y in [-1, 0, 1]] if item != (0, 0)],\n",
    "    stochastic=True,\n",
    "    max_steps=2000)\n",
    "agent = TDAgentOnPolicy(env=env, seed=seed)\n",
    "result = agent.train(episodes=5000, seed=seed)\n",
    "print(f'Time Steps: {sum(result.steps)}')\n",
    "plot(env=env, agent=agent, result=result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
