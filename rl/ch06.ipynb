{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 06 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "If V changes during the episode, then (6.6) only holds approximately; what would the difference be between the two sides? Let $V_t$ denote the array of state values used at time t in the TD error (6.5) and in the TD update (6.2). Redo the derivation above to determine the additional amount that must be added to the sum of TD errors in order to equal the Monte Carlo error.\n",
    "\n",
    "**A**\n",
    "\n",
    "The equation 6.2, that defines how the values are updated using the TD method, is:\n",
    "\n",
    "\\begin{align*}\n",
    "V(S_t) \\gets V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)] \\tag{6.2}\n",
    "\\end{align*}\n",
    "\n",
    "The TD error (equation 6.5) is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_t \\stackrel{.}{=} R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\tag{6.5}\n",
    "\\end{align*}\n",
    "\n",
    "The equation 6.6 is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1}) \\tag{from (3.9)} \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V(S_{t+2})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V(S_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\tag{6.6}\n",
    "\\end{align*}\n",
    "\n",
    "If V changes during the episode, then at the start of time t+1 the new V would be (for TD(0)):\n",
    "\n",
    "\\begin{align*}\n",
    "V_{t+1}(S_t) \\gets V_t(S_t) + \\alpha [R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)]\n",
    "\\end{align*}\n",
    "\n",
    "where $V_t$ hold the state values at time t.\n",
    "\n",
    "The TD error at time t is now:\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_t \\stackrel{.}{=} R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)\n",
    "\\end{align*}\n",
    "\n",
    "The equation 6.6 considering the initial $V_t$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V_t(S_t) &= R_{t+1} + \\gamma G_{t+1} - V_t(S_t) + \\gamma V_t(S_{t+1}) - \\gamma V_t(S_{t+1})\\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V_t(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V_t(S_{t+1}) + V_{t+1}(S_{t+1}) - V_{t+1}(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V_{t+1}(S_{t+1})) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V_{t+2}(S_{t+2})) + \\gamma^2 (V_{t+2}(S_{t+2}) - V_{t+1}(S_{t+2})) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V_T(S_T)) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) + \\gamma^2 (V_{t+2}(S_{t+2}) - V_{t+1}(S_{t+2})) + ... + \\gamma^{T-t} (V_T(S_T) - V_{T-1}(S_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) + \\gamma^2 (V_{t+2}(S_{t+2}) - V_{t+1}(S_{t+2})) + ... + \\gamma^{T-t} (V_T(S_T) - V_{T-1}(S_T)) \\\\\n",
    "&= \\left[ \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\right] + \\left[ \\sum_{k=t+1}^{T} \\gamma^{k-t} (V_k(S_k) - V_{k-1}(S_k)) \\right] \\\\\n",
    "&= \\left[ \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\right] + \\left[ \\sum_{k=t}^{T-1} \\gamma^{k-t+1} (V_{k+1}(S_{k+1}) - V_k(S_{k+1})) \\right] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} (V_{k+1}(S_{k+1}) - V_k(S_{k+1})) ]\n",
    "\\end{align*}\n",
    "\n",
    "We have $V_{k+1}(S_{k+1}) = V_k(S_{k+1})$, unless $S_{k+1} = S_k$, in which case there would be a change in the state-value for the state $S_{k+1} = S_k$ in the transition from the time-step k to k+1, and we would have $V_{k+1}(S_{k+1}) = V_{k+1}(S_k)$, because $S_k = S_{k+1}$. The equation $V_{k+1}(S_{k+1}) = V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_{k+1}(S_k) - V_k(S_{k+1}))$ corresponds to these 2 cases. So:\n",
    "\n",
    "\\begin{align*}\n",
    "V_{k+1}(S_{k+1}) &= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_{k+1}(S_k) - V_k(S_{k+1})) \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_k(S_k) + \\alpha [R_{k+1} + \\gamma V_k(S_{k+1}) - V_k(S_k)] - V_k(S_{k+1})) \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_k(S_k) + \\alpha [R_{k+1} + \\gamma V_k(S_{k+1}) - V_k(S_k)] - V_k(S_k)) \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} \\alpha [R_{k+1} + \\gamma V_k(S_{k+1}) - V_k(S_k)] \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} \\alpha \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V_t(S_t) &= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} (V_{k+1}(S_{k+1}) - V_k(S_{k+1})) ] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} (V_k(S_{k+1}) + 1_{S_{k+1} = S_k} \\alpha \\delta_k - V_k(S_{k+1})) ] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + 1_{S_{k+1} = S_k} \\gamma^{k-t+1} \\alpha \\delta_k ] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ (\\gamma^{k-t} + 1_{S_{k+1} = S_k} \\gamma^{k-t+1} \\alpha) \\delta_k ]\n",
    "\\end{align*}\n",
    "\n",
    "The above equation is the answer of this question. We can also consider the error using the values after the episode ended (after the terminal state $S_T$ is reached).\n",
    "\n",
    "The final value at the end of the episode, $V_T$, is:\n",
    "\n",
    "\\begin{align*}\n",
    "V(S_t) = V_T(S_t) &= V_{T-1}(S_t) + 1_{S_{T-1} = S_t} \\alpha [R_T + \\gamma V_{T-1}(S_T) - V_{T-1}(S_{T-1})] \\\\\n",
    "&= V_{T-1}(S_t) + 1_{S_{T-1} = S_t} \\alpha \\delta_{T-1} \\\\\n",
    "&= [V_{T-2}(S_t) + 1_{S_{T-2} = S_t} \\alpha \\delta_{T-2}] + 1_{S_{T-1} = S_t} \\alpha \\delta_{T-1} \\\\\n",
    "&= V_t(S_t) + \\alpha \\sum_{k=t}^{T-1} 1_{S_k = S_t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "in which $1_{S_k = S_t}$ means that the value will be 1 if $S_k = S_t$ and 0 otherwise, because $V(S_t)$ changes after the time-step k only if the state at that time-step, $S_k$, is the same state at the time-step t, $S_t$. \n",
    "\n",
    "It's important to keep in mind that the same state may be visited several times depending on the choosen actions and the environment dynamics. For the special case in which we have the state $S_t$ only visited at time t, then $V(S_t) = V_t(S_t) + \\alpha \\delta_t$ (there was only one change: it happened in the transition from t to t+1). In the other extreme, if the state $S_t$ was the only state for all time-steps before the terminal state, then $V(S_t) = V_t(S_t) + \\alpha \\sum_{k=t}^{T-1} \\delta_k$, which is the initial value at time t plus all the changes after each time-step.\n",
    "\n",
    "The new derivation (6.6) is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) &= G_t - V_t(S_t) - \\alpha \\sum_{k=t}^{T-1} 1_{S_k = S_t} \\delta_k \\\\\n",
    "&= \\left[ \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} 1_{S_{k+1} = S_k} \\alpha \\delta_k ] \\right] - \\alpha \\left[ \\sum_{k=t}^{T-1} 1_{S_k = S_t} \\delta_k \\right] \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\left[ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} 1_{S_{k+1} = S_k} \\alpha \\delta_k - 1_{S_k = S_t} \\alpha \\delta_k \\right] \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\left[ (\\gamma^{k-t} + \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right]\n",
    "\\end{align*}\n",
    "\n",
    "When $\\alpha$ is infinitesimally small, the above error becomes the Monte Carlo error:\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{lim}_{\\alpha \\to 0} \\sum_{k=t}^{T-1} \\left[ (\\gamma^{k-t} + \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right] = \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "The above cases were considered for TD(0). For n-step TD, the state-values change only in time-steps multiple of n, so we would have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) = \\sum_{k=t}^{T-1} \\left[ (\\gamma^{k-t} + 1_{(k+1) \\equiv 0 \\pmod{n}} \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right]\n",
    "\\end{align*}\n",
    "\n",
    "In the case in which $n \\gt T$, the condition $(k+1) \\equiv 0 \\pmod{n}$ would always be false, and the error above would become the Monte Carlo error, which is expected, because Monte Carlo is equivalent to TD(T+1), as there are T+1 time-steps in t = 0, 1, 2, ..., T-1, T.\n",
    "\n",
    "Actually, the above equation can be generalized to find the error for the state-value $V(S_t)$ after having calculated the values until the time-step $t_{target} > t$, for a n-step TD:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V_{t_{target}}(S_t) = \\sum_{k=t}^{t_{target}-1} \\left[ (\\gamma^{k-t} + 1_{(k+1) \\equiv 0 \\pmod{n}} \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right]\n",
    "\\end{align*}\n",
    "\n",
    "The above gives the current error for the state-value $V(S_t)$ (with $V_{t_{target}}(S_t)$ probably closer to the true value, $v_{\\pi} (S_t)$, than $V_t(S_t)$, because it may have been updated more times after t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2\n",
    "\n",
    "**Q**\n",
    "\n",
    "This is an exercise to help develop your intuition about why TD methods are often more efficient than Monte Carlo methods. Consider the driving home example and how it is addressed by TD and Monte Carlo methods. Can you imagine a scenario in which a TD update would be better on average than a Monte Carlo update? Give an example scenario—a description of past experience and a current state—in which you would expect the TD update to be better. Here’s a hint: Suppose you have lots of experience driving home from work. Then you move to a new building and a new parking lot (but you still enter the highway at the same place). Now you are starting to learn predictions for the new building. Can you see why TD updates are likely to be much better, at least initially, in this case? Might the same sort of thing happen in the original scenario?\n",
    "\n",
    "**A**\n",
    "\n",
    "One important advantage of TD over Monte Carlo is that TD adjusts itself during the episode. If in the given example each situation (leaving office, reach car, etc...) along with the elapsed time is considered to be a state, and the transitions between them as the single possible action, with the next state deterministic and the reward (the elapsed time for the step) depending on the environment dynamics, then the Monte Carlo method can be seen as an average over all the randomness in all possible steps, while TD adjusts for the randomness in each step (for TD(0)).\n",
    "\n",
    "The graphs show the total elapsed time, but policy evaluation for Monte Carlo and TD define the expected rewards (state values or state-action values) for the states, from that state onwards (not from the initial state). For $\\gamma = 1$ and $\\alpha = 1$, The Monte Carlo would change the expected reward of the state \"exiting the highway\" from 15 to 23, while TD(0) would change from 15 to 20 ($R + \\gamma G_{next} = 10 + 1 \\cdot 10 = 20$, with $G_{next}$ the expected reward of the next state, which is the remaining expected time when reaching the state \"2ndary road\", which in this case, because there's a single action in each state, corresponds to $V_{next}$).\n",
    "\n",
    "In the proposed case, when moving to a new building and a new parking lot, that trajectory would initially give wrong times (predicted rewards), which would affect the expected reward for the entire episode in the case of Monte Carlo, but TD mainly adjust for the new subpaths, while the known subpaths should give the same expected reward. This means that Monte Carlo expected reward should have a higher variance initially because the new subpath will end up affecting the entire expected reward, while TD would be more precise adjusting mainly the subpaths expected rewards (which will affect the total expected reward, but as a consequence). After lots episodes, the new subpaths will have the average expected reward and will be the same for Monte Carlo and TD. An interesting point is that if there's a need to change the environment to use the proposed scenario, then the values from TD could be bootstrapped to be used for the existing states (the new states should be defined arbitrarily), giving a faster convergence to the expected values.\n",
    "\n",
    "This also happen in the original scenario, but the error should reduce in both cases after a large amount of episodes. Considering that it's a real scenario with a huge amount of possible rewards (it has a high entropy due to rain, traffic, and a lot of other situations that can happen), TD should converge faster to more precise rewards because it can adapt better for each step, instead of just having a global view. There should be exceptions to this, especially in scenarios in which a change in the reward of a state affect the reward of another, like choosing a different highway that takes a bit more time, but ends at the end of the secondary road, reducing the total time (this wouldn't be an MDP environment tough; to turn it in an MDP the states should be divided in more states to preserve the MDP property; in this case, removing the state \"2ndary road\" and adding the states \"2ndary road - part 1\" and \"2ndary road - part 2\" that corresponds the parts of the secondary road before and after the intersection with the new highway, respectively, should turn it back into an MDP).\n",
    "\n",
    "It's important to note that TD only adjusts the expected reward (value) for the previous state (after transitioning from $S_1$ to $S_2$ due to action $A_1$ and receiving the reward $R_2$, it will update the state value V($S_1$), or the state-action value Q($S_1$, $A_1$)). In the proposed scenario, the states do not repeat in an episode, so the changes will only benefit the next episode, even for TD(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.3\n",
    "\n",
    "**Q**\n",
    "\n",
    "From the results shown in the left graph of the random walk example it appears that the first episode results in a change in only V(A). What does this tell you about what happened on the first episode? Why was only the estimate for this one state changed? By exactly how much was it changed?\n",
    "\n",
    "**A**\n",
    "\n",
    "The first episoded ended in the terminal state that returns a reward of 0 (the left terminal state). \n",
    "\n",
    "Initially, all states have the same state value (0.5), and because all the rewards are 0, there was no change in the state-value before the terminal state (because the task is undiscounted, that is, $\\gamma = 1$):\n",
    "\n",
    "\\begin{align*}\n",
    "V(S_t) \\gets V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)] = 0.5 + \\alpha [0 + 1 \\cdot 0.5 - 0.5] = 0.5 + \\alpha \\cdot 0 = 0.5\n",
    "\\end{align*}\n",
    "\n",
    "The only exception is for the last time-step in the episode, because $V(S_T) = 0$, and the reward received going from A to the terminal state at the left ($R_{T_{left}}$) is 0:\n",
    "\n",
    "\\begin{align*}\n",
    "V(A) \\gets V(A) + \\alpha [R_{T_{left}} + \\gamma V(S_T) - V(A)] = V(A) + \\alpha \\cdot [0 + 1 \\cdot 0 - V(A)] = V(A) - \\alpha V(A) = (1 - \\alpha) V(A)\n",
    "\\end{align*}\n",
    "\n",
    "The case shown in the left graph used $\\alpha = 0.1$, so:\n",
    "\n",
    "\\begin{align*}\n",
    "V(A) \\gets (1 - \\alpha) V(A) = (1 - 0.1) \\cdot 0.5 = 0.9 \\cdot 0.5 = 0.45\n",
    "\\end{align*}\n",
    "\n",
    "The state-value of A decreased by 0.05 (from 0.5 to 0.45)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.4\n",
    "\n",
    "**Q**\n",
    "\n",
    "The specific results shown in the right graph of the random walk example are dependent on the value of the step-size parameter, $\\alpha$. Do you think the conclusions\n",
    "about which algorithm is better would be affected if a wider range of $\\alpha$ values were used? Is there a different, fixed value of $\\alpha$ at which either algorithm would have performed significantly better than shown? Why or why not?\n",
    "\n",
    "**A**\n",
    "\n",
    "If a wider range of $\\alpha$ values were used, the TD algorithm should still have better results, on average. \n",
    "\n",
    "The general rule is that the lesser the value of $\\alpha$, the smaller the error will be after a lot of episodes, with less variance between episodes (similar to how derivation finds the direction of a curve after an infinitesimal distance, with greater error the larger the distance considered is). \n",
    "\n",
    "Although very small values of $\\alpha$ end up with less variance, they take longer start giving better values (which can be seen for the cases of $\\alpha = 0.05$ for TD and $\\alpha = 0.01$ for MC). On the other hand, large values of $\\alpha$ will change faster, but with greater variance, ending up worse than the cases shown for smaller $\\alpha$ after a sufficient amount of episodes (like $\\alpha = 0.15$ for TD and $\\alpha = 0.04$ for MC).\n",
    "\n",
    "Smaller values of $\\alpha$ should give better results after a large amount of episodes, but for the 100 episodes of the proposed scenario, the algorithm would not have performed significantly better than shown (it could end up being worse because it would take too long to give good results, giving a worse result after 100 episodes than a larger $\\alpha$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.5\n",
    "\n",
    "**Q**\n",
    "\n",
    "In the right graph of the random walk example, the RMS error of the TD method seems to go down and then up again, particularly at high $\\alpha$’s. What could have caused this? Do you think this always occurs, or might it be a function of how the approximate value function was initialized?\n",
    "\n",
    "**A**\n",
    "\n",
    "As the predicted state-values approach the true state-values, the changes to the values will have more impact in the error, causing the error to increase sometimes, although it might decrease again after more episodes. Smaller values of $\\alpha$ decrease this variance, with an infinitesimally small $\\alpha$ giving a smooth graph (see the previous exercise). This situation should generally occur, epecially at high $\\alpha$’s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.6\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Example 6.2 we stated that the true values for the random walk example are $\\frac{1}{6}$, $\\frac{2}{6}$, $\\frac{3}{6}$, $\\frac{4}{6}$ and $\\frac{5}{6}$, for states A through E. Describe at least two different ways that these could have been computed. Which would you guess we actually used? Why?\n",
    "\n",
    "**A**\n",
    "\n",
    "One and more general way to solve it is to solve the system of equations for every state value ($\\gamma = 1$ because the task is undiscounted):\n",
    "\n",
    "\\begin{align*}\n",
    "V(s) = \\mathbb{E} [R_{t+1} + \\gamma V(S_{t+1}), S_t=s] = \\mathbb{E} [R_{t+1} + V(S_{t+1}), S_t=s] = 0.5 \\cdot [R_{t_{left}} + V(S_{t_{left}})] + 0.5 \\cdot [R_{t_{right}} + V(S_{t_{right}})]\n",
    "\\end{align*}\n",
    "\n",
    "Each state has 50% chance of going either left or right, so each case is multiplied (weighted) by 0.5.\n",
    "\n",
    "For all states A through E:\n",
    "\n",
    "\\begin{align*}\n",
    "V(A) &= 0.5 \\cdot [R_{T_{left}} + V(S_{T_{left}})] + 0.5 \\cdot [R_B + V(B)] = 0.5 \\cdot [0 + 0] + 0.5 \\cdot [0 + V(B)] = 0.5 \\cdot V(B) \\\\\n",
    "V(B) &= 0.5 \\cdot [R_A + V(A)] + 0.5 \\cdot [R_C + V(C)] = 0.5 \\cdot [0 + V(A)] + 0.5 \\cdot [0 + V(C)] = 0.5 \\cdot [V(A) + V(C)] \\\\\n",
    "V(C) &= 0.5 \\cdot [R_B + V(B)] + 0.5 \\cdot [R_D + V(D)] = 0.5 \\cdot [0 + V(B)] + 0.5 \\cdot [0 + V(D)] = 0.5 \\cdot [V(B) + V(D)] \\\\\n",
    "V(D) &= 0.5 \\cdot [R_C + V(C)] + 0.5 \\cdot [R_E + V(E)] = 0.5 \\cdot [0 + V(C)] + 0.5 \\cdot [0 + V(E)] = 0.5 \\cdot [V(C) + V(E)] \\\\\n",
    "V(E) &= 0.5 \\cdot [R_D + V(D)] + 0.5 \\cdot [R_{T_{right}} + V(S_{T_{right}})] = 0.5 \\cdot [0 + V(D)] + 0.5 \\cdot [1 + 0] = 0.5 \\cdot V(D) + 0.5\n",
    "\\end{align*}\n",
    "\n",
    "V(B) in terms of V(A):\n",
    "\n",
    "\\begin{align*}\n",
    "V(A) &= 0.5 \\cdot V(B) \\\\\n",
    "V(B) &= 2 \\cdot V(A)\n",
    "\\end{align*}\n",
    "\n",
    "V(C) in terms of V(A):\n",
    "\n",
    "\\begin{align*}\n",
    "V(B) &= 0.5 \\cdot [V(A) + V(C)] \\\\\n",
    "2 \\cdot V(A) &= 0.5 \\cdot V(A) + 0.5 \\cdot V(C) \\\\\n",
    "1.5 \\cdot V(A) &= 0.5 \\cdot V(C) \\\\\n",
    "V(C) &= 3 \\cdot V(A) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "V(D) in terms of V(A):\n",
    "\n",
    "\\begin{align*}\n",
    "V(C) &= 0.5 \\cdot [V(B) + V(D)] \\\\\n",
    "3 \\cdot V(A) V(B) &= 0.5 (2 \\cdot V(A)) + 0.5 \\cdot V(D) \\\\\n",
    "3 \\cdot V(A) V(B) &= V(A) + 0.5 \\cdot V(D) \\\\\n",
    "2 \\cdot V(A) V(B) &= 0.5 \\cdot V(D) \\\\\n",
    "V(D) &= 4 \\cdot V(A)\n",
    "\\end{align*}\n",
    "\n",
    "V(E) in terms of V(A):\n",
    "\n",
    "\\begin{align*}\n",
    "V(D) &= 0.5 \\cdot [V(C) + V(E)] \\\\\n",
    "4 \\cdot V(A) &= 0.5 (3 \\cdot V(A)) + 0.5 \\cdot V(E) \\\\\n",
    "8 \\cdot V(A) &= 3 \\cdot V(A) + V(E) \\\\\n",
    "V(E) &= 5 \\cdot V(A)\n",
    "\\end{align*}\n",
    "\n",
    "Solving for V(A):\n",
    "\n",
    "\\begin{align*}\n",
    "V(E) &= 0.5 \\cdot V(D) + 0.5 \\\\\n",
    "2 \\cdot V(E) &= V(D) + 1 \\\\\n",
    "10 \\cdot V(A) &= 4 \\cdot V(A) + 1 \\\\\n",
    "6 \\cdot V(A) &= 1 \\\\\n",
    "V(A) &= \\frac{1}{6}\n",
    "\\end{align*}\n",
    "\n",
    "Solving for V(B), V(C), V(D) and V(E):\n",
    "\n",
    "\\begin{align*}\n",
    "V(B) &= 2 \\cdot V(A) = 2 \\cdot \\frac{1}{6} = \\frac{2}{6} \\\\\n",
    "V(C) &= 3 \\cdot V(A) = 3 \\cdot \\frac{1}{6} = \\frac{3}{6} \\\\\n",
    "V(D) &= 4 \\cdot V(A) = 4 \\cdot \\frac{1}{6} = \\frac{4}{6} \\\\\n",
    "V(E) &= 5 \\cdot V(A) = 5 \\cdot \\frac{1}{6} = \\frac{5}{6}\n",
    "\\end{align*}\n",
    "\n",
    "Due to the symmetry of the environment, the reward of going to the leftmost terminal state being 0 and the rightmost 1, with all other rewards 0, and being undiscounted, with an odd number of states with equally random chances of going left or right, the state at the center, C, could already be considered having a return of 0.5, so $V(C) = 0.5$, and then the values of the other states could be found faster.\n",
    "\n",
    "Actually, due to the symmetry, a convenient way to define the returns (and state-values), is to attribute weights from the leftmost state (terminal) to the rightmost (terminal), from 0 to 6 ($weight_{left} = 0$, $weight_A = 1$, ..., $weight_E = 5$, $weight_{right} = 6$, that is, 7 weights for 7 states, with 5 inner weights corresponding to the non-terminal states), and divide by 6 to define the value of the state, $V(left) = \\frac{0}{6} = 0$, $V(A) = \\frac{1}{6}$, ..., $V(E) = \\frac{5}{6}$, $V(right) = \\frac{6}{6} = 1$, and then ignore the first and last states (terminal states have value 0; they were given other values just for the sake of continuity, with weights to define a continuous sequence from 0 to 1). This is probably the way that the values were computed due to its simplicity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
