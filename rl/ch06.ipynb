{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 06 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "If V changes during the episode, then (6.6) only holds approximately; what would the difference be between the two sides? Let $V_t$ denote the array of state values used at time t in the TD error (6.5) and in the TD update (6.2). Redo the derivation above to determine the additional amount that must be added to the sum of TD errors in order to equal the Monte Carlo error.\n",
    "\n",
    "**A**\n",
    "\n",
    "The equation 6.2, that defines how the values are updated using the TD method, is:\n",
    "\n",
    "\\begin{align*}\n",
    "V(S_t) \\gets V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)] \\tag{6.2}\n",
    "\\end{align*}\n",
    "\n",
    "The TD error (equation 6.5) is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_t \\stackrel{.}{=} R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\tag{6.5}\n",
    "\\end{align*}\n",
    "\n",
    "The equation 6.6 is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1}) \\tag{from (3.9)} \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V(S_{t+2})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V(S_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\tag{6.6}\n",
    "\\end{align*}\n",
    "\n",
    "If V changes during the episode, then at the start of time t+1 the new V would be (for TD(0)):\n",
    "\n",
    "\\begin{align*}\n",
    "V_{t+1}(S_t) \\gets V_t(S_t) + \\alpha [R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)]\n",
    "\\end{align*}\n",
    "\n",
    "where $V_t$ hold the state values at time t.\n",
    "\n",
    "The TD error at time t is now:\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_t \\stackrel{.}{=} R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)\n",
    "\\end{align*}\n",
    "\n",
    "The equation 6.6 considering the initial $V_t$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V_t(S_t) &= R_{t+1} + \\gamma G_{t+1} - V_t(S_t) + \\gamma V_t(S_{t+1}) - \\gamma V_t(S_{t+1})\\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V_t(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V_t(S_{t+1}) + V_{t+1}(S_{t+1}) - V_{t+1}(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V_{t+1}(S_{t+1})) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V_{t+2}(S_{t+2})) + \\gamma^2 (V_{t+2}(S_{t+2}) - V_{t+1}(S_{t+2})) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V_T(S_T)) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) + \\gamma^2 (V_{t+2}(S_{t+2}) - V_{t+1}(S_{t+2})) + ... + \\gamma^{T-t} (V_T(S_T) - V_{T-1}(S_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) + \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) + \\gamma^2 (V_{t+2}(S_{t+2}) - V_{t+1}(S_{t+2})) + ... + \\gamma^{T-t} (V_T(S_T) - V_{T-1}(S_T)) \\\\\n",
    "&= \\left[ \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\right] + \\left[ \\sum_{k=t+1}^{T} \\gamma^{k-t} (V_k(S_k) - V_{k-1}(S_k)) \\right] \\\\\n",
    "&= \\left[ \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\right] + \\left[ \\sum_{k=t}^{T-1} \\gamma^{k-t+1} (V_{k+1}(S_{k+1}) - V_k(S_{k+1})) \\right] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} (V_{k+1}(S_{k+1}) - V_k(S_{k+1})) ]\n",
    "\\end{align*}\n",
    "\n",
    "We have $V_{k+1}(S_{k+1}) = V_k(S_{k+1})$, unless $S_{k+1} = S_k$, in which case there would be a change in the state-value for the state $S_{k+1} = S_k$ in the transition from the time-step k to k+1, and we would have $V_{k+1}(S_{k+1}) = V_{k+1}(S_k)$, because $S_k = S_{k+1}$. The equation $V_{k+1}(S_{k+1}) = V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_{k+1}(S_k) - V_k(S_{k+1}))$ corresponds to these 2 cases. So:\n",
    "\n",
    "\\begin{align*}\n",
    "V_{k+1}(S_{k+1}) &= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_{k+1}(S_k) - V_k(S_{k+1})) \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_k(S_k) + \\alpha [R_{k+1} + \\gamma V_k(S_{k+1}) - V_k(S_k)] - V_k(S_{k+1})) \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} (V_k(S_k) + \\alpha [R_{k+1} + \\gamma V_k(S_{k+1}) - V_k(S_k)] - V_k(S_k)) \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} \\alpha [R_{k+1} + \\gamma V_k(S_{k+1}) - V_k(S_k)] \\\\\n",
    "&= V_k(S_{k+1}) + 1_{S_{k+1} = S_k} \\alpha \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V_t(S_t) &= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} (V_{k+1}(S_{k+1}) - V_k(S_{k+1})) ] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} (V_k(S_{k+1}) + 1_{S_{k+1} = S_k} \\alpha \\delta_k - V_k(S_{k+1})) ] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + 1_{S_{k+1} = S_k} \\gamma^{k-t+1} \\alpha \\delta_k ] \\\\\n",
    "&= \\sum_{k=t}^{T-1} [ (\\gamma^{k-t} + 1_{S_{k+1} = S_k} \\gamma^{k-t+1} \\alpha) \\delta_k ]\n",
    "\\end{align*}\n",
    "\n",
    "The above equation is the answer of this question. We can also consider the error using the values after the episode ended (after the terminal state $S_T$ is reached).\n",
    "\n",
    "The final value at the end of the episode, $V_T$, is:\n",
    "\n",
    "\\begin{align*}\n",
    "V(S_t) = V_T(S_t) &= V_{T-1}(S_t) + 1_{S_{T-1} = S_t} \\alpha [R_T + \\gamma V_{T-1}(S_T) - V_{T-1}(S_{T-1})] \\\\\n",
    "&= V_{T-1}(S_t) + 1_{S_{T-1} = S_t} \\alpha \\delta_{T-1} \\\\\n",
    "&= [V_{T-2}(S_t) + 1_{S_{T-2} = S_t} \\alpha \\delta_{T-2}] + 1_{S_{T-1} = S_t} \\alpha \\delta_{T-1} \\\\\n",
    "&= V_t(S_t) + \\alpha \\sum_{k=t}^{T-1} 1_{S_k = S_t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "in which $1_{S_k = S_t}$ means that the value will be 1 if $S_k = S_t$ and 0 otherwise, because $V(S_t)$ changes after the time-step k only if the state at that time-step, $S_k$, is the same state at the time-step t, $S_t$. \n",
    "\n",
    "It's important to keep in mind that the same state may be visited several times depending on the choosen actions and the environment dynamics. For the special case in which we have the state $S_t$ only visited at time t, then $V(S_t) = V_t(S_t) + \\alpha \\delta_t$ (there was only one change: it happened in the transition from t to t+1). In the other extreme, if the state $S_t$ was the only state for all time-steps before the terminal state, then $V(S_t) = V_t(S_t) + \\alpha \\sum_{k=t}^{T-1} \\delta_k$, which is the initial value at time t plus all the changes after each time-step.\n",
    "\n",
    "The new derivation (6.6) is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) &= G_t - V_t(S_t) - \\alpha \\sum_{k=t}^{T-1} 1_{S_k = S_t} \\delta_k \\\\\n",
    "&= \\left[ \\sum_{k=t}^{T-1} [ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} 1_{S_{k+1} = S_k} \\alpha \\delta_k ] \\right] - \\alpha \\left[ \\sum_{k=t}^{T-1} 1_{S_k = S_t} \\delta_k \\right] \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\left[ \\gamma^{k-t} \\delta_k + \\gamma^{k-t+1} 1_{S_{k+1} = S_k} \\alpha \\delta_k - 1_{S_k = S_t} \\alpha \\delta_k \\right] \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\left[ (\\gamma^{k-t} + \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right]\n",
    "\\end{align*}\n",
    "\n",
    "When $\\alpha$ is infinitesimally small, the above error becomes the Monte Carlo error:\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{lim}_{\\alpha \\to 0} \\sum_{k=t}^{T-1} \\left[ (\\gamma^{k-t} + \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right] = \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "The above cases were considered for TD(0). For n-step TD, the state-values change only in time-steps multiple of n, so we would have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) = \\sum_{k=t}^{T-1} \\left[ (\\gamma^{k-t} + 1_{(k+1) \\equiv 0 \\pmod{n}} \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right]\n",
    "\\end{align*}\n",
    "\n",
    "In the case in which $n \\gt T$, the condition $(k+1) \\equiv 0 \\pmod{n}$ would always be false, and the error above would become the Monte Carlo error, which is expected, because Monte Carlo is equivalent to TD(T+1), as there are T+1 time-steps in t = 0, 1, 2, ..., T-1, T.\n",
    "\n",
    "Actually, the above equation can be generalized to find the error for the state-value $V(S_t)$ after having calculated the values until the time-step $t_{target} > t$, for a n-step TD:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V_{t_{target}}(S_t) = \\sum_{k=t}^{t_{target}-1} \\left[ (\\gamma^{k-t} + 1_{(k+1) \\equiv 0 \\pmod{n}} \\alpha (1_{S_{k+1} = S_k} \\gamma^{k-t+1} - 1_{S_k = S_t})) \\delta_k \\right]\n",
    "\\end{align*}\n",
    "\n",
    "The above gives the current error for the state-value $V(S_t)$ (with $V_{t_{target}}(S_t)$ probably closer to the true value, $v_{\\pi} (S_t)$, than $V_t(S_t)$, because it may have been updated more times after t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2\n",
    "\n",
    "**Q**\n",
    "\n",
    "This is an exercise to help develop your intuition about why TD methods are often more efficient than Monte Carlo methods. Consider the driving home example and how it is addressed by TD and Monte Carlo methods. Can you imagine a scenario in which a TD update would be better on average than a Monte Carlo update? Give an example scenario—a description of past experience and a current state—in which you would expect the TD update to be better. Here’s a hint: Suppose you have lots of experience driving home from work. Then you move to a new building and a new parking lot (but you still enter the highway at the same place). Now you are starting to learn predictions for the new building. Can you see why TD updates are likely to be much better, at least initially, in this case? Might the same sort of thing happen in the original scenario?\n",
    "\n",
    "**A**\n",
    "\n",
    "One important advantage of TD over Monte Carlo is that TD adjusts itself during the episode. If in the given example each situation (leaving office, reach car, etc...) along with the elapsed time is considered to be a state, and the transitions between them as the single possible action, with the next state deterministic and the reward (the elapsed time for the step) depending on the environment dynamics, then the Monte Carlo method can be seen as an average over all the randomness in all possible steps, while TD adjusts for the randomness in each step (for TD(0)).\n",
    "\n",
    "The graphs show the total elapsed time, but policy evaluation for Monte Carlo and TD define the expected rewards (state values or state-action values) for the states, from that state onwards (not from the initial state). For $\\gamma = 1$ and $\\alpha = 1$, The Monte Carlo would change the expected reward of the state \"exiting the highway\" from 15 to 23, while TD(0) would change from 15 to 20 ($R + \\gamma G_{next} = 10 + 1 \\cdot 10 = 20$, with $G_{next}$ the expected reward of the next state, which is the remaining expected time when reaching the state \"2ndary road\", which in this case, because there's a single action in each state, corresponds to $V_{next}$).\n",
    "\n",
    "In the proposed case, when moving to a new building and a new parking lot, that trajectory would initially give wrong times (predicted rewards), which would affect the expected reward for the entire episode in the case of Monte Carlo, but TD mainly adjust for the new subpaths, while the known subpaths should give the same expected reward. This means that Monte Carlo expected reward should have a higher variance initially because the new subpath will end up affecting the entire expected reward, while TD would be more precise adjusting mainly the subpaths expected rewards (which will affect the total expected reward, but as a consequence). After lots episodes, the new subpaths will have the average expected reward and will be the same for Monte Carlo and TD.\n",
    "\n",
    "This also happen in the original scenario, but the error should reduce in both cases after a large amount of episodes. Considering that it's a real scenario with a huge amount of possible rewards (it has a high entropy due to rain, traffic, and a lot of other situations that can happen), TD should converge faster to more precise rewards because it can adapt better for each step, instead of just having a global view. There should be exceptions to this, especially in scenarios in which a change in the reward of a state affect the reward of another, like choosing a different highway that takes a bit more time, but ends at the end of the secondary road, reducing the total time (this wouldn't be an MDP environment tough; to turn it in an MDP the states should be divided in more states to preserve the MDP property; in this case, removing the state \"2ndary road\" and adding the states \"2ndary road - part 1\" and \"2ndary road - part 2\" that corresponds the parts of the secondary road before and after the intersection with the new highway, respectively, should turn it back into an MDP).\n",
    "\n",
    "It's important to note that TD only adjusts the expected reward (value) for the previous state (after transitioning from $S_1$ to $S_2$ due to action $A_1$ and receiving the reward $R_2$, it will update the state value V($S_1$), or the state-action value Q($S_1$, $A_1$)). In the proposed scenario, the states do not repeat in an episode, so the changes will only benefit the next episode, even for TD(0)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
