{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 08 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "The nonplanning method looks particularly poor in Figure 8.3 because it is a one-step method; a method using multi-step bootstrapping would do better. Do you think one of the multi-step bootstrapping methods from Chapter 7 could do as well as the Dyna method? Explain why or why not.\n",
    "\n",
    "**A**\n",
    "\n",
    "A similar corresponding case is using a nonplanning method with multi-step bootstrapping with $n = 51$, because the planning simulates 50 cases after the direct RL update, and would update the last 51 state-action values after each step (in the first episode, only the last 51 states visited before the end of the episode will be updated, because only the final reward is non-zero). \n",
    "\n",
    "In the second episode, while the planning method starts with only the last non terminal state updated, after each step it can update a previously visited states considering the actions taken, and while in the beginning it has little chance of choosing a useful state-action pair that takes to a next state that was already updated, as the steps go on it updates more and more cases, going fast toward states closest to the beginning.\n",
    "\n",
    "In the same second episode, for the non-planing 51-step TD, updating the values using the Q-learning method (that uses the best state-action value after in the last step, or 0 for the terminal state), all the initial steps wouldn't be able to update any state-action values. Only after the agent reaches an already updated state, it would update the states of the last steps, so it might be faster than the first case, TD(0), but it would probably take longer than the planning method because it can only update the last 51 steps, while the planning method may even update a state that was not reached yet in the episode. \n",
    "\n",
    "Furthermore, because the value of n for the n-step method is high, the possibility of a high variance is higher (although, because this environment has all rewards 0 except the last, the variance may ending up not being so high). One thing that might cause a high variance is a state s1 close to the end, but the agent took a bad direction and went to a lot of states farther from the final state, and then went to the final state without passing from the indicated state s1, making it have a worse value than it should have (this is more difficult to happen in the planning case, that uses TD(0), because it only consider neighbor states, reducing considerably the chances like the above scenario to happen, even if it's actually possible to happen too).\n",
    "\n",
    "So, using a 51-step TD with Q-learning (using the maximum state-action value in the last step) may converge faster than TD(0) (or not, due to the variance), and can update a number of states just like the planning method with 50 simulations per step, but because the updates need to follow the last 50 steps, and can not be benefited from updates from previous episodes if it has not reached a state already updated in a previous episode, it probably will need more episodes than the planning method (using less steps for the nonplanning method may actually help to converge faster, although it probably won't have the same performance as the planning method). This answer is just a speculation tough. A real experiment may give a more concrete explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2 \n",
    "\n",
    "**Q**\n",
    "\n",
    "Why did the Dyna agent with exploration bonus, Dyna-Q+, perform better in the first phase as well as in the second phase of the blocking and shortcut experiments?\n",
    "\n",
    "**A**\n",
    "\n",
    "It was better in the second phase due to the reasons already stated  in the example: it explores more than the Dyna agent, so when the environment changed, it found the change faster (in the shortcut experiment, Dyna haven't found the shortcut even in the next 3000 time-steps, while Dyna-Q+ found it after about 1000 time-steps).\n",
    "\n",
    "For the first phase, the reason is less obvious, but it has to do with the fact that Dyna-Q+ gives priority to action-states not tried before. Considering that both used $\\epsilon$-greedy policies, which means that there's already a possibility of exploration at each time-step, Dyna-Q+ gives priority to unknown states, making it explore and find the terminal state faster (Dyna may end up repeating the same states several times before finding the path to the terminal state, at least in the first episode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.3 \n",
    "\n",
    "**Q**\n",
    "\n",
    "Careful inspection of Figure 8.5 reveals that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason for this?\n",
    "\n",
    "**A**\n",
    "\n",
    "The Dyna-Q+ method explores more and consequently is able to find the best path faster, but that has a cost, and that cost is that it continues exploring even if the best path was already found. Both methods explore due to the soft policy used, but in the Dyna case that exploration should be minimal. This means that after the optimal path is found, if no changes happen to the environment, the Dyna method may end up being a bit better than Dyna-Q+ over time (more reward per time-step, on average)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.4 (programming) \n",
    "\n",
    "**Q**\n",
    "\n",
    "The exploration bonus described above actually changes the estimated values of states and actions. Is this necessary? Suppose the bonus $\\kappa \\sqrt{\\tau}$ was used not in updates, but solely in action selection. That is, suppose the action selected was always that for which $Q(S_t, a) + \\kappa \\sqrt{\\tau(S_t, a)}$ was maximal. Carry out a gridworld experiment that tests and illustrates the strengths and weaknesses of this alternate approach.\n",
    "\n",
    "**A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnv:\n",
    "    def __init__(self, n_states: int, n_actions: int):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self, seed: int = 0) -> int:\n",
    "        self.steps = 0\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, env: BaseEnv):\n",
    "        self.env = env\n",
    "\n",
    "    def act(self, state: int) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, steps: int):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "from numpy.random import MT19937, Generator\n",
    "\n",
    "def random_generator(seed: int | None = None):\n",
    "    bg = MT19937(seed)\n",
    "    rg = Generator(bg)\n",
    "    return rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "NORMAL_COLOR = np.array([1., 1., 1.])\n",
    "START_COLOR = np.array([255, 99, 71]) / 255\n",
    "FINISH_COLOR = np.array([152, 249, 152]) / 255\n",
    "BARRIER_COLOR = np.array([0.5, 0.5, 0.5])\n",
    "AGENT_COLOR = np.array([39, 116, 218]) / 255\n",
    "\n",
    "TYPE_NORMAL = 0\n",
    "TYPE_START = 1\n",
    "TYPE_FINISH = 2\n",
    "TYPE_BARRIER = 3\n",
    "\n",
    "class GridWorldEnv(BaseEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: tuple[int, int],\n",
    "        start: tuple[int, int],\n",
    "        finish: tuple[int, int],\n",
    "        barriers_over_time: list[tuple[int, list[tuple[int, int]]]],\n",
    "        max_steps: int | None = None,\n",
    "    ):\n",
    "        rows, cols = size\n",
    "        start_x, start_y = start\n",
    "        s_start = start_y * cols + start_x\n",
    "        finish_x, finish_y = finish\n",
    "        s_finish = finish_y * cols + finish_x\n",
    "\n",
    "        assert rows > 2 and cols > 2, \"Invalid grid size\"\n",
    "        assert (start_y >= 0 and start_y < rows), f\"Invalid start row: {start_y} (row should be in [0, {rows}))\"\n",
    "        assert (start_x >= 0 and start_x < cols), f\"Invalid start column: {start_x} (column should be in [0, {cols}))\"\n",
    "        assert (finish_y >= 0 and finish_y < rows), f\"Invalid finish row: {finish_y} (row should be in [0, {rows}))\"\n",
    "        assert (finish_x >= 0 and finish_x < cols), f\"Invalid finish column: {finish_x} (column should be in [0, {cols}))\"\n",
    "\n",
    "        for time_step, barriers in barriers_over_time:\n",
    "            for barrier in barriers:\n",
    "                x, y = barrier\n",
    "                assert (y >= 0 and y < rows), f\"Invalid barrier row at time-step {time_step}: {y} (row should be in [0, {rows}))\"\n",
    "                assert (x >= 0 and x < cols), f\"Invalid barrier column at time-step {time_step}: {x} (column should be in [0, {cols}))\"\n",
    "                assert barrier != start, f\"Invalid barrier at time-step {time_step}: should not be in the start cell\"\n",
    "                assert barrier != finish, f\"Invalid barrier at time-step {time_step}: should not be in the finish cell\"\n",
    "\n",
    "        grid = np.full((rows, cols), TYPE_NORMAL)\n",
    "        grid[start_y, start_x] = TYPE_START\n",
    "        grid[finish_y, finish_x] = TYPE_FINISH\n",
    "\n",
    "        actions = [\n",
    "            (0, 1),\n",
    "            (1, 0),\n",
    "            (0, -1),\n",
    "            (-1, 0),\n",
    "        ]\n",
    "\n",
    "        n_states = rows * cols\n",
    "        n_actions = len(actions)\n",
    "\n",
    "        super().__init__(\n",
    "            n_states=n_states,\n",
    "            n_actions=n_actions)\n",
    "\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start = start\n",
    "        self.s_start = s_start\n",
    "        self.finish = finish\n",
    "        self.s_finish = s_finish\n",
    "        self.actions = actions\n",
    "        self.grid = grid\n",
    "        self.max_steps = max_steps\n",
    "        self.barriers_over_time = barriers_over_time\n",
    "        self.barrier_time_step = -1\n",
    "        self.barriers: list[tuple[int, int]] = []\n",
    "\n",
    "        self.steps = 0\n",
    "        self.all_steps = 0\n",
    "        self.state: int | None = None\n",
    "\n",
    "        self.update_barriers()\n",
    "\n",
    "    def update_barriers(self):\n",
    "        rows = self.rows\n",
    "        cols = self.cols\n",
    "        all_steps = self.all_steps\n",
    "        barriers_over_time = self.barriers_over_time\n",
    "        grid = self.grid\n",
    "\n",
    "        time_step = 0\n",
    "        barriers: list[tuple[int, int]] = []\n",
    "\n",
    "        for t, b in barriers_over_time:\n",
    "            if t > all_steps:\n",
    "                break\n",
    "\n",
    "            time_step = t\n",
    "            barriers = b\n",
    "\n",
    "        if time_step > self.barrier_time_step:\n",
    "            self.barrier_time_step = time_step\n",
    "            self.barriers = barriers\n",
    "\n",
    "            for row in range(rows):\n",
    "                for col in range(cols):\n",
    "                    if grid[row, col] == TYPE_BARRIER:\n",
    "                        grid[row, col] = TYPE_NORMAL\n",
    "\n",
    "            for x, y in barriers:\n",
    "                grid[y, x] = TYPE_BARRIER\n",
    "\n",
    "    def reset(self, seed: int = 0) -> int:\n",
    "        state = self.s_start\n",
    "        self.steps = 0\n",
    "        self.state = state\n",
    "        return state\n",
    "\n",
    "    def reset_all_steps(self) -> int:\n",
    "        state = self.reset()\n",
    "        self.all_steps = 0\n",
    "        self.barrier_time_step = -1\n",
    "        self.update_barriers()\n",
    "        return state\n",
    "\n",
    "    def invalid_position(self, row: int, col: int) -> bool:\n",
    "        return row < 0 or row >= self.rows or col < 0 or col >= self.cols or self.grid[row, col] == TYPE_BARRIER\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        steps = self.steps + 1\n",
    "        state = self.state\n",
    "\n",
    "        assert state is not None, \"The environment was not initialized\"\n",
    "        assert state != self.s_finish, \"The environment is in a terminal state\"\n",
    "\n",
    "        row = state // self.cols\n",
    "        col = state % self.cols\n",
    "\n",
    "        action_move = self.actions[action]\n",
    "        move_h, move_v = action_move\n",
    "\n",
    "        new_row = row + move_v\n",
    "        new_col = col + move_h\n",
    "        new_state = state\n",
    "        reward = 0\n",
    "        terminated = False\n",
    "\n",
    "        if not self.invalid_position(row=new_row, col=new_col):\n",
    "            new_state = new_row * self.cols + new_col\n",
    "            terminated = (new_state == self.s_finish)\n",
    "\n",
    "            if terminated:\n",
    "                reward = 1\n",
    "\n",
    "        truncated = (not terminated) and (self.max_steps is not None) and (self.max_steps <= steps)\n",
    "\n",
    "        self.steps = steps\n",
    "        self.state = new_state\n",
    "        self.all_steps += 1\n",
    "\n",
    "        self.update_barriers()\n",
    "\n",
    "        return new_state, reward, terminated, truncated\n",
    "\n",
    "    def plot(self, title: str | None = None, history: list[int] = []):\n",
    "        rows, cols = self.rows, self.cols\n",
    "\n",
    "        color_grid = np.full((rows, cols, 3), NORMAL_COLOR)\n",
    "\n",
    "        for row in range(rows):\n",
    "            inv_row = rows - row - 1\n",
    "            for col in range(cols):\n",
    "                if self.grid[row, col] == TYPE_START:\n",
    "                    color_grid[inv_row, col] = START_COLOR\n",
    "                elif self.grid[row, col] == TYPE_FINISH:\n",
    "                    color_grid[inv_row, col] = FINISH_COLOR\n",
    "                elif self.grid[row, col] == TYPE_BARRIER:\n",
    "                    color_grid[inv_row, col] = BARRIER_COLOR\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(color_grid, aspect='auto')\n",
    "        plt.grid(which='both', color='#333', linestyle='-', linewidth=1)\n",
    "        plt.xticks(np.arange(.5, self.cols, 1), [])\n",
    "        plt.yticks(np.arange(.5, self.rows, 1), [])\n",
    "        plt.tick_params(bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)\n",
    "\n",
    "        for i, state in enumerate(history):\n",
    "            row = state // self.cols\n",
    "            col = state % self.cols\n",
    "            row = rows - row - 1\n",
    "            # color starts black and ends as AGENT_COLOR\n",
    "            shade = i/len(history)\n",
    "            color = np.array(AGENT_COLOR) * shade\n",
    "            plt.plot(col, row, 'o', color=color, markersize=20)\n",
    "\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def blocking_maze(cls, max_steps: int | None = None):\n",
    "        rows, cols = 6, 9\n",
    "        start = (8, 5)\n",
    "        finish = (3, 0)\n",
    "        barriers_over_time = [\n",
    "            (0, [(x, 2) for x in range(8)]),\n",
    "            (1000, [(x, 2) for x in range(1, 9)]),\n",
    "        ]\n",
    "        return cls(\n",
    "            size=(rows, cols),\n",
    "            start=start,\n",
    "            finish=finish,\n",
    "            barriers_over_time=barriers_over_time,\n",
    "            max_steps=max_steps)\n",
    "\n",
    "    @classmethod\n",
    "    def shortcut_maze(cls, max_steps: int | None = None):\n",
    "        rows, cols = 6, 9\n",
    "        start = (8, 5)\n",
    "        finish = (3, 0)\n",
    "        barriers_over_time = [\n",
    "            (0, [(x, 2) for x in range(1, 9)]),\n",
    "            (3000, [(x, 2) for x in range(1, 8)]),\n",
    "        ]\n",
    "        return cls(\n",
    "            size=(rows, cols),\n",
    "            start=start,\n",
    "            finish=finish,\n",
    "            barriers_over_time=barriers_over_time,\n",
    "            max_steps=max_steps)\n",
    "\n",
    "class GridWorldAgentParams:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state: int,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: int,\n",
    "        terminated: bool,\n",
    "        truncated: bool,\n",
    "    ):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.terminated = terminated\n",
    "        self.truncated = truncated\n",
    "\n",
    "\n",
    "class GridWorldAgent(BaseAgent):\n",
    "    def __init__(self, env: GridWorldEnv, name: str):\n",
    "        super().__init__(env)\n",
    "        self.grid_env = env\n",
    "        self.name = name\n",
    "\n",
    "    def train(self, steps: int):\n",
    "        self.train_with_rewards(steps=steps, show=False)\n",
    "\n",
    "    def update(self, params: GridWorldAgentParams) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_with_rewards(self, steps: int, plot=True, no_history=False) -> list[float]:\n",
    "        state = self.grid_env.reset_all_steps()\n",
    "        all_steps = 0\n",
    "        rewards = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        history = [state]\n",
    "        last_full_history: tuple[list[int], bool, bool] | None = None\n",
    "        reward_history: list[float] = [0]\n",
    "\n",
    "        while all_steps < steps:\n",
    "            all_steps += 1\n",
    "            action = self.act(state)\n",
    "            next_state, r, c, t = self.env.step(action)\n",
    "            rewards += r\n",
    "            terminated = c\n",
    "            truncated = t\n",
    "\n",
    "            self.update(GridWorldAgentParams(\n",
    "                state=state,\n",
    "                action=action,\n",
    "                reward=r,\n",
    "                next_state=next_state,\n",
    "                terminated=terminated,\n",
    "                truncated=truncated,\n",
    "            ))\n",
    "\n",
    "            if not no_history:\n",
    "                history.append(next_state)\n",
    "            reward_history.append(rewards)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                state = self.env.reset()\n",
    "                last_full_history = (history, terminated, truncated)\n",
    "                history = [state]\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        if not last_full_history:\n",
    "            last_full_history = [history, terminated, truncated]\n",
    "\n",
    "        history, _, _ = last_full_history\n",
    "\n",
    "        if plot:\n",
    "            self.grid_env.plot(title=self.name, history=[] if no_history else history)\n",
    "\n",
    "        return reward_history\n",
    "\n",
    "class TestAgent(GridWorldAgent):\n",
    "    def __init__(self, env: GridWorldEnv, name: str):\n",
    "        super().__init__(env=env, name=name)\n",
    "\n",
    "    def act(self, state: int) -> int:\n",
    "        return 0 # Always North\n",
    "\n",
    "    def update(self, params: GridWorldAgentParams) -> None:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv.blocking_maze(max_steps=300)\n",
    "agent = TestAgent(env, name='Blocking Maze (999 steps)')\n",
    "agent.train_with_rewards(steps=999, no_history=True)\n",
    "state = env.reset()\n",
    "action = agent.act(state)\n",
    "env.step(action)\n",
    "env.plot(title='Blocking Maze (1000 steps)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv.shortcut_maze(max_steps=300)\n",
    "agent = TestAgent(env, name='Shortcut Maze (2999 steps)')\n",
    "agent.train_with_rewards(steps=2999, no_history=True)\n",
    "state = env.reset()\n",
    "action = agent.act(state)\n",
    "env.step(action)\n",
    "env.plot(title='Shortcut Maze (3000 steps)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaAgent(GridWorldAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: GridWorldEnv,\n",
    "        name: str,\n",
    "        n_plan: int = 0,\n",
    "        alpha: float = 0.2,\n",
    "        gamma: float = 0.9,\n",
    "        epsilon: float = 0.1,\n",
    "        kappa: float = 0,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        super().__init__(env=env, name=name)\n",
    "        self.n_plan = n_plan\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.kappa = kappa\n",
    "        self.Q = np.ones((env.n_states, env.n_actions), dtype=float)\n",
    "        self.Q[env.s_finish, :] = 0\n",
    "        self.M: dict[tuple[int, int], tuple[int, int]] = dict()\n",
    "        self.actions = 0\n",
    "        self.T: dict[tuple[int, int], int] = dict()\n",
    "        self.rg = random_generator(seed)\n",
    "\n",
    "    def train_with_rewards(self, steps: int, plot=True, no_history=False) -> list[float]:\n",
    "        self.actions = 0\n",
    "        self.tau = dict()\n",
    "        result = super().train_with_rewards(steps=steps, plot=plot, no_history=no_history)\n",
    "        return result\n",
    "\n",
    "    def act(self, state: int) -> int:\n",
    "        n_actions = self.env.n_actions\n",
    "        epsilon = self.epsilon\n",
    "        kappa = self.kappa\n",
    "        qs = self.Q[state]\n",
    "\n",
    "        if kappa > 0:\n",
    "            action = next((a for a in range(n_actions) if (state, a) not in self.T), None)\n",
    "\n",
    "            if action is None:\n",
    "                taus = [self.actions - self.T[(state, a)] for a in range(n_actions)]\n",
    "                qs = [q + kappa * (tau**(1/2)) for q, tau in zip(qs, taus)]\n",
    "                action: int = np.argmax(qs)\n",
    "\n",
    "            self.T[(state, action)] = self.actions\n",
    "            self.actions += 1\n",
    "            return action\n",
    "        else:\n",
    "            probs = np.ones(n_actions, dtype=float) * epsilon / n_actions\n",
    "            probs[np.argmax(qs)] += 1 - epsilon\n",
    "            action = self.rg.choice(len(probs), p=probs)\n",
    "            return action\n",
    "\n",
    "    def sample(self) -> tuple[int, int]:\n",
    "        M = self.M\n",
    "        keys = list(M.keys())\n",
    "        idx = self.rg.choice(len(keys))\n",
    "        state, action = keys[idx]\n",
    "        return state, action\n",
    "\n",
    "    def update(self, params: GridWorldAgentParams) -> None:\n",
    "        state = params.state\n",
    "        action = params.action\n",
    "        reward = params.reward\n",
    "        next_state = params.next_state\n",
    "\n",
    "        Q = self.Q\n",
    "        n_plan = self.n_plan\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "\n",
    "        Q[state, action] += alpha * (reward + gamma * max(Q[next_state]) - Q[state, action])\n",
    "        self.M[(state, action)] = (reward, next_state)\n",
    "\n",
    "        for _ in range(n_plan):\n",
    "            s, a = self.sample()\n",
    "            r, sp = self.M[(s, a)]\n",
    "            Q[s, a] += alpha * (r + gamma * max(Q[sp]) - Q[s, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gridworld(title: str, agents: list[DynaAgent], steps: int):\n",
    "    rewards: list[tuple[str, list[float]]] = []\n",
    "\n",
    "    for agent in agents:\n",
    "        r = agent.train_with_rewards(steps=steps, plot=False, no_history=True)\n",
    "        rewards.append((agent.name, r))\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    for name, r in rewards:\n",
    "        plt.plot(r, label=name)\n",
    "\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Cumulative reward')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gridworld(name: str, env: GridWorldEnv, steps: int, seed: int | None = 1):\n",
    "    agents: list[DynaAgent] = [\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name='No Exploration Bonus - n=10 - epsilon=10%',\n",
    "            n_plan=10,\n",
    "            kappa=0,\n",
    "            epsilon=0.1,\n",
    "            seed=seed,\n",
    "        ),\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name='No Exploration Bonus - n=50 - epsilon=10%',\n",
    "            n_plan=50,\n",
    "            kappa=0,\n",
    "            epsilon=0.1,\n",
    "            seed=seed,\n",
    "        ),\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name='No Exploration Bonus - n=10 - epsilon=1%',\n",
    "            n_plan=10,\n",
    "            kappa=0,\n",
    "            epsilon=0.01,\n",
    "            seed=seed,\n",
    "        ),\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name='No Exploration Bonus - n=50 - epsilon=1%',\n",
    "            n_plan=50,\n",
    "            kappa=0,\n",
    "            epsilon=0.01,\n",
    "            seed=seed,\n",
    "        ),\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name='Exploration 10% - n=10',\n",
    "            n_plan=10,\n",
    "            kappa=0.1,\n",
    "            seed=seed,\n",
    "        ),\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name='Exploration 10% - n=50',\n",
    "            n_plan=50,\n",
    "            kappa=0.1,\n",
    "            seed=seed,\n",
    "        ),\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name='Exploration 1% - n=10',\n",
    "            n_plan=10,\n",
    "            kappa=0.01,\n",
    "            seed=seed,\n",
    "        ),\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name='Exploration 1% - n=50',\n",
    "            n_plan=50,\n",
    "            kappa=0.01,\n",
    "            seed=seed,\n",
    "        ),\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name='Exploration 0.1% - n=10',\n",
    "            n_plan=10,\n",
    "            kappa=0.001,\n",
    "            seed=seed,\n",
    "        ),\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name='Exploration 0.1% - n=50',\n",
    "            n_plan=50,\n",
    "            kappa=0.001,\n",
    "            seed=seed,\n",
    "        ),\n",
    "    ]\n",
    "    plot_gridworld(title=name, agents=agents, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv.blocking_maze(max_steps=300)\n",
    "test_gridworld(name='Blocking Maze', env=env, steps=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv.shortcut_maze(max_steps=300)\n",
    "test_gridworld(name='Shortcut Maze', env=env, steps=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests were made for $n=10$ and $n=50$ (n is the number of simulations in the planning step) and for the cases of no exploration ($\\kappa = 0$) and for $\\kappa=0.1$, $\\kappa=0.01$ and $\\kappa=0.001$. For the cases of no exploration, it was used $\\epsilon=0.1$ and $\\epsilon=0.01$ for the $\\epsilon$-greedy policy (the values of $\\epsilon$ have no impact in the case with $\\kappa > 0$, because it only considers the highest value; the exploration happen due to places that were not visited recently applying a bonus in the value). \n",
    "\n",
    "So, there're a total of 10 different cases for both the blocking maze and the shortcut maze.\n",
    "\n",
    "Based on the results above, the worst cases in both mazes were for $\\kappa=0.1$, followed by the cases of $\\kappa=0.01$ (for both $n=10$ and $n=50$). This is because it explores too much, and takes too much time to reach the destination (this is specially evident in this deterministic environment that changes only once).\n",
    "\n",
    "The best result is both mazes were for $(\\kappa=0, n=50, \\epsilon=0.01)$ and for $(\\kappa=0.001, n=50)$. This is due to the small value of $\\epsilon$ for the case without exploration bonus, and the small value of $\\kappa$ for the case with exploration bonus. After the agent finds the best path, the less exploration to happen, the better. In the initial steps, and also after the environment changes, it's important to have some exploration. These cases found it relatively fast in the initial steps for the blocking maze, and after the environment changed in the shortcut maze, even with a small exploration, due to a high value of $n$ (a lot of simulations in the planning step).\n",
    "\n",
    "Following that, the next best cases are for $(\\kappa=0.001, n=10)$ and $(\\kappa=0, n=50, \\epsilon=0.1)$, The first due to the lower exploration, but still having the bonus exploration that helped it find the changes in the environment relatively fast in the shortcut maze, and to find the initial best path in the blocking maze, the later because it has a high value of $n$ and a good exploration rate due to the value of $\\epsilon$, finding the best path faster due to the high number of simulations per step (this is especially evident when compared to the other identical case except for the value of n, $(\\kappa=0, n=10, \\epsilon=0.1)$, in the shortcut maze, that takes considerably more time to find the best path after the change in the enviroment).\n",
    "\n",
    "To make the agents find the best path faster, the initial values of $Q$ were defined as 1, because they decrease the values of the places that are more distant from the terminal state, as more actions are done in those states, due to the discount factor ($\\gamma$) and reward 0 for the actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.5 \n",
    "\n",
    "**Q**\n",
    "\n",
    "How might the tabular Dyna-Q algorithm shown on page 164 be modified to handle stochastic environments? How might this modification perform poorly on changing environments such as considered in this section? How could the algorithm be modified to handle stochastic environments and changing environments?\n",
    "\n",
    "**A**\n",
    "\n",
    "In a stochastic environment, given a state and an action done in this state, instead of a fixed next state and reward, there's a distribution of probabilities for each possible next state and reward. To handle such an environment, each next state and reward can be given a weight, incremented by 1 every time that case happen, with the planning done based on this distribution: the model in this case returns the distribution probability, and the simulation is done picking one of the possibilities based on its probability.\n",
    "\n",
    "This might perform poorly when the environment changes due to the weights from the previous environment, and lots of real experiments will be required to make the new (actually, updated) weights outweight the (now incorrect) previous weights. To handle both scenarios, before incrementing the weight of the next state and reward given a state and an action, it can multiply all the weights of the distribution of the given state and action by a discount factor smaller than 1 to reduce the overall impact of the previous cases, but higher than 0 to not discard them completely.\n",
    "\n",
    "Of course, making the agent explore more, especially state-action pairs not used for a long time, can also be seen as a possible solution, in which case the Dyna-Q method would actually become Dyna-Q+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.6\n",
    "\n",
    "**Q**\n",
    "\n",
    "The analysis above assumed that all of the *b* possible next states were equally likely to occur. Suppose instead that the distribution was highly skewed, that some of the *b* states were much more likely to occur than most. Would this strengthen or weaken the case for sample updates over expected updates? Support your answer.\n",
    "\n",
    "**A**\n",
    "\n",
    "This would strengthen the case for sample updates over expected updates. The expected updates will continue updating over all possible actions, even for the *b* states that are very unlikely to occur, and consequently won't affect the final value too much. The sample updates, on the other hand, are chosen based on the actual transitions, so the *b* states that are much more likely to occur will be more present in those transitions, and that means that the value updates will be done using the cases that are more likely to happen, which are also the cases that have the most impact in the real value (higher values of $p(s', r | s, a)$, in which *b* affects the amount of possible next states, $s'$, given a state and an action, $s$ and $a$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
