{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 07 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Chapter 6 we noted that the Monte Carlo error can be written as the sum of TD errors (6.6) if the value estimates don’t change from step to step. Show that the n-step error used in (7.2) can also be written as a sum TD errors (again if the value estimates don’t change) generalizing the earlier result.\n",
    "\n",
    "**A**\n",
    "\n",
    "The TD error is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_t \\doteq R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\n",
    "\\end{align*}\n",
    "\n",
    "Equation 6.6 is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1}) \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V(S_{t+2})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V(S_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "Equation 7.2 is:\n",
    "\n",
    "\\begin{align*}\n",
    "V_{t+n} (S_t) \\doteq V_{t+n-1} (S_t) + \\alpha[G_{t:t+n} - V_{t+n-1} (S_t)], \\quad 0 \\leq t \\lt T\n",
    "\\end{align*}\n",
    "\n",
    "with:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1} (S_{t+n})\n",
    "\\end{align*}\n",
    "\n",
    "The n-step TD error in 7.2 is the expression whose difference is multiplied by $\\alpha$ to be added to the old value and give the new (estimated) state value:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} - V_{t+n-1} (S_t) &= R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1} (S_{t+n}) - V_{t+n-1} (S_t) \\\\\n",
    "&= R_{t+1} + \\gamma [R_{t+2} + ... + \\gamma^{n-2} R_{t+n} + \\gamma^{n-1} V_{t+n-1} (S_{t+n})] - V_{t+n-1} (S_t) \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1:t+n} - V_{t+n-1} (S_t) \\\\\n",
    "&= R_{t+1} + \\gamma [G_{t+1:t+n} + V_{t+n-1} (S_{t+1}) - V_{t+n-1} (S_{t+1})] - V_{t+n-1} (S_t) \\\\\n",
    "&= [R_{t+1} + \\gamma V_{t+n-1} (S_{t+1}) - V_{t+n-1} (S_t)] + \\gamma [G_{t+1:t+n} - V_{t+n-1} (S_{t+1})]\n",
    "\\end{align*}\n",
    "\n",
    "Assuming that the value estimates don’t change, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "V_k = V, \\quad \\forall k\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} - V_{t+n-1} (S_t) = G_{t:t+n} - V (S_t)\n",
    "\\end{align*}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} - V_{t+n-1} (S_t) &= [R_{t+1} + \\gamma V_{t+n-1} (S_{t+1}) - V_{t+n-1} (S_t)] + \\gamma [G_{t+1:t+n} - V_{t+n-1} (S_{t+1})] \\\\\n",
    "G_{t:t+n} - V (S_t) &= [R_{t+1} + \\gamma V (S_{t+1}) - V (S_t)] + \\gamma [G_{t+1:t+n} - V (S_{t+1})] \\\\\n",
    "&= \\delta_t + \\gamma [G_{t+1:t+n} - V (S_{t+1})] \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 [G_{t+2:t+n} - V (S_{t+2})] \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{n-2} \\delta_{t+n-2} + \\gamma^{n-1} [G_{t+n-1:t+n} - V (S_{t+n-1})] \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{n-2} \\delta_{t+n-2} + \\gamma^{n-1} [R_{t+n} + \\gamma V_{t+n-1} (S_{t+n}) - V (S_{t+n-1})] \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{n-2} \\delta_{t+n-2} + \\gamma^{n-1} [R_{t+n} + \\gamma V(S_{t+n}) - V (S_{t+n-1})] \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{n-2} \\delta_{t+n-2} + \\gamma^{n-1} \\delta_{t+n-1} \\\\\n",
    "&= \\sum_{k=t}^{t+n-1} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "It's good to take into account that for a final time-step T, the equation corresponds to the Monte Carlo error (t + n = T):\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} - V (S_t) &= \\sum_{k=t}^{t+n-1} \\gamma^{k-t} \\delta_k \\\\\n",
    "G_{t:T} - V (S_t) &= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "It's also importante to note that the value estimates were considered to not change, but in a real scenario it's expected that they change (see exercise 6.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2 (programming) \n",
    "\n",
    "**Q**\n",
    "\n",
    "With an n-step method, the value estimates do change from step to step, so an algorithm that used the sum of TD errors (see previous exercise) in place of the error in (7.2) would actually be a slightly different algorithm. Would it be a better algorithm or a worse one? Devise and program a small experiment to answer this question empirically.\n",
    "\n",
    "**A**\n",
    "\n",
    "The error of n-step TD is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} - V_{t+n-1} (S_t)\n",
    "\\end{align*}\n",
    "\n",
    "The update of n-step TD is:\n",
    "\n",
    "\\begin{align*}\n",
    "V_{t+n} (S_t) \\doteq V_{t+n-1} (S_t) + \\alpha[G_{t:t+n} - V_{t+n-1} (S_t)], \\quad 0 \\leq t \\lt T\n",
    "\\end{align*}\n",
    "\n",
    "Using the sum of TD errors in place of the error, the update would be:\n",
    "\n",
    "\\begin{align*}\n",
    "V_{t+n} (S_t) \\doteq V_{t+n-1} (S_t) + \\alpha \\left[\\sum_{k=t}^{t+n-1} \\gamma^{k-t} \\delta_k \\right], \\quad 0 \\leq t \\lt T\n",
    "\\end{align*}\n",
    "\n",
    "For the small expirement it will be used the random walk experiment and compare the state values across the epochs for the 2 cases, considering the sum of the absolute differences between each state value and the optimal state value. The policy does not matter because it's an MRP, so there's no actions to be taken (for the sake of implementation, it can also be considered to have only one action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy as np\n",
    "from numpy.random import MT19937, Generator\n",
    "\n",
    "def random_generator(seed: int | None = None):\n",
    "    bg = MT19937(seed)\n",
    "    rg = Generator(bg)\n",
    "    return rg\n",
    "\n",
    "class BaseEnv():\n",
    "    def __init__(\n",
    "        self,\n",
    "        states: list[int],\n",
    "        actions: list[int],\n",
    "        terminal_states: set[int] = set(),\n",
    "    ):\n",
    "        num_states = len(states)\n",
    "        num_actions = len(actions)\n",
    "\n",
    "        assert num_states > 0\n",
    "        assert num_actions > 0\n",
    "\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.terminal_states = terminal_states\n",
    "\n",
    "    def reset(self, seed: int | None) -> int:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "def n_step_td_eval(\n",
    "    env: BaseEnv,\n",
    "    v: list[float],\n",
    "    policy: typing.Callable[[int], int],\n",
    "    n: int,\n",
    "    alpha=0.5,\n",
    "    gamma=0.9,\n",
    "    seed: int | None = None,\n",
    "    sum_td_errors=False,\n",
    "):\n",
    "    s_steps = [0] * (n+1)\n",
    "    r_steps = [0] * (n+1)\n",
    "\n",
    "    state = env.reset(seed)\n",
    "    s_steps[0] = state\n",
    "    T: int | None = None\n",
    "    t = 0\n",
    "    tau: int | None = None\n",
    "\n",
    "    def delta(k: int):\n",
    "        return r_steps[(k+1) % (n+1)] + gamma*v[s_steps[(k+1) % (n+1)]] - v[s_steps[k % (n+1)]]\n",
    "\n",
    "    while T is None or tau is None or tau < T-1:\n",
    "        if T is None or t < T:\n",
    "            action = policy(state)\n",
    "            next_state, reward, terminated, truncated = env.step(action)\n",
    "            r_steps[(t+1) % (n+1)] = reward\n",
    "            s_steps[(t+1) % (n+1)] = next_state\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                T = t + 1\n",
    "            state = next_state\n",
    "\n",
    "        tau = t - n + 1\n",
    "\n",
    "        if tau >= 0:\n",
    "            if sum_td_errors:\n",
    "                max_time_step = (min(tau+n-1, T) if T is not None else (tau+n-1))\n",
    "                error = sum([(gamma**(k-tau))*delta(k) for k in range(tau, max_time_step + 1)])\n",
    "            else:\n",
    "                max_time_step = (min(tau+n, T) if T is not None else (tau+n))\n",
    "                G = sum([(gamma**(i-tau-1))*r_steps[i % (n+1)] for i in range(tau+1, max_time_step + 1)])\n",
    "                if T is None or tau + n < T:\n",
    "                    G += (gamma**n)*v[s_steps[(tau+n) % (n+1)]]\n",
    "                error = G - v[s_steps[tau % (n+1)]]\n",
    "\n",
    "            v[s_steps[tau % (n+1)]] += alpha * error\n",
    "\n",
    "        t += 1\n",
    "\n",
    "    return v\n",
    "\n",
    "def evaluate(\n",
    "    env: BaseEnv,\n",
    "    optimal_v: list[float],\n",
    "    policy: typing.Callable[[int], int],\n",
    "    n: int,\n",
    "    episodes: int,\n",
    "    alpha=0.5,\n",
    "    gamma=0.9,\n",
    "    seed: int | None = None,\n",
    "    sum_td_errors=False,\n",
    "):\n",
    "    v = [0.0] * env.num_states\n",
    "    diffs: list[float] = []\n",
    "    rg = random_generator(seed)\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        inner_seed = rg.integers(1e9)\n",
    "        v = n_step_td_eval(\n",
    "            env=env,\n",
    "            v=v,\n",
    "            policy=policy,\n",
    "            n=n,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            seed=inner_seed,\n",
    "            sum_td_errors=sum_td_errors,\n",
    "        )\n",
    "        diff = sum(abs(v[i] - optimal_v[i]) for i in range(len(v)))\n",
    "        diffs.append(diff)\n",
    "\n",
    "    return diffs, v\n",
    "\n",
    "class RandomWalkEnv(BaseEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_steps: int | None = None,\n",
    "    ):\n",
    "        states = list(range(7))\n",
    "        actions = [0] # there's no action (or equivalently, only 1 action)\n",
    "\n",
    "        super().__init__(\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            terminal_states=set([0, len(states) - 1])\n",
    "        )\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "        self.steps = 0\n",
    "        self.state: int | None = None\n",
    "        self.rg = random_generator()\n",
    "\n",
    "    def reset(self, seed: int | None) -> int:\n",
    "        state = 3\n",
    "        self.steps = 0\n",
    "        self.state = state\n",
    "        self.rg = random_generator(seed)\n",
    "        return state\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        max_steps = self.max_steps\n",
    "        steps = self.steps\n",
    "        state = self.state\n",
    "        assert state is not None\n",
    "        assert steps < max_steps\n",
    "\n",
    "        steps += 1\n",
    "        next_state = state + self.rg.choice([1, -1])\n",
    "        reward = 1 if next_state == self.num_states - 1 else 0\n",
    "        terminated = next_state in self.terminal_states\n",
    "        truncated = (not terminated) and (max_steps is not None) and (steps >= max_steps)\n",
    "\n",
    "        self.steps = steps\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, terminated, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_random_walk(n: int):\n",
    "    env = RandomWalkEnv(max_steps=500)\n",
    "    optimal_v = [0] + [i/6 for i in range(1, 6)] + [0]\n",
    "    episodes = 5000\n",
    "    alpha = 0.01\n",
    "    gamma = 1\n",
    "    seed = 0\n",
    "\n",
    "    diffs_n_step_error, v_n_step_error = evaluate(\n",
    "        env=env,\n",
    "        optimal_v=optimal_v,\n",
    "        policy=lambda state: 0,\n",
    "        n=n,\n",
    "        episodes=episodes,\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        seed=seed,\n",
    "        sum_td_errors=False,\n",
    "    )\n",
    "\n",
    "    diffs_sum_error_td, v_sum_error_td = evaluate(\n",
    "        env=env,\n",
    "        optimal_v=optimal_v,\n",
    "        policy=lambda state: 0,\n",
    "        n=n,\n",
    "        episodes=episodes,\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        seed=seed,\n",
    "        sum_td_errors=True,\n",
    "    )\n",
    "\n",
    "    print(f'[n={n}] v_n_step_error={v_n_step_error}')\n",
    "    print(f'[n={n}] v_sum_error_td={v_sum_error_td}')\n",
    "    print('-' * 80)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(diffs_n_step_error, label='n-step error')\n",
    "    plt.plot(diffs_sum_error_td, label='sum td error')\n",
    "    plt.title(f'Algorithms Comparation (n={n})')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1, 10):\n",
    "    evaluate_random_walk(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the graphs (the closer to 0, the better), for $n = 1$ both cases are equally good (the difference between the predicted values after running all the episodes and the optimal/expected values is the same after each episode, using the same seeds for both, which is expected, as they are basically the same algorithm in this case). For $n \\gt 1$, the default n-step error is better, with the sum of the TD errors to define the n-step error giving a greater variance in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3 \n",
    "\n",
    "**Q**\n",
    "\n",
    "Why do you think a larger random walk task (19 states instead of 5) was used in the examples of this chapter? Would a smaller walk have shifted the advantage to a different value of n? How about the change in left-side outcome from 0 to -1 made in the larger walk? Do you think that made any difference in the best value of n?\n",
    "\n",
    "**A**\n",
    "\n",
    "More states give more opportunities for an episode have more steps. Too few states might make several episodes have less than n steps for and an n-step TD, in which case a k-step TD would provide the same result, for k being the number of the time-steps of the episode (k < n in these cases). More states would make the probability of having more time-steps higher, which would create a more adequate scenario to compare n-steps TD for different values of n. With less states, the best value of n should decrease as well.\n",
    "\n",
    "Giving a value of -1 to the leftmost state shouldn't have a significant impact. It could speed up the initial iterations, tough, if the values started with 0, instead of 0.5 (because receiving a reward of -1 when reaching the leftmost state would decrease its value even when the value was not changed before). In the proposed scenario, tough, it should not have a great impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4 \n",
    "\n",
    "**Q**\n",
    "\n",
    "Prove that the n-step return of Sarsa (7.4) can be written exactly in terms of a novel TD error, as:\n",
    "\n",
    "$$\n",
    "G_{t:t+n} = Q_{t-1}(S_t, A_t) + \\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} [R_{k+1} + \\gamma Q_k(S_{k+1}, A_{k+1}) - Q_{k-1}(S_k, A_k)] \n",
    "$$\n",
    "\n",
    "**A**\n",
    "\n",
    "The n-step return of Sarsa (7.4) is:\n",
    "\n",
    "$$\n",
    "G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}), \\quad n \\geq 1, 0 \\leq t \\lt T - n\n",
    "$$\n",
    "\n",
    "The update algorithm is:\n",
    "\n",
    "$$\n",
    "Q_{t+n} (S_t, A_t) \\doteq Q_{t+n-1}(S_t, A_t) + \\alpha [G_{t:t+n} - Q_{t+n-1}(S_t, A_t)], \\quad 0 \\leq t \\lt T\n",
    "$$\n",
    "\n",
    "Expanding 7.4, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} &\\doteq R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}) \\\\\n",
    "&= [R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n})] \\\\\n",
    "&+ [Q_{t-1}(S_t, A_t) - Q_{t-1}(S_t, A_t)] + \\gamma [Q_t(S_{t+1}, A_{t+1}) - Q_t(S_{t+1}, A_{t+1})] \\\\\n",
    "&+ ... + \\gamma^{n-1} [Q_{t+n-2}(S_{t+n-1}, A_{t+n-1}) - Q_{t+n-2}(S_{t+n-1}, A_{t+n-1})] \\\\\n",
    "&= Q_{t-1}(S_t, A_t) + [R_{t+1} + \\gamma Q_t(S_{t+1}, A_{t+1}) - Q_{t-1}(S_t, A_t)] \\\\\n",
    "& + \\gamma [R_{t+2} + \\gamma Q_{t+1}(S_{t+2}, A_{t+2}) - Q_t(S_{t+1}, A_{t+1})] + ... \\\\\n",
    "&+ ... + \\gamma^{n-1} [R_{t+n} + \\gamma Q_{t+n-1}(S_{t+n}, A_{t+n}) - Q_{t+n-2}(S_{t+n-1}, A_{t+n-1})] \\\\\n",
    "&= Q_{t-1}(S_t, A_t) + \\sum_{k=t}^{t+n-1} \\gamma^{k-t} [R_{k+1} + \\gamma Q_k(S_{k+1}, A_{k+1}) - Q_{k-1}(S_k, A_k)], \\quad n \\geq 1, 0 \\leq t \\lt T - n\n",
    "\\end{align*}\n",
    "\n",
    "The above is valid for $0 \\leq t \\lt T - n$. It can be generalized for all $t \\leq T$ (with $G_{T:T+n} = G_T = 0$) considering the interval of time-steps in the sum going at most until T-1 (the last time-step in which an action can be taken), with reward from time-step T as $R_T$, and the action-value $Q_{T-1}(S_T, A_T) = 0$, because $S_T$ is the terminal state:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} &\\doteq Q_{t-1}(S_t, A_t) + \\sum_{k=t}^{min(t+n, T)-1} \\gamma^{k-t} [R_{k+1} + \\gamma Q_k(S_{k+1}, A_{k+1}) - Q_{k-1}(S_k, A_k)]\n",
    "\\end{align*}\n",
    "\n",
    "It can also be proved inversely:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} &= Q_{t-1}(S_t, A_t) + \\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} [R_{k+1} + \\gamma Q_k(S_{k+1}, A_{k+1}) - Q_{k-1}(S_k, A_k)] \\\\\n",
    "&= [Q_{t-1}(S_t, A_t)] + \\left[\\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} R_{k+1} \\right] + \\left[\\gamma \\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} Q_k(S_{k+1}, A_{k+1}) \\right] - \\left[\\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} Q_{k-1}(S_k, A_k) \\right] \\\\\n",
    "&= [Q_{t-1}(S_t, A_t)] + \\left[\\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} R_{k+1} \\right] + \\left[\\gamma \\sum_{k=t+1}^{min(t+n, T)} \\gamma^{k-t-1} Q_{k-1}(S_k, A_k) \\right] - \\left[\\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} Q_{k-1}(S_k, A_k) \\right] \\\\\n",
    "&= [Q_{t-1}(S_t, A_t)] + \\left[\\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} R_{k+1} \\right] + \\left[\\sum_{k=t+1}^{min(t+n, T)} \\gamma^{k-t} Q_{k-1}(S_k, A_k) \\right] - \\left[\\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} Q_{k-1}(S_k, A_k) \\right] \\\\\n",
    "&= [Q_{t-1}(S_t, A_t)] + \\left[\\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} R_{k+1} \\right] \\\\\n",
    "&+ \\left[ \\left[\\sum_{k=t}^{min(t+n, T)} \\gamma^{k-t} Q_{k-1}(S_k, A_k) \\right] - \\left[\\gamma^{t-t} Q_{t-1}(S_t, A_t) \\right] \\right] \\\\\n",
    "&- \\left[ \\left[\\sum_{k=t}^{min(t+n, T)} \\gamma^{k-t} Q_{k-1}(S_k, A_k) \\right] - \\left[ \\gamma^{min(t+n, T)-t} Q_{min(t+n, T)-1}(S_{min(t+n, T)}, A_{min(t+n, T)}) \\right] \\right] \\\\\n",
    "&= \\left[\\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} R_{k+1} \\right] + \\left[ \\gamma^{min(t+n, T)-t} Q_{min(t+n, T)-1}(S_{min(t+n, T)}, A_{min(t+n, T)}) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "For $0 \\leq t \\lt T - n$, $min(t+n, T) = t+n$, and the above equation becomes 7.4:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} &= \\left[\\sum_{k=t}^{min(t+n, T) - 1} \\gamma^{k-t} R_{k+1} \\right] + \\left[ \\gamma^{min(t+n, T)-t} Q_{min(t+n, T)-1}(S_{min(t+n, T)}, A_{min(t+n, T)}) \\right] \\\\\n",
    "&= \\left[\\sum_{k=t}^{t+n - 1} \\gamma^{k-t} R_{k+1} \\right] + \\left[ \\gamma^{t+n-t} Q_{t+n-1}(S_{t+n}, A_{t+n}) \\right] \\\\\n",
    "&= R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}), \\quad n \\geq 1, 0 \\leq t \\lt T - n\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5 \n",
    "\n",
    "**Q**\n",
    "\n",
    "Write the pseudocode for the off-policy state-value prediction algorithm described above.\n",
    "\n",
    "**A**\n",
    "\n",
    "The update will be done as defined by 7.2:\n",
    "\n",
    "$$\n",
    "V_{t+n}(S_t) \\doteq V_{t+n-1}(S_t) + \\alpha [G_{t:t+n} - V_{t+n-1}(S_t)], \\quad 0 \\leq t < T\n",
    "$$\n",
    "\n",
    "The return using the importance sampling ratios, as defined by 7.13:\n",
    "\n",
    "$$\n",
    "G_{t:h} \\doteq \\rho_t (R_{t+1} + \\gamma G_{t+1:h}) + (1 - \\rho_t) V_{h-1}(S_t), \\quad t < h < T\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "G_{h:h} \\doteq V_{h-1}(S_h)\n",
    "$$\n",
    "\n",
    "The algorithm is:\n",
    "\n",
    "> Input: an arbitrary behavior policy $b$ such that $b(a|s) > 0$, for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$<br/>\n",
    "> Initialize $V(s)$ arbitrarily, for all $s \\in \\mathcal{S}$ ($V(S_T) = 0$ for all terminal states)<br/>\n",
    "> Initialize $\\pi$ as a fixed given policy (this is a prediction algorithm; for a control algorithm it's recommended to use state-action values instead of state values)<br/>\n",
    "> Algorithm parameters: step size $\\alpha \\in (0, 1]$, a positive integer $n$<br/>\n",
    "> All store and access operations (for $S_t$, $A_t$, and $R_t$) can take their index mod $n + 1$\n",
    ">\n",
    "> Loop for each episode:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Initialize and store $S_0 \\neq terminal$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$T \\gets \\infty$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Loop for $t = 0, 1, 2, ...$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If $t < T$, then:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select and store an action $A_t \\sim b(\\cdot | S_t)$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Take action $A_t$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If $S_{t+1}$ is terminal, then:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T \\gets t + 1$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\tau \\gets t - n + 1$ ($\\tau$ is the time whose estimate is being updated)<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If $\\tau \\geq 0$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$k \\gets min(t, T) - 1$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$G \\gets V(S_{k+1})$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while $k \\geq \\tau$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\rho \\gets \\frac{\\pi(A_k | S_k)}{b(A_k | S_k)}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$G \\gets \\rho [R_{k+1} + \\gamma G] + (1 - \\rho) V(S_k)$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$k \\gets k - 1$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(S_{\\tau}) \\gets V(S_{\\tau}) + \\alpha [G - V(S_{\\tau})]$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;until $\\tau = T - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6 \n",
    "\n",
    "**Q**\n",
    "\n",
    "Prove that the control variate in the above equations does not change the expected value of the return.\n",
    "\n",
    "**A**\n",
    "\n",
    "Equation 7.12 (to calculate $G_{t:h}$ using the on-policy algorithm with the state values) is:\n",
    "\n",
    "$$\n",
    "G_{t:h} \\doteq R_{t+1} + \\gamma G_{t+1:h}, \\quad t < h < T \\tag{a1}\n",
    "$$\n",
    "\n",
    "Equation 7.13 (to calculate $G_{t:h}$ using the off-policy algorithm with the state values) is:\n",
    "\n",
    "$$\n",
    "G_{t:h} \\doteq \\rho_t (R_{t+1} + \\gamma G_{t+1:h}) + (1 - \\rho_t) V_{h-1}(S_t), \\quad t < h < T \\tag{a2}\n",
    "$$\n",
    "\n",
    "Equation 7.7 (to calculate $G_{t:t+n}$ using the on-policy algorithm with the state-action values according to the expected Sarsa algorithm, with $n \\geq 1$) is:\n",
    "\n",
    "$$\n",
    "G_{t:t+n} \\doteq R_{t+1} + ... +  \\gamma^{n-1} R_{t+n} + \\gamma^n \\overline{V}_{t+n-1} (S_{t+n}), \\quad t + n < T\n",
    "$$\n",
    "\n",
    "or, equivalently, for $h = t + n$:\n",
    "\n",
    "$$\n",
    "G_{t:h} \\doteq R_{t+1} + ... +  \\gamma^{n-1} R_h + \\gamma^n \\overline{V}_{h-1} (S_h), \\quad t < h < T\n",
    "$$\n",
    "\n",
    "or even:\n",
    "\n",
    "$$\n",
    "G_{t:h} \\doteq R_{t+1} + \\gamma G_{t+1:h}, \\quad t < h < T \\tag{a3}\n",
    "$$\n",
    "\n",
    "which is the same as $a1$, with $G_{t:h} \\doteq G_t$, for $h \\geq T$ and, according to 7.8:\n",
    "\n",
    "$$\n",
    "\\overline{V}_t (s) \\doteq \\sum_a \\pi(a | s)Q_t(s, a), \\quad \\forall s \\in \\mathcal{S} \\tag{a4}\n",
    "$$\n",
    "\n",
    "Equation 7.14 (to calculate $G_{t:h}$ using the off-policy algorithm with the state-action values) is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:h} &\\doteq R_{t+1} + \\gamma(\\rho_{t+1} G_{t+1:h} + \\overline{V}_{h-1} (S_{t+1}) - \\rho_{t+1} Q_{h-1} (S_{t+1}, A_{t+1})) \\\\\n",
    "&= R_{t+1} + \\gamma \\rho_{t+1} (G_{t+1:h} - Q_{h-1} (S_{t+1}, A_{t+1})) + \\gamma \\overline{V}_{h-1} (S_{t+1}), \\quad t \\lt h \\leq T \\tag{a5}\n",
    "\\end{align*}\n",
    "\n",
    "The value of $V_k(s)$ can be considered a constant $\\forall k \\in [0, T]$ and $\\forall s \\in \\mathcal{S}$ (because it's the actual value assigned to V to be used by the algorithm, even if it's not the correct state value), so the expectation of it is the value itself ($\\mathbb{E} [V_k(s)] = V_k(s)$).\n",
    "\n",
    "The policy used to choose the actions is $b$, but the values are updated based on the policy $\\pi$, so the control variate does not change the expectation if, and only if, the expectation of the off-police $G_{t:h}$ over $b$ is the same as the expectation of the on-policy $G_{t:h}$ over $\\pi$, that is, for the equation using state values (according to the off-policy algorithm, $a2$, and the on-policy algorithm, $a1$):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_b [\\rho_t (R_{t+1} + \\gamma G_{t+1:h}) + (1 - \\rho_t) V_{h-1}(S_t)] = \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma G_{t+1:h}] = \\mathbb{E}_{\\pi} [G_{t:h}]\n",
    "$$\n",
    "\n",
    "and for state-action values, according to the off-policy $a5$ and the on-policy $a3$ (which is the same as $a1$, the most basic form of definition of $G_{t:h}$):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_b [R_{t+1} + \\gamma \\rho_{t+1} (G_{t+1:h} - Q_{h-1} (S_{t+1}, A_{t+1})) + \\gamma \\overline{V}_{h-1} (S_{t+1})] = \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma G_{t+1:h}] = \\mathbb{E}_{\\pi} [G_{t:h}]\n",
    "$$\n",
    "\n",
    "It's important to note that the expectation of $\\rho_t$ over the policy $b$, for any $t \\in [0, T]$, is 1:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_b [\\rho_t | S_t=s] = \\mathbb{E}_b \\left[ \\frac{\\pi(A_t | s)}{b(A_t | s)} | S_t=s \\right] = \\sum_a b(a | s) \\frac{\\pi(a | s)}{b(a | s)} = \\sum_a \\pi(a | s) = 1\n",
    "$$\n",
    "\n",
    "because for any policy, the sum of probabilities for choosing each action for a given state is 1 (a policy is basically a distribution of probabilities of choosing an action given a state).\n",
    "\n",
    "So, for the case of using state values, according to 7.13 ($a2$), the expected return is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_b [G_{t:h} | S_t=s] &= \\mathbb{E}_b[\\rho_t (R_{t+1} + \\gamma G_{t+1:h}) + (1 - \\rho_t) V_{h-1}(s) | S_t=s] \\\\\n",
    "&= \\mathbb{E}_b[\\rho_t (R_{t+1} + \\gamma G_{t+1:h}) | S_t=s] + \\mathbb{E}_b[(1 - \\rho_t) V_{h-1}(s) | S_t=s] \\\\\n",
    "&= \\mathbb{E}_b\\left[ \\frac{\\pi(A_t | s)}{b(A_t | s)} (R_{t+1} + \\gamma G_{t+1:h}) | S_t=s \\right] + \\mathbb{E}_b\\left[ \\left(1 - \\frac{\\pi(A_t | s)}{b(A_t | s)} \\right) V_{h-1}(s) | S_t=s \\right] \\\\\n",
    "&= \\sum_a b(a | s) \\frac{\\pi(a | s)}{b(a | s)} (R_{t+1} + \\gamma G_{t+1:h}) + \\sum_a b(a | s) \\left(1 - \\frac{\\pi(a | s)}{b(a | s)} \\right) V_{h-1}(s) \\\\\n",
    "&= \\sum_a \\pi(a | s) (R_{t+1} + \\gamma G_{t+1:h}) + \\sum_a (b(a | s) - \\pi(a | s)) V_{h-1}(s) \\\\\n",
    "&= \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma G_{t+1:h} | S_t=s] + V_{h-1}(s) \\left( \\sum_a [b(a | s)] - \\sum_a [\\pi(a | s)] \\right) \\\\\n",
    "&= \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma G_{t+1:h} | S_t=s] + V_{h-1}(s) (1 - 1) \\\\\n",
    "&= \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma G_{t+1:h} | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi} [G_{t:h} | S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "For the case of using state-action values, according to 7.14 ($a5$), the expected return is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_b [G_{t:h} | S_t=s, A_t=a] &= \\mathbb{E}_b[R_{t+1} + \\gamma \\rho_{t+1} (G_{t+1:h} - Q_{h-1} (S_{t+1}, A_{t+1})) + \\gamma \\overline{V}_{h-1} (S_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}_b\\left[ \\sum_{r, s'} p(s', r | s, a) \\left[ r + \\gamma \\frac{\\pi(A_{t+1} | s')}{b(A_{t+1} | s')} (G_{t+1:h} - Q_{h-1} (s', A_{t+1})) + \\gamma \\overline{V}_{h-1} (s') \\right] | S_t=s, A_t=a, R_{t+1}=r, S_{t+1}=s' \\right] \\\\\n",
    "&= \\sum_{r, s'} p(s', r | s, a) \\sum_{a'} b(a' | s') \\left[ r + \\gamma \\frac{\\pi(a' | s')}{b(a' | s')} (G_{t+1:h} - Q_{h-1} (s', a')) + \\gamma \\left( \\sum_{a''} \\pi(a'' | s')Q_{h-1}(s', a'') \\right) \\right] \\\\\n",
    "&= \\sum_{r, s'} p(s', r | s, a) \\left[ \\left[ \\sum_{a'} b(a' | s') r \\right] + \\left[ \\sum_{a'} [\\gamma \\pi(a' | s') G_{t+1:h}] \\right] - \\left[ \\sum_{a'} [\\gamma \\pi(a' | s') Q_{h-1} (s', a')] \\right] + \\left[ \\sum_{a'} b(a' | s') \\left( \\gamma \\sum_{a''} \\pi(a'' | s')Q_{h-1}(s', a'') \\right) \\right] \\right] \\\\\n",
    "&= \\sum_{r, s'} p(s', r | s, a) \\left[ r + \\left[ \\sum_{a'} [\\gamma \\pi(a' | s') G_{t+1:h}] \\right] - \\left[ \\sum_{a'} [\\gamma \\pi(a' | s') Q_{h-1} (s', a')] \\right] + \\left( \\sum_{a''} \\gamma \\pi(a'' | s')Q_{h-1}(s', a'') \\right) \\right] \\\\\n",
    "&= \\sum_{r, s'} p(s', r | s, a) \\left[ r + \\left[ \\sum_{a'} [\\gamma \\pi(a' | s') G_{t+1:h}] \\right] \\right] \\\\\n",
    "&= \\sum_{r, s'} p(s', r | s, a) \\left[ \\left[ \\sum_{a'} \\pi(a' | s') r \\right] + \\left[ \\sum_{a'} [\\gamma \\pi(a' | s') G_{t+1:h}] \\right] \\right] \\\\\n",
    "&= \\sum_{r, s'} p(s', r | s, a) \\sum_{a'} \\pi(a' | s') \\left[ r + \\gamma G_{t+1:h} \\right] \\\\\n",
    "&= \\sum_{r, s'} p(s', r | s, a) \\mathbb{E}_{\\pi} \\left[ r + \\gamma G_{t+1:h} | S_t=s, A_t=a, R_{t+1}=r, S_{t+1}=s' \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma G_{t+1:h} | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}_{\\pi} [G_{t:h} | S_t=s, A_t=a] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The above 2 cases proved the expectation for the 2 equations.\n",
    "\n",
    "It's important to note that if a value is constant regarding an action $a$ (or $a'$), then the sum of the distribution over all actions for a state is the value itself. So, for example:\n",
    "\n",
    "$$\n",
    "\\sum_{a'} [b(a' | s') r] = r \\sum_{a'} b(a' | s') = r \\cdot 1 = r\n",
    "$$\n",
    "\n",
    "and inversely, for $\\pi$:\n",
    "\n",
    "$$\n",
    "r = r \\sum_{a'} \\pi(a' | s') = \\sum_{a'} [\\pi(a' | s') r]\n",
    "$$\n",
    "\n",
    "This is valid even for more complex cases:\n",
    "\n",
    "$$\n",
    "\\sum_{a'} \\left[ b(a' | s') \\left( \\gamma \\sum_{a''} \\pi(a'' | s')Q_{h-1}(s', a'') \\right) \\right] = \\gamma \\sum_{a''} \\pi(a'' | s')Q_{h-1}(s', a'') \\cdot \\left[ \\sum_{a'} b(a' | s') \\right] = \\gamma \\sum_{a''} \\pi(a'' | s')Q_{h-1}(s', a'')\n",
    "$$\n",
    "\n",
    "Also, $\\sum_x f(x) = \\sum_{x'} f(x')$ if both $x$ and $x'$ use all possible values from the same space. So, for $f(x) = \\gamma \\pi(x | s') Q_{h-1} (s', x)$, with $x \\in \\mathcal{A}(s')$ and $x' \\in \\mathcal{A}(s')$:\n",
    "\n",
    "$$\n",
    "\\sum_{a'} [\\gamma \\pi(a' | s') Q_{h-1} (s', a')] = \\sum_{a''} \\left[ \\gamma \\pi(a'' | s')Q_{h-1}(s', a'') \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.7 \n",
    "\n",
    "**Q**\n",
    "\n",
    "Write the pseudocode for the off-policy action-value prediction algorithm described immediately above. Pay particular attention to the termination conditions for the recursion upon hitting the horizon or the end of episode.\n",
    "\n",
    "**A**\n",
    "\n",
    "The action-value prediction algorithm described is 7.14:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:h} &\\doteq R_{t+1} + \\gamma(\\rho_{t+1} G_{t+1:h} + \\overline{V}_{h-1} (S_{t+1}) - \\rho_{t+1} Q_{h-1} (S_{t+1}, A_{t+1})) \\\\\n",
    "&= R_{t+1} + \\gamma \\rho_{t+1} (G_{t+1:h} - Q_{h-1} (S_{t+1}, A_{t+1})) + \\gamma \\overline{V}_{h-1} (S_{t+1}), \\quad t \\lt h \\leq T\n",
    "\\end{align*}\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "G_{h:h} \\doteq Q_{h-1}(S_h, A_h)\n",
    "$$\n",
    "\n",
    "and (7.8):\n",
    "\n",
    "$$\n",
    "\\overline{V}_t (s) \\doteq \\sum_a \\pi(a | s)Q_t(s, a), \\quad \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "The pseudocode is:\n",
    "\n",
    "> Input: an arbitrary behavior policy $b$ such that $b(a|s) > 0$, for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$<br/>\n",
    "> Initialize $Q(s, a)$ arbitrarily, for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$<br/>\n",
    "> Initialize $\\pi$ to be greedy with respect to Q, or as a fixed given policy<br/>\n",
    "> Initialize $\\overline{V} (s) = \\sum_a \\pi(a | s)Q(s, a)$, for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$<br/>\n",
    "> Algorithm parameters: step size $\\alpha \\in (0, 1]$, a positive integer $n$<br/>\n",
    "> All store and access operations (for $S_t$, $A_t$, and $R_t$) can take their index mod $n + 1$\n",
    ">\n",
    "> Loop for each episode:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Initialize and store $S_0 \\neq terminal$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Select and store an action $A_0 \\sim b(\\cdot | S_0)$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$T \\gets \\infty$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Loop for $t = 0, 1, 2, ...$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If $t < T$, then:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Take action $A_t$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If $S_{t+1}$ is terminal, then:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T \\gets t + 1$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select and store an action $A_{t+1} \\sim b(\\cdot | S_{t+1})$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\tau \\gets t - n + 1$ ($\\tau$ is the time whose estimate is being updated)<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If $\\tau \\geq 0$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$k \\gets min(t, T) - 1$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $k + 1 = T$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$G \\gets 0$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$G \\gets Q(S_{k+1}, A_{k+1})$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while $k \\geq \\tau$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $k + 1 = T$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\rho \\gets 1$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$q \\gets 0$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\rho \\gets \\frac{\\pi(A_{k+1} | S_{k+1})}{b(A_{k+1} | S_{k+1})}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$q \\gets Q(S_{k+1}, A_{k+1})$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$G \\gets R_{k+1} + \\gamma \\rho [G - q] + \\gamma \\overline{V} (S_{k+1})$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$k \\gets k - 1$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$Q(S_{\\tau}, A_{\\tau}) \\gets Q(S_{\\tau}, A_{\\tau}) + \\alpha [G - Q(S_{\\tau}, A_{\\tau})]$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\overline{V}(S_{\\tau}) \\gets 0$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for all $a$ in $\\mathcal{A}(S_{\\tau})$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\overline{V}(S_{\\tau}) \\gets \\overline{V}(S_{\\tau}) + \\pi(a | S_{\\tau}) Q(S_{\\tau}, a)$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If $\\pi$ is being learned, then ensure that $\\pi(\\cdot | S_{\\tau})$ is greedy wrt Q<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;until $\\tau = T - 1$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
