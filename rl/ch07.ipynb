{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 07 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "In Chapter 6 we noted that the Monte Carlo error can be written as the sum of TD errors (6.6) if the value estimates don’t change from step to step. Show that the n-step error used in (7.2) can also be written as a sum TD errors (again if the value estimates don’t change) generalizing the earlier result.\n",
    "\n",
    "**A**\n",
    "\n",
    "The TD error is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_t \\doteq R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\n",
    "\\end{align*}\n",
    "\n",
    "Equation 6.6 is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1}) \\\\\n",
    "&= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V(S_{t+2})) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V(S_T)) \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) \\\\\n",
    "&= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "Equation 7.2 is:\n",
    "\n",
    "\\begin{align*}\n",
    "V_{t+n} (S_t) \\doteq V_{t+n-1} (S_t) + \\alpha[G_{t:t+n} - V_{t+n-1} (S_t)], \\quad 0 \\leq t \\lt T\n",
    "\\end{align*}\n",
    "\n",
    "with:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1} (S_{t+n})\n",
    "\\end{align*}\n",
    "\n",
    "The n-step TD error in 7.2 is the expression whose difference is multiplied by $\\alpha$ to be added to the old value and give the new (estimated) state value:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} - V_{t+n-1} (S_t) &= R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1} (S_{t+n}) - V_{t+n-1} (S_t) \\\\\n",
    "&= R_{t+1} + \\gamma [R_{t+2} + ... + \\gamma^{n-2} R_{t+n} + \\gamma^{n-1} V_{t+n-1} (S_{t+n})] - V_{t+n-1} (S_t) \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1:t+n} - V_{t+n-1} (S_t) \\\\\n",
    "&= R_{t+1} + \\gamma [G_{t+1:t+n} + V_{t+n-1} (S_{t+1}) - V_{t+n-1} (S_{t+1})] - V_{t+n-1} (S_t) \\\\\n",
    "&= [R_{t+1} + \\gamma V_{t+n-1} (S_{t+1}) - V_{t+n-1} (S_t)] + \\gamma [G_{t+1:t+n} - V_{t+n-1} (S_{t+1})]\n",
    "\\end{align*}\n",
    "\n",
    "Assuming that the value estimates don’t change, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "V_k = V, \\quad \\forall k\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} - V_{t+n-1} (S_t) = G_{t:t+n} - V (S_t)\n",
    "\\end{align*}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} - V_{t+n-1} (S_t) &= [R_{t+1} + \\gamma V_{t+n-1} (S_{t+1}) - V_{t+n-1} (S_t)] + \\gamma [G_{t+1:t+n} - V_{t+n-1} (S_{t+1})] \\\\\n",
    "G_{t:t+n} - V (S_t) &= [R_{t+1} + \\gamma V (S_{t+1}) - V (S_t)] + \\gamma [G_{t+1:t+n} - V (S_{t+1})] \\\\\n",
    "&= \\delta_t + \\gamma [G_{t+1:t+n} - V (S_{t+1})] \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 [G_{t+2:t+n} - V (S_{t+2})] \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{n-2} \\delta_{t+n-2} + \\gamma^{n-1} [G_{t+n-1:t+n} - V (S_{t+n-1})] \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{n-2} \\delta_{t+n-2} + \\gamma^{n-1} [R_{t+n} + \\gamma V_{t+n-1} (S_{t+n}) - V (S_{t+n-1})] \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{n-2} \\delta_{t+n-2} + \\gamma^{n-1} [R_{t+n} + \\gamma V(S_{t+n}) - V (S_{t+n-1})] \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + ... + \\gamma^{n-2} \\delta_{t+n-2} + \\gamma^{n-1} \\delta_{t+n-1} \\\\\n",
    "&= \\sum_{k=t}^{t+n-1} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "It's good to take into account that for a final time-step T, the equation corresponds to the Monte Carlo error (t + n = T):\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} - V (S_t) &= \\sum_{k=t}^{t+n-1} \\gamma^{k-t} \\delta_k \\\\\n",
    "G_{t:T} - V (S_t) &= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k\n",
    "\\end{align*}\n",
    "\n",
    "It's also importante to note that the value estimates were considered to not change, but in a real scenario it's expected that they change (see exercise 6.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2 (programming) \n",
    "\n",
    "**Q**\n",
    "\n",
    "With an n-step method, the value estimates do change from step to step, so an algorithm that used the sum of TD errors (see previous exercise) in place of the error in (7.2) would actually be a slightly different algorithm. Would it be a better algorithm or a worse one? Devise and program a small experiment to answer this question empirically.\n",
    "\n",
    "**A**\n",
    "\n",
    "The error of n-step TD is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{t:t+n} - V_{t+n-1} (S_t)\n",
    "\\end{align*}\n",
    "\n",
    "The update of n-step TD is:\n",
    "\n",
    "\\begin{align*}\n",
    "V_{t+n} (S_t) \\doteq V_{t+n-1} (S_t) + \\alpha[G_{t:t+n} - V_{t+n-1} (S_t)], \\quad 0 \\leq t \\lt T\n",
    "\\end{align*}\n",
    "\n",
    "Using the sum of TD errors in place of the error, the update would be:\n",
    "\n",
    "\\begin{align*}\n",
    "V_{t+n} (S_t) \\doteq V_{t+n-1} (S_t) + \\alpha \\left[\\sum_{k=t}^{t+n-1} \\gamma^{k-t} \\delta_k \\right], \\quad 0 \\leq t \\lt T\n",
    "\\end{align*}\n",
    "\n",
    "For the small expirement it will be used the random walk experiment and compare the state values across the epochs for the 2 cases, considering the sum of the absolute differences between each state value and the optimal state value. The policy does not matter because it's an MRP, so there's no actions to be taken (for the sake of implementation, it can also be considered to have only one action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy as np\n",
    "from numpy.random import MT19937, Generator\n",
    "\n",
    "def random_generator(seed: int | None = None):\n",
    "    bg = MT19937(seed)\n",
    "    rg = Generator(bg)\n",
    "    return rg\n",
    "\n",
    "class BaseEnv():\n",
    "    def __init__(\n",
    "        self,\n",
    "        states: list[int],\n",
    "        actions: list[int],\n",
    "        terminal_states: set[int] = set(),\n",
    "    ):\n",
    "        num_states = len(states)\n",
    "        num_actions = len(actions)\n",
    "\n",
    "        assert num_states > 0\n",
    "        assert num_actions > 0\n",
    "\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.terminal_states = terminal_states\n",
    "\n",
    "    def reset(self, seed: int | None) -> int:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "def n_step_td_eval(\n",
    "    env: BaseEnv,\n",
    "    v: list[float],\n",
    "    policy: typing.Callable[[int], int],\n",
    "    n: int,\n",
    "    alpha=0.5,\n",
    "    gamma=0.9,\n",
    "    seed: int | None = None,\n",
    "    sum_td_errors=False,\n",
    "):\n",
    "    s_steps = [0] * (n+1)\n",
    "    r_steps = [0] * (n+1)\n",
    "\n",
    "    state = env.reset(seed)\n",
    "    s_steps[0] = state\n",
    "    T: int | None = None\n",
    "    t = 0\n",
    "    tau: int | None = None\n",
    "\n",
    "    def delta(k: int):\n",
    "        return r_steps[(k+1) % (n+1)] + gamma*v[s_steps[(k+1) % (n+1)]] - v[s_steps[k % (n+1)]]\n",
    "\n",
    "    while T is None or tau is None or tau < T-1:\n",
    "        if T is None or t < T:\n",
    "            action = policy(state)\n",
    "            next_state, reward, terminated, truncated = env.step(action)\n",
    "            r_steps[(t+1) % (n+1)] = reward\n",
    "            s_steps[(t+1) % (n+1)] = next_state\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                T = t + 1\n",
    "            state = next_state\n",
    "\n",
    "        tau = t - n + 1\n",
    "\n",
    "        if tau >= 0:\n",
    "            if sum_td_errors:\n",
    "                max_time_step = (min(tau+n-1, T) if T is not None else (tau+n-1))\n",
    "                error = sum([(gamma**(k-tau))*delta(k) for k in range(tau, max_time_step + 1)])\n",
    "            else:\n",
    "                max_time_step = (min(tau+n, T) if T is not None else (tau+n))\n",
    "                G = sum([(gamma**(i-tau-1))*r_steps[i % (n+1)] for i in range(tau+1, max_time_step + 1)])\n",
    "                if T is None or tau + n < T:\n",
    "                    G += (gamma**n)*v[s_steps[(tau+n) % (n+1)]]\n",
    "                error = G - v[s_steps[tau % (n+1)]]\n",
    "\n",
    "            v[s_steps[tau % (n+1)]] += alpha * error\n",
    "\n",
    "        t += 1\n",
    "\n",
    "    return v\n",
    "\n",
    "def evaluate(\n",
    "    env: BaseEnv,\n",
    "    optimal_v: list[float],\n",
    "    policy: typing.Callable[[int], int],\n",
    "    n: int,\n",
    "    episodes: int,\n",
    "    alpha=0.5,\n",
    "    gamma=0.9,\n",
    "    seed: int | None = None,\n",
    "    sum_td_errors=False,\n",
    "):\n",
    "    v = [0.0] * env.num_states\n",
    "    diffs: list[float] = []\n",
    "    rg = random_generator(seed)\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        inner_seed = rg.integers(1e9)\n",
    "        v = n_step_td_eval(\n",
    "            env=env,\n",
    "            v=v,\n",
    "            policy=policy,\n",
    "            n=n,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            seed=inner_seed,\n",
    "            sum_td_errors=sum_td_errors,\n",
    "        )\n",
    "        diff = sum(abs(v[i] - optimal_v[i]) for i in range(len(v)))\n",
    "        diffs.append(diff)\n",
    "\n",
    "    return diffs, v\n",
    "\n",
    "class RandomWalkEnv(BaseEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_steps: int | None = None,\n",
    "    ):\n",
    "        states = list(range(7))\n",
    "        actions = [0] # there's no action (or equivalently, only 1 action)\n",
    "\n",
    "        super().__init__(\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            terminal_states=set([0, len(states) - 1])\n",
    "        )\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "        self.steps = 0\n",
    "        self.state: int | None = None\n",
    "        self.rg = random_generator()\n",
    "\n",
    "    def reset(self, seed: int | None) -> int:\n",
    "        state = 3\n",
    "        self.steps = 0\n",
    "        self.state = state\n",
    "        self.rg = random_generator(seed)\n",
    "        return state\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        max_steps = self.max_steps\n",
    "        steps = self.steps\n",
    "        state = self.state\n",
    "        assert state is not None\n",
    "        assert steps < max_steps\n",
    "\n",
    "        steps += 1\n",
    "        next_state = state + self.rg.choice([1, -1])\n",
    "        reward = 1 if next_state == self.num_states - 1 else 0\n",
    "        terminated = next_state in self.terminal_states\n",
    "        truncated = (not terminated) and (max_steps is not None) and (steps >= max_steps)\n",
    "\n",
    "        self.steps = steps\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, terminated, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_random_walk(n: int):\n",
    "    env = RandomWalkEnv(max_steps=500)\n",
    "    optimal_v = [0] + [i/6 for i in range(1, 6)] + [0]\n",
    "    episodes = 5000\n",
    "    alpha = 0.01\n",
    "    gamma = 1\n",
    "    seed = 0\n",
    "\n",
    "    diffs_n_step_error, v_n_step_error = evaluate(\n",
    "        env=env,\n",
    "        optimal_v=optimal_v,\n",
    "        policy=lambda state: 0,\n",
    "        n=n,\n",
    "        episodes=episodes,\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        seed=seed,\n",
    "        sum_td_errors=False,\n",
    "    )\n",
    "\n",
    "    diffs_sum_error_td, v_sum_error_td = evaluate(\n",
    "        env=env,\n",
    "        optimal_v=optimal_v,\n",
    "        policy=lambda state: 0,\n",
    "        n=n,\n",
    "        episodes=episodes,\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        seed=seed,\n",
    "        sum_td_errors=True,\n",
    "    )\n",
    "\n",
    "    print(f'[n={n}] v_n_step_error={v_n_step_error}')\n",
    "    print(f'[n={n}] v_sum_error_td={v_sum_error_td}')\n",
    "    print('-' * 80)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(diffs_n_step_error, label='n-step error')\n",
    "    plt.plot(diffs_sum_error_td, label='sum td error')\n",
    "    plt.title(f'Algorithms Comparation (n={n})')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1, 10):\n",
    "    evaluate_random_walk(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the graphs (the closer to 0, the better), for $n = 1$ both cases are equally good (the difference between the predicted values after running all the episodes and the optimal/expected values is the same after each episode, using the same seeds for both, which is expected, as they are basically the same algorithm in this case). For $n \\gt 1$, the default n-step error is better, with the sum of the TD errors to define the n-step error giving a greater variance in the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
