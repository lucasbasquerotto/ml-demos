{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Source: http://web.archive.org/web/20160830014637/https://gym.openai.com/docs/rl#id16\n",
    "\n",
    "### Algorithm 1: Cross Entropy Method\n",
    "\n",
    ">Initialize $\\mu \\in \\mathbb{R}^d, \\sigma \\in \\mathbb{R}^d<br/>\n",
    ">For iteration = 1, 2, ...<br/>\n",
    ">\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Collect $n$ samples of $\\theta_i \\sim N(\\mu, diag(\\sigma))$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Perform a noise evaluation $f(\\theta_i, \\zeta_i)$ on each one<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Select the top $p%$ of samples $(e.g. p = 20)$, which we'll call the \"elite set\"<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new $\\mu$, $\\sigma$.\n",
    ">\n",
    ">Return the final $\\mu$\n",
    "\n",
    "In the RL setting, we evaluate $f(θ_i, ζ_i)$ by executing the policy parameterized by $θ_i$ for one or more episodes, and computing the total return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Exercises\n",
    "\n",
    "### 1. Apply the cross-entropy method to the CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalParams:\n",
    "    def __init__(self, n_samples=100, elite_percent=0.2, iterations=50, noise_factor=0.1, seed: int | None = None):\n",
    "        self.n_samples = n_samples\n",
    "        self.elite_percent = elite_percent\n",
    "        self.iterations = iterations\n",
    "        self.noise_factor = noise_factor\n",
    "        self.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "\n",
    "def cross_entropy_method(dimension: int, evaluator: typing.Callable[[np.ndarray, float], float], params=EvalParams()) -> np.ndarray:\n",
    "    n_samples = params.n_samples\n",
    "    elite_percent = params.elite_percent\n",
    "    iterations = params.iterations\n",
    "    noise_factor = params.noise_factor\n",
    "    seed = params.seed\n",
    "\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Step 1: Initialize μ and σ\n",
    "    mu = np.zeros(dimension)  # Initial mean\n",
    "    sigma = np.ones(dimension)  # Initial standard deviation (diagonal of covariance)\n",
    "\n",
    "    n_elite = int(elite_percent * n_samples)  # Number of elite samples\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # Step 2: Collect n samples of θ_i ∼ N(μ, diag(σ))\n",
    "        samples = np.random.multivariate_normal(mu, np.diag(sigma), n_samples)\n",
    "\n",
    "        # Step 3: Perform a noisy evaluation f(θ_i, ζ_i) on each one\n",
    "        evaluations = np.array([evaluator(theta, noise_factor) for theta in samples])\n",
    "\n",
    "        # Step 4: Select the top p% of samples (elite set)\n",
    "        elite_indices = evaluations.argsort()[-n_elite:]  # Indices of top p% evaluations\n",
    "        elite_samples = samples[elite_indices]\n",
    "\n",
    "        # Step 5: Fit a new Gaussian distribution to the elite set (new μ, σ)\n",
    "        mu = np.mean(elite_samples, axis=0)\n",
    "        sigma = np.std(elite_samples, axis=0)\n",
    "\n",
    "        best_evaluation = np.max(evaluations)\n",
    "        worst_evaluation = np.min(evaluations)\n",
    "        mean_evaluation = np.mean(evaluations)\n",
    "        std_evaluation = np.std(evaluations)\n",
    "\n",
    "        # Print progress\n",
    "        iter_str = f\"Evaluation at iteration {iteration + 1}\"\n",
    "        best_str = f\"Best = {best_evaluation}\"\n",
    "        worst_str = f\"Worst = {worst_evaluation}\"\n",
    "        mean_str = f\"Mean = {mean_evaluation}\"\n",
    "        std_str = f\"Std = {std_evaluation}\"\n",
    "        print(f\"> {iter_str}: {best_str} | {worst_str} | {mean_str} | {std_str}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    evaluations = np.array([evaluator(theta, 0) for theta in samples])\n",
    "\n",
    "    best_str = f\"Best = {np.max(evaluations)}\"\n",
    "    worst_str = f\"Worst = {np.min(evaluations)}\"\n",
    "    mean_str = f\"Mean = {np.mean(evaluations)}\"\n",
    "    std_str = f\"Std = {np.std(evaluations)}\"\n",
    "    print(f\"Final evaluation (no noise): {best_str} | {worst_str} | {mean_str} | {std_str}\")\n",
    "\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Evaluation at iteration 1: Best = -0.7163898685709817 | Worst = -16.630537879032783 | Mean = -4.81642544861259 | Std = 3.0088976229443496\n",
      "> Evaluation at iteration 2: Best = -0.030286620483872893 | Worst = -8.395106551733688 | Mean = -2.695192797417966 | Std = 1.7424096212903402\n",
      "> Evaluation at iteration 3: Best = -0.183146818144696 | Worst = -5.784244448889563 | Mean = -1.8918651297640066 | Std = 1.142651509511421\n",
      "> Evaluation at iteration 4: Best = -0.01041125216644051 | Worst = -5.965552672451809 | Mean = -1.7013528648158394 | Std = 1.1336715866212825\n",
      "> Evaluation at iteration 5: Best = -0.09221563865514357 | Worst = -8.175666769391277 | Mean = -1.4474574674884966 | Std = 1.0886044912391468\n",
      "> Evaluation at iteration 6: Best = -0.14374786467820738 | Worst = -4.537120205517496 | Mean = -1.7124577548664752 | Std = 0.9337543613322696\n",
      "> Evaluation at iteration 7: Best = -0.016785042731554983 | Worst = -5.338396521998412 | Mean = -1.6470026657881511 | Std = 1.0951476764484096\n",
      "> Evaluation at iteration 8: Best = -0.005830718940566665 | Worst = -7.107788810801438 | Mean = -1.4746809746105902 | Std = 0.9725576042265411\n",
      "> Evaluation at iteration 9: Best = -0.028499345314620617 | Worst = -4.909239283421525 | Mean = -1.5547449584837962 | Std = 0.9564813920353793\n",
      "> Evaluation at iteration 10: Best = -0.04298503904160958 | Worst = -5.113196519843032 | Mean = -1.7449757496964906 | Std = 1.0999763296425238\n",
      "> Evaluation at iteration 11: Best = -0.21536731529083744 | Worst = -5.577431201705024 | Mean = -1.636654572953365 | Std = 0.9725406773983448\n",
      "> Evaluation at iteration 12: Best = -0.08287936571724519 | Worst = -6.3147425995506214 | Mean = -2.055902791423682 | Std = 1.1898687733334927\n",
      "> Evaluation at iteration 13: Best = -0.2049854500606691 | Worst = -6.601484243283388 | Mean = -1.9377511446914593 | Std = 1.3088739700771255\n",
      "> Evaluation at iteration 14: Best = 0.1129769556633517 | Worst = -7.505941005097122 | Mean = -1.9247556493472096 | Std = 1.4978426430722187\n",
      "> Evaluation at iteration 15: Best = 0.04455283449485701 | Worst = -4.189536490445539 | Mean = -1.5001320181634064 | Std = 0.9428876979275738\n",
      "> Evaluation at iteration 16: Best = -0.02934981988657967 | Worst = -3.155015525333171 | Mean = -1.181774676523329 | Std = 0.6588003090380705\n",
      "> Evaluation at iteration 17: Best = -0.07766934602399025 | Worst = -4.727517772636598 | Mean = -1.2384892436931907 | Std = 0.8048238825115225\n",
      "> Evaluation at iteration 18: Best = -0.10341331958340379 | Worst = -3.632804517061215 | Mean = -1.2128329543484453 | Std = 0.7553125589886305\n",
      "> Evaluation at iteration 19: Best = -0.07627704399005285 | Worst = -4.945231706287899 | Mean = -1.2100491756424316 | Std = 0.8567356857575457\n",
      "> Evaluation at iteration 20: Best = -0.06740652141808952 | Worst = -3.8107758970040075 | Mean = -1.3662296718952285 | Std = 0.8325958190383071\n",
      "> Evaluation at iteration 21: Best = -0.12950161871378058 | Worst = -3.5780151860538836 | Mean = -1.136211252507572 | Std = 0.6805820133864935\n",
      "> Evaluation at iteration 22: Best = 0.02437046943586363 | Worst = -3.8445143836986078 | Mean = -1.3830955442056074 | Std = 0.7697096458309993\n",
      "> Evaluation at iteration 23: Best = -0.11742042311388924 | Worst = -4.480052193529199 | Mean = -1.5463058382698682 | Std = 0.897858966272291\n",
      "> Evaluation at iteration 24: Best = -0.07172503251813368 | Worst = -7.267764241954888 | Mean = -1.7081315118668652 | Std = 1.1549708896806508\n",
      "> Evaluation at iteration 25: Best = -0.18342967446675368 | Worst = -6.274550254537798 | Mean = -1.6558533118838736 | Std = 1.0536789704738911\n",
      "> Evaluation at iteration 26: Best = -0.10574269962612902 | Worst = -6.465285029765386 | Mean = -1.6190176099179254 | Std = 1.042582981632895\n",
      "> Evaluation at iteration 27: Best = -0.17591431843319383 | Worst = -6.161978915576989 | Mean = -1.6261291497814625 | Std = 1.0361670642718175\n",
      "> Evaluation at iteration 28: Best = -0.006368283705795591 | Worst = -4.607863588334734 | Mean = -1.6880449741781833 | Std = 0.923966808143266\n",
      "> Evaluation at iteration 29: Best = -0.04103606924202596 | Worst = -3.872233823245373 | Mean = -1.6040041468968953 | Std = 1.019348828679352\n",
      "> Evaluation at iteration 30: Best = -0.24615297272076136 | Worst = -4.651834184400813 | Mean = -1.5200722210797972 | Std = 0.9377669729321296\n",
      "> Evaluation at iteration 31: Best = -0.09970576446519347 | Worst = -5.550300925563765 | Mean = -1.5558197756670733 | Std = 1.0732889251475763\n",
      "> Evaluation at iteration 32: Best = 0.019356572837626443 | Worst = -6.018933109289957 | Mean = -1.4704951951625265 | Std = 1.0196394842935297\n",
      "> Evaluation at iteration 33: Best = -0.06556582532995602 | Worst = -4.757053849327699 | Mean = -1.2927455932488439 | Std = 0.9023443463906192\n",
      "> Evaluation at iteration 34: Best = -0.21355903254528363 | Worst = -4.820621371720394 | Mean = -1.4872990414317522 | Std = 0.8921489393498189\n",
      "> Evaluation at iteration 35: Best = -0.02593950862441985 | Worst = -6.206140083438268 | Mean = -1.672793366975474 | Std = 1.0561867481671032\n",
      "> Evaluation at iteration 36: Best = -0.034633155634431256 | Worst = -3.876347292002529 | Mean = -1.47932737765818 | Std = 0.8966038152895757\n",
      "> Evaluation at iteration 37: Best = -0.24025632116978685 | Worst = -4.838196445778964 | Mean = -1.3875901780715023 | Std = 0.7636817191673212\n",
      "> Evaluation at iteration 38: Best = -0.15347249074306613 | Worst = -4.9478209367613815 | Mean = -1.5996687523568829 | Std = 0.9964589859680265\n",
      "> Evaluation at iteration 39: Best = -0.0844254671178735 | Worst = -4.790082811456535 | Mean = -1.6988982849513825 | Std = 0.9344767074533316\n",
      "> Evaluation at iteration 40: Best = -0.16386747915633781 | Worst = -3.703964223894205 | Mean = -1.3379724415944145 | Std = 0.7364705529384554\n",
      "> Evaluation at iteration 41: Best = 0.12963267989460253 | Worst = -4.563302867849263 | Mean = -1.2645882053369397 | Std = 0.8534914759989951\n",
      "> Evaluation at iteration 42: Best = 0.11716541474532469 | Worst = -4.669384293198709 | Mean = -1.1166844548948938 | Std = 0.7705894812790377\n",
      "> Evaluation at iteration 43: Best = -0.1481030650115568 | Worst = -3.7248162547283683 | Mean = -1.2941867165552505 | Std = 0.8593608337070772\n",
      "> Evaluation at iteration 44: Best = -0.08326924205449471 | Worst = -4.8171036355900165 | Mean = -1.4015935683156255 | Std = 0.8282381458041276\n",
      "> Evaluation at iteration 45: Best = -0.12499566070748533 | Worst = -4.237030747056169 | Mean = -1.59075576326375 | Std = 0.8951864272253887\n",
      "> Evaluation at iteration 46: Best = -0.11166338013592508 | Worst = -5.731032065992383 | Mean = -1.8003127384520925 | Std = 1.176207832053087\n",
      "> Evaluation at iteration 47: Best = 0.046802640333874906 | Worst = -5.837023680175564 | Mean = -1.8047413820192648 | Std = 1.1506938749657825\n",
      "> Evaluation at iteration 48: Best = -0.17353262697112742 | Worst = -6.473059210272861 | Mean = -1.6266612152096753 | Std = 1.0738505429448646\n",
      "> Evaluation at iteration 49: Best = -0.10133067350654526 | Worst = -3.98149891193264 | Mean = -1.4203619897211524 | Std = 0.7306949071439541\n",
      "> Evaluation at iteration 50: Best = 0.07007401631993743 | Worst = -5.258014635095309 | Mean = -1.7617489199249394 | Std = 1.1620032500142026\n",
      "Final evaluation (no noise): Best = -0.032838528175699426 | Worst = -5.458116908981243 | Mean = -1.760732509450331 | Std = 1.1518847291306449\n",
      "Final μ: [-0.00671388  0.03218686  0.00390431  0.03454659  0.07452639]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "# Noisy evaluation function (replace this with your real function)\n",
    "def evaluate(theta, noise_factor=0.1):\n",
    "    # Example evaluation: simple quadratic function with noise\n",
    "    noise = np.random.randn() * noise_factor\n",
    "    return -np.sum(theta**2) + noise\n",
    "\n",
    "dimension = 5  # Dimensionality of the problem\n",
    "final_mu = cross_entropy_method(dimension, evaluate, params=EvalParams(seed=42))\n",
    "print(\"Final μ:\", final_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "\n",
    "class EvalParams:\n",
    "    def __init__(self, n_samples=100, elite_percent=0.2, iterations=50, noise_factor=0.1, seed: int | None = None):\n",
    "        self.n_samples = n_samples\n",
    "        self.elite_percent = elite_percent\n",
    "        self.iterations = iterations\n",
    "        self.noise_factor = noise_factor\n",
    "        self.seed = seed\n",
    "\n",
    "def evaluate_cem(\n",
    "    env: Env,\n",
    "    action_selector: typing.Callable[[typing.Any, np.ndarray], typing.Any],\n",
    "    params=EvalParams(),\n",
    "):\n",
    "    def evaluate_episode(theta: np.ndarray, noise_factor: float):\n",
    "        total_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = action_selector(state, theta)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "        # Add noise to the total reward\n",
    "        noise = np.random.randn() * noise_factor\n",
    "        return total_reward * (1 + noise)\n",
    "\n",
    "    dimension = env.observation_space.shape[0]  # Dimensionality of the problem\n",
    "    final_mu = cross_entropy_method(dimension, evaluate_episode, params=params)\n",
    "    print(\"Final μ:\", final_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Evaluation at iteration 1: Best = 496.8660451363414 | Worst = 6.3460463199680985 | Mean = 65.55948112716766 | Std = 103.63333550068732\n",
      "> Evaluation at iteration 2: Best = 578.8726639881737 | Worst = 6.240955234703934 | Mean = 173.73825082919709 | Std = 145.0170946003178\n",
      "> Evaluation at iteration 3: Best = 630.0841557090198 | Worst = 8.879384549994374 | Mean = 287.8641077829541 | Std = 186.80293159114547\n",
      "> Evaluation at iteration 4: Best = 656.8874266829997 | Worst = 8.511593580239381 | Mean = 346.16772556139944 | Std = 192.4093922567234\n",
      "> Evaluation at iteration 5: Best = 618.193624730968 | Worst = 8.294610900972243 | Mean = 383.4159721498082 | Std = 160.6921455712447\n",
      "> Evaluation at iteration 6: Best = 620.5838337789211 | Worst = 8.087745024580222 | Mean = 364.42431274092473 | Std = 187.5510565639899\n",
      "> Evaluation at iteration 7: Best = 613.6217360223442 | Worst = 8.711692382536462 | Mean = 357.88972994547805 | Std = 193.36674079306135\n",
      "> Evaluation at iteration 8: Best = 657.6028367256042 | Worst = 11.116833775529955 | Mean = 406.43962788214253 | Std = 143.9430851015838\n",
      "> Evaluation at iteration 9: Best = 583.8688543902745 | Worst = 7.074306705029383 | Mean = 368.8400453304977 | Std = 167.69928811830817\n",
      "> Evaluation at iteration 10: Best = 655.6455100521921 | Worst = 8.99155126745265 | Mean = 421.88271561328406 | Std = 160.4046829454689\n",
      "> Evaluation at iteration 11: Best = 604.1330161054899 | Worst = 7.621618216280721 | Mean = 408.5345977207284 | Std = 149.97341238091457\n",
      "> Evaluation at iteration 12: Best = 611.5749455513875 | Worst = 7.904336585852743 | Mean = 372.783025045751 | Std = 177.29158798296413\n",
      "> Evaluation at iteration 13: Best = 624.9215241860747 | Worst = 7.74720660526866 | Mean = 424.693761812278 | Std = 150.90293191989093\n",
      "> Evaluation at iteration 14: Best = 633.3452611160004 | Worst = 8.667551652105434 | Mean = 398.0454319301844 | Std = 174.5039569108883\n",
      "> Evaluation at iteration 15: Best = 639.6104433954981 | Worst = 8.620982803779285 | Mean = 428.46272483178626 | Std = 134.0197015608156\n",
      "> Evaluation at iteration 16: Best = 610.4927832874232 | Worst = 9.44083616494079 | Mean = 412.6386369506687 | Std = 147.44398193123413\n",
      "> Evaluation at iteration 17: Best = 610.4802788021188 | Worst = 7.420044970165835 | Mean = 392.1150878046984 | Std = 163.15206870393908\n",
      "> Evaluation at iteration 18: Best = 592.0907804693304 | Worst = 112.71165513236065 | Mean = 450.7210937323591 | Std = 106.87330799375809\n",
      "> Evaluation at iteration 19: Best = 586.702097721131 | Worst = 8.166453062865113 | Mean = 454.73410471196047 | Std = 115.92822467342313\n",
      "> Evaluation at iteration 20: Best = 616.1304612631383 | Worst = 7.452962628341537 | Mean = 460.3245715806886 | Std = 113.51591148997419\n",
      "> Evaluation at iteration 21: Best = 631.619310233875 | Worst = 8.020896289101668 | Mean = 463.8995082400109 | Std = 113.53720371338757\n",
      "> Evaluation at iteration 22: Best = 592.5390571217866 | Worst = 9.934830550421085 | Mean = 445.6483646749983 | Std = 121.6188718652133\n",
      "> Evaluation at iteration 23: Best = 664.3880593694369 | Worst = 9.53374864525472 | Mean = 464.54218901284287 | Std = 94.4233661689191\n",
      "> Evaluation at iteration 24: Best = 627.3027267971396 | Worst = 18.542236464164503 | Mean = 461.5873531191671 | Std = 92.26180058459762\n",
      "> Evaluation at iteration 25: Best = 598.1034180949264 | Worst = 9.470007068296525 | Mean = 471.10989100865254 | Std = 93.37156865360076\n",
      "> Evaluation at iteration 26: Best = 605.8865001504282 | Worst = 278.66179693719755 | Mean = 481.0445949073936 | Std = 64.18438371205804\n",
      "> Evaluation at iteration 27: Best = 659.3287455905528 | Worst = 9.946948008074894 | Mean = 462.5277446729855 | Std = 123.0407496498718\n",
      "> Evaluation at iteration 28: Best = 664.2861856840922 | Worst = 10.674971104709138 | Mean = 474.97340579029463 | Std = 100.69531115641804\n",
      "> Evaluation at iteration 29: Best = 615.8758115598428 | Worst = 8.917841864172004 | Mean = 453.358231725153 | Std = 137.75375317725025\n",
      "> Evaluation at iteration 30: Best = 657.0211455051118 | Worst = 8.875083239149017 | Mean = 485.2789038738091 | Std = 107.0457342938422\n",
      "> Evaluation at iteration 31: Best = 643.7041349004958 | Worst = 19.98126895946147 | Mean = 485.3551313975914 | Std = 95.33858244567075\n",
      "> Evaluation at iteration 32: Best = 603.0170771918416 | Worst = 8.624887513975493 | Mean = 448.4615411461462 | Std = 129.7418789911026\n",
      "> Evaluation at iteration 33: Best = 621.2706295901946 | Worst = 9.902205002915798 | Mean = 444.3676096496585 | Std = 150.98344643250167\n",
      "> Evaluation at iteration 34: Best = 624.1882996293754 | Worst = 10.21465179662449 | Mean = 483.74767454010833 | Std = 84.18925496120497\n",
      "> Evaluation at iteration 35: Best = 654.8347756886027 | Worst = 271.5896345234116 | Mean = 485.8313758879644 | Std = 55.99220453731866\n",
      "> Evaluation at iteration 36: Best = 604.8080638025476 | Worst = 20.630745792254782 | Mean = 487.9968039041916 | Std = 79.61537718517364\n",
      "> Evaluation at iteration 37: Best = 642.3504794478821 | Worst = 13.734762055989705 | Mean = 480.4058724822768 | Std = 100.24355979613556\n",
      "> Evaluation at iteration 38: Best = 584.7681747089968 | Worst = 10.225889347602541 | Mean = 481.6494567623777 | Std = 70.46886167404983\n",
      "> Evaluation at iteration 39: Best = 646.5160142746256 | Worst = 58.03665752715301 | Mean = 491.7850155131333 | Std = 68.45368555992034\n",
      "> Evaluation at iteration 40: Best = 616.9998160966223 | Worst = 222.52300526117494 | Mean = 479.0353287905228 | Std = 71.16894922582904\n",
      "> Evaluation at iteration 41: Best = 616.9411470764122 | Worst = 214.87003934123027 | Mean = 479.88000204659227 | Std = 67.74631579897104\n",
      "> Evaluation at iteration 42: Best = 625.6821734104452 | Worst = 29.609029415264782 | Mean = 484.3414011171287 | Std = 84.82787872738504\n",
      "> Evaluation at iteration 43: Best = 594.3775675127923 | Worst = 21.490902396362834 | Mean = 486.2705510576033 | Std = 86.4630555268244\n",
      "> Evaluation at iteration 44: Best = 610.4814326095432 | Worst = 23.646333984581247 | Mean = 489.35802864124423 | Std = 86.59034470274297\n",
      "> Evaluation at iteration 45: Best = 606.166025664051 | Worst = 12.20680493205447 | Mean = 486.3473728121651 | Std = 73.6219592891435\n",
      "> Evaluation at iteration 46: Best = 613.7927940938432 | Worst = 18.269578799292532 | Mean = 486.4418252307569 | Std = 83.12731324021068\n",
      "> Evaluation at iteration 47: Best = 623.7532319584651 | Worst = 232.89112379225466 | Mean = 497.53115200784026 | Std = 67.9623808627428\n",
      "> Evaluation at iteration 48: Best = 613.578722945228 | Worst = 20.869972166012555 | Mean = 484.7659485520559 | Std = 73.98187490248468\n",
      "> Evaluation at iteration 49: Best = 581.1437992235606 | Worst = 8.253487009656984 | Mean = 477.01289508498274 | Std = 88.79993757307787\n",
      "> Evaluation at iteration 50: Best = 603.8398292664774 | Worst = 36.55091431001214 | Mean = 491.8766627523006 | Std = 71.55885227271489\n",
      "Final evaluation (no noise): Best = 500.0 | Worst = 23.0 | Mean = 491.73 | Std = 51.38868649809994\n",
      "Final μ: [-0.34322547  4.08241826  4.30999154  5.35532248]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def evaluate_cem_cartpole(params=EvalParams()):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    def action_selector(state, theta):\n",
    "        return 0 if np.dot(theta, state) < 0 else 1\n",
    "\n",
    "    evaluate_cem(env=env, action_selector=action_selector, params=params)\n",
    "\n",
    "# Test the function\n",
    "evaluate_cem_cartpole(EvalParams(seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (Practice *) Apply it to the Swimmer environment, which has a continuous action space. Try artificially increasing the variance and gradually lowering this noise to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_windows():\n",
    "    import platform\n",
    "    return platform.system() == \"Windows\"\n",
    "\n",
    "def evaluate_cem_swimmer(params=EvalParams()):\n",
    "    env = gym.make(\"Swimmer-v3\")\n",
    "\n",
    "    def action_selector(state, theta):\n",
    "        return np.clip(np.dot(theta, state), -1, 1)\n",
    "\n",
    "    evaluate_cem(env=env, action_selector=action_selector, params=params)\n",
    "\n",
    "if not is_windows():\n",
    "    # Test the function\n",
    "    evaluate_cem_swimmer(EvalParams(seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Exercises\n",
    "\n",
    "### 1. Implement a policy gradient algorithm and apply it to the CartPole environment. Compare the following variants:\n",
    "\n",
    "$\\quad a.\\text{ }\\widehat{A}_t = R$\n",
    "\n",
    "$\\quad b.\\text{ }\\widehat{A}_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... - V(s_t)$, with and without the discount and baseline (4 variants total)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
