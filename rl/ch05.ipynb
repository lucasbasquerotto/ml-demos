{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 05 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.6\n",
    "\n",
    "**Q**\n",
    "\n",
    "What is the equation analogous to (5.6) for action values Q(s, a) instead of state values V (s), again given returns generated using b?\n",
    "\n",
    "**A**\n",
    "\n",
    "The equation 5.6 is:\n",
    "\n",
    "\\begin{align*}\n",
    "V(s) \\stackrel{.}{=} \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1} G_t}{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1}}\n",
    "\\end{align*}\n",
    "\n",
    "And:\n",
    "\n",
    "\\begin{align*}\n",
    "\\rho_{t:T(t)-1} \\stackrel{.}{=} \\frac{\\prod_{k=t}^{T-1} \\pi(A_k | S_k) p(S_{k+1} | S_k, A_k)}{\\prod_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)} = \\frac{\\prod_{k=t}^{T-1} \\pi(A_k | S_k)}{\\prod_{k=t}^{T-1} b(A_k | S_k)}\n",
    "\\end{align*}\n",
    "\n",
    "For $Q(s, a)$, we have $A_t=a$, so:\n",
    "\n",
    "\\begin{align*}\n",
    "Q(s, a) &\\stackrel{.}{=} \\frac{\\sum_{t \\in \\mathcal{T}(s, a)} \\rho_{t:T(t)-1} G_t}{\\sum_{t \\in \\mathcal{T}(s, a)} \\rho_{t:T(t)-1}} \\\\\n",
    "&= \\frac{\\sum_{t \\in \\mathcal{T}(s, a)} \\rho_{t:t} \\rho_{t+1:T(t)-1} G_t}{\\sum_{t \\in \\mathcal{T}(s, a)} \\rho_{t:t} \\rho_{t+1:T(t)-1}} \\\\\n",
    "&= \\frac{\\sum_{t \\in \\mathcal{T}(s, a)} \\frac{\\pi(A_t | S_t)}{b(A_t | S_t)} \\rho_{t+1:T(t)-1} G_t}{\\sum_{t \\in \\mathcal{T}(s, a)} \\frac{\\pi(A_t | S_t)}{b(A_t | S_t)} \\rho_{t+1:T(t)-1}} \\\\\n",
    "&= \\frac{\\sum_{t \\in \\mathcal{T}(s, a)} \\frac{\\pi(a | s)}{b(a | s)} \\rho_{t+1:T(t)-1} G_t}{\\sum_{t \\in \\mathcal{T}(s, a)} \\frac{\\pi(a | s)}{b(a | s)} \\rho_{t+1:T(t)-1}} \\\\\n",
    "&= \\frac{\\frac{\\pi(a | s)}{b(a | s)} \\sum_{t \\in \\mathcal{T}(s, a)} \\rho_{t+1:T(t)-1} G_t}{\\frac{\\pi(a | s)}{b(a | s)} \\sum_{t \\in \\mathcal{T}(s, a)} \\rho_{t+1:T(t)-1}} \\\\\n",
    "&= \\frac{\\sum_{t \\in \\mathcal{T}(s, a)} \\rho_{t+1:T(t)-1} G_t}{\\sum_{t \\in \\mathcal{T}(s, a)} \\rho_{t+1:T(t)-1}}\n",
    "\\end{align*}\n",
    "\n",
    "In which $\\mathcal{T}(s, a)$ are the occurences (or first occurence in the episode for first-visit Monte Carlo) of the state s and action a across all episodes.\n",
    "\n",
    "Because $A_t=a$, the first term in the product that generate $\\rho_{t:T(t)-1}$ is a constant, and can be isolated outside the sum. \n",
    "\n",
    "In a weighted importance sampling (which is the case above), the constant is present both in the numerator and denominator and cancel each other, that is, the importance-sampling ratio, $\\rho$, can consider only the times from $t+1$ onwards in a weighted importance sampling for $Q(s, a)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.7\n",
    "\n",
    "**Q**\n",
    "\n",
    "In learning curves such as those shown in Figure 5.3 error generally decreases with training, as indeed happened for the ordinary importance-sampling method. But for the weighted importance-sampling method error first increased and then decreased. Why do you think this happened?\n",
    "\n",
    "**A**\n",
    "\n",
    "As defined in the example:\n",
    "\n",
    "- The dealer is showing a deuce.\n",
    "- The sum of the player's card is 13.\n",
    "- The player has a usable ace.\n",
    "- The target policy is to stick only on a sum of 20 or 21.\n",
    "- The behavior policy is to choose to hit or stick randomly.\n",
    "- The error for each amount of episodes is averaged among 100 independent runs.\n",
    "\n",
    "An increased mean squared error means that the calculated value for the state was more distant from the target. In this state, we have an ace, which is expected to show different behaviors depending on whether you reach 20/21 and stop, or exceed, changing the value of the ace from 11 to 1 and trying to reach 20/21 again, which can be seen as a 2nd chance to reach the expected sum.\n",
    "\n",
    "The values are estimated from the behavior policy, scaling the returns by the importance-sampling ratio ($\\rho$), and then dividing by the sum of the ratios (weighted average). Because the target policy only accepts time-steps that ends in 20 or 21 (stopping when reaching any of these states), or exceeds (because the new card value makes the sum exceed 21), all other cases are dismissed (the ratio is 0). Also, the target policy is deterministic ($\\pi(hit|s) = 1$ when $sum < 20$, $\\pi(stick|s) = 1$ when $20 <= sum$) while the behavior policy for $n-1$ hits and 1 stick (ending the episode), or n hits, exceeding 21 and losing, has n time-steps and has a probability of happening equal to $2^{-n}$, because it's a random policy with 2 actions, so $b(a|s) = 0.5$ for any $a \\in \\mathcal{A}(s) = \\{hit, stick\\}$. \n",
    "\n",
    "This means that for a higher amount of hits, the probability of the random policy actually executing those actions reduces exponentially, maybe requiring the completion of a huge number of episodes to simulate one such case.\n",
    "\n",
    "In the given state, the player has an ace, but the number of time-steps to end the episode when 20 or 21 is reached directly is smaller than the number of time-steps if it's exceeded and the ace has its value changed to 1, trying again and reaching 20 or 21, or exceeding 21 and losing. The cases in which the player loses will, on average, have more time-steps (due to more hits). It's possible that many (or even all) among the 100 independent runs of the episodes in the first attempts never reach the case in which the player loses, choosing to stick before (even in cases in which the player has sum less than 20, which will be ignored in the ratio), or exceeds 21 after the player reaches 20 or 21 (in which it would have stopped in the target policy, so not considered in the ratio), making the state have a value higher than it should have.\n",
    "\n",
    "It eventually converges in the expected value as more episodes are completed, and more runs have cases in which the player loses that can happen in the target policy. \n",
    "\n",
    "It's important to note that cases in which the player exceeds 21 will have a much lower probability of happening than cases in which the player sticks to 20/21 using the behavior (random) policy, and because the probability for the target policy will be 1 (in the valid cases in which 20 or 21 was not reached in some previous step), the value of $\\rho$ will be higher, giving more weight to these cases, evetually compensating the lower frequency.\n",
    "\n",
    "*Note:* It's also possible that the behavior policy never reaches 20/21 in the first episodes, which would also influence the target policy value (it would be 0, because no sequence of actions across all runs using the behavior policy are possible under the target policy). Actually, this can happen for any initial states, even those without a usable ace. That said, considering that it's done an average across 100 independent runs, and reaching 20/21 can be done in very few steps, it's more likely to be the first case provided in this answer (there are no case in which the player exceeds 21, but there are cases in which the player stops at 20/21)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.8\n",
    "\n",
    "**Q**\n",
    "\n",
    "The results with Example 5.5 and shown in Figure 5.4 used a first-visit MC method. Suppose that instead an every-visit MC method was used on the same problem. Would the variance of the estimator still be infinite? Why or why not?\n",
    "\n",
    "**A**\n",
    "\n",
    "Yes. The increase in value is exponential, while the every-visit MC would simply average the results of an episode accross the occurences, so the total would still diverges into infinite.\n",
    "\n",
    "Mathematically, while the case for first visits can be defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[X^2] &= \\mathbb{E}_b[(\\rho_{t:T(t)-1} G_0)^2] \\\\\n",
    "&= \\mathbb{E}_b[\\rho_{t:T(t)-1}^2] \\\\\n",
    "&= \\mathbb{E}_b\\left[\\left(\\prod_{t=0}^{T-1} \\frac{\\pi(A_t | S_t)}{b(A_t | S_t)}\\right)^2\\right] \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\left(b(left | s) \\cdot p(s_T | s, left) \\cdot \\left(\\frac{\\pi(left | s)}{b(left | s)}\\right)^2\\right) \\left(b(left | s)^k \\cdot p(s | s, left)^k \\cdot \\left(\\prod_{t=0}^k \\frac{\\pi(left | s)}{b(left | s)}\\right)^2\\right) \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\left(0.5 \\cdot 0.1 \\cdot \\left(\\frac{1}{0.5}\\right)^2\\right) \\cdot \\left(0.5^k \\cdot 0.9^k \\cdot \\left(\\prod_{t=0}^k \\frac{1}{0.5}\\right)^2\\right) \\\\\n",
    "&= \\sum_{k=0}^{\\infty} (0.5 \\cdot 0.1 \\cdot 2^2) \\cdot \\left(0.5^k \\cdot 0.9^k \\cdot \\left(\\prod_{t=0}^k 2^k\\right)^2\\right) \\\\\n",
    "&= \\sum_{k=0}^{\\infty} 0.2 \\cdot \\left(\\left(\\frac{1}{2}\\right)^k \\cdot 0.9^k \\cdot 2^{2k}\\right) \\\\\n",
    "&= 0.2 \\sum_{k=0}^{\\infty} 0.9^k \\cdot \\frac{2^{2k}}{2^k} \\\\\n",
    "&= 0.2 \\sum_{k=0}^{\\infty} 0.9^k \\cdot 2^k \\\\\n",
    "&= 0.2 \\sum_{k=0}^{\\infty} 1.8^k \\\\\n",
    "&= \\infty\n",
    "\\end{align*}\n",
    "\n",
    "The above is calculated considering only the cases in which the left action is chosen (the ratio, $\\rho$, ignores the right action, because the target policy only chooses the left action). So it considers all possibilities, weighed by the probabilities of each occurence (the probability of choosing the action using the behaviour (random) policy, 0.5, and the probability of staying in the same state, 0.9, or ending the episode, 0.1), multiplied by what is expected, $\\rho^2$. \n",
    "\n",
    "The possibilities are: \n",
    "\n",
    "- The first left action ends the episode with probability 0.1, k=0; \n",
    "- The first left action stays in the same state, probability 0.9, then the second left action ends the episode with probability 0.1, k=1; \n",
    "- And so on for k going to $\\infty$, because it can stay in the same state indefinitely when choosing the left action.\n",
    "\n",
    "For an every-visit MC, for a case in which the left action returned back n times, and in the n+1 time-step it ended the episode, instead of considering only the probability of the first occurence, $0.1 \\cdot 0.9^n$, it will consider all cases and calculate the average, $\\frac{\\sum_{i=0}^n 0.1 \\cdot 0.9^i}{n+1}$ (it's n+1 time-steps: n time-steps returning to the same position, and one last time-step ending the episode), so the variance is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[X^2] &= \\mathbb{E}_b[\\rho_{t:T(t)-1}^2] \\\\\n",
    "&= \\mathbb{E}_b\\left[\\left(\\prod_{t=0}^{T-1} \\frac{\\pi(A_t | S_t)}{b(A_t | S_t)}\\right)^2\\right] \\\\\n",
    "&= 0.2 \\sum_{k=0}^{\\infty} \\frac{\\sum_{i=0}^k 1.8^i}{k} \\\\\n",
    "&= 0.2 \\sum_{k=0}^{\\infty} \\frac{1.8^k}{k} + 0.2 \\sum_{k=0}^{\\infty} \\frac{\\sum_{i=0}^{k-1} 1.8^i}{k} \\\\\n",
    "&\\geq 0.2 \\sum_{k=0}^{\\infty} \\frac{1.8^k}{k} \\\\\n",
    "&= \\infty\n",
    "\\end{align*}\n",
    "\n",
    "The above equation is infinite because the first term of the sum is infinite the second is positive, and it ends up in a sum of positive terms in which the highest term goes to infinity:\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname*{lim}_{x \\to \\infty}{0.2 \\sum_{k=0}^x \\frac{1.8^k}{k}} \\geq 0.2 \\operatorname*{lim}_{x \\to \\infty}{\\frac{1.8^x}{x}} = \\infty\n",
    "\\end{align*}\n",
    "\n",
    "In the above equation, the numerator grows exponentially, while the denominator grows linearly, both toward $\\infty$, so the division is $\\infty$. We can apply L'HÃ´pital's rule since both terms grow to infinity:\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname*{lim}_{x \\to \\infty}{\\frac{1.8^x}{x}} &= \\operatorname*{lim}_{x \\to \\infty}{\\frac{\\frac{d(1.8^x)}{dx}}{\\frac{dx}{dx}}} \\\\\n",
    "&= \\operatorname*{lim}_{x \\to \\infty}{\\frac{1.8^x ln(1.8)}{1}} \\\\\n",
    "&= ln(1.8) \\operatorname*{lim}_{x \\to \\infty}{1.8^x} \\\\\n",
    "& = \\infty\n",
    "\\end{align*}\n",
    "\n",
    "So, the variance of the estimator would still be infinite (the first of all the cases in an episode grows to infinity exponentially, averaged by the number of cases that also grow to infinity, but linearly, continues to diverge into infinity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.9\n",
    "\n",
    "**Q**\n",
    "\n",
    "Modify the algorithm for first-visit MC policy evaluation (Section 5.1) to use the incremental implementation for sample averages described in Section 2.4.\n",
    "\n",
    "**A**\n",
    "\n",
    "The original algorithm for first-visit MC policy evaluation is as follows:\n",
    "\n",
    "> Input: a policy $\\pi$ to be evaluated\n",
    ">\n",
    "> Initialize:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$V(s) \\in \\mathbb{R}$, arbitrarily, for all $s \\in \\mathcal{S}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$Returns(s) \\gets$ an empty list, for all $s \\in \\mathcal{S}$\n",
    ">\n",
    ">Loop forever (for each episode):<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Generate an episode following $\\pi$: $S_0$, $A_0$, $R_1$, $S_1$, $A_1$, $R_2$, ..., $S_{T-1}$, $A_{T-1}$, $R_T$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$G \\gets 0$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Loop for each episode, t = T - 1, T - 2, ..., 0:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$G \\gets \\gamma G + R_{t+1}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Unless $S_t$ appears in $S_0$, $S_1$, ..., $S_{t-1}$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Append G to $Returns(S_t)$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(S_t) \\gets average(Returns(S_t))$\n",
    "\n",
    "The incremental implementation follows the form:\n",
    "\n",
    "\\begin{align*}\n",
    "NewEstimate \\gets OldEstimate + StepSize [Target  - OldEstimate] \\tag{2.4}\n",
    "\\end{align*}\n",
    "\n",
    "For example, the estimated action value $Q_n$ considering the average reward among the rewards received previously $R_1$, $R_2$, ..., $R_{n-1}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "Q_n \\stackrel{.}{=} \\frac{R_1 + R_2 + ... + R_{n-1}}{n-1}\n",
    "\\end{align*}\n",
    "\n",
    "The next action value follow the same approach, and can also be defined based only on the current reward and the previous value (instead of keeping track of all rewards):\n",
    "\n",
    "\\begin{align*}\n",
    "Q_{n+1} = \\frac{1}{n} \\sum_{i=1}^n R_i = Q_n + \\frac{1}{n}[R_n - Q_n] \\tag{2.3}\n",
    "\\end{align*}\n",
    "\n",
    "The modified algorithm is:\n",
    "\n",
    "> Input: a policy $\\pi$ to be evaluated\n",
    ">\n",
    "> Initialize:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$V(s) \\in \\mathbb{R}$, arbitrarily, for all $s \\in \\mathcal{S}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$Returns(s) \\gets$ 0, for all $s \\in \\mathcal{S}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$Amount(s) \\gets$ 0, for all $s \\in \\mathcal{S}$\n",
    ">\n",
    ">Loop forever (for each episode):<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Generate an episode following $\\pi$: $S_0$, $A_0$, $R_1$, $S_1$, $A_1$, $R_2$, ..., $S_{T-1}$, $A_{T-1}$, $R_T$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$G \\gets 0$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Loop for each episode, t = T - 1, T - 2, ..., 0:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$G \\gets \\gamma G + R_{t+1}$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Unless $S_t$ appears in $S_0$, $S_1$, ..., $S_{t-1}$:<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$Amount(S_t) \\gets Amount(S_t) + 1$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$Returns(S_t) \\gets Returns(S_t) + \\frac{1}{Amount(S_t)}[G - Returns(S_t)]$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(S_t) \\gets Returns(S_t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.10\n",
    "\n",
    "**Q**\n",
    "\n",
    "Derive the weighted-average update rule (5.8) from (5.7). Follow the pattern of the derivation of the unweighted rule (2.3).\n",
    "\n",
    "**A**\n",
    "\n",
    "The equation 5.7 is:\n",
    "\n",
    "\\begin{align*}\n",
    "V_n \\stackrel{.}{=} \\frac{\\sum_{k=1}^{n-1} W_k G_k}{\\sum_{k=1}^{n-1} W_k}, n \\geq 2\n",
    "\\end{align*}\n",
    "\n",
    "The equation 5.8 is:\n",
    "\n",
    "\\begin{align*}\n",
    "V_{n+1} \\stackrel{.}{=} V_n + \\frac{W_n}{C_n}[G_n - V_n], n \\geq 1\n",
    "\\end{align*}\n",
    "\n",
    "Following the pattern of the derivation of the unweighted rule (2.3), applied to the equation 5.7, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "V_{n+1} &\\stackrel{.}{=} \\frac{\\sum_{k=1}^n W_k G_k}{\\sum_{k=1}^n W_k}, n \\geq 1 \\tag{5.7, for n+1} \\\\\n",
    "&= \\frac{W_n G_n + \\sum_{k=1}^{n-1} W_k G_k}{C_n} \\\\\n",
    "&= \\frac{W_n G_n + \\left(\\frac{\\sum_{k=1}^{n-1} W_k}{\\sum_{k=1}^{n-1} W_k}\\right) \\sum_{k=1}^{n-1} W_k G_k}{C_n} \\\\\n",
    "&= \\frac{W_n G_n + \\left(\\sum_{k=1}^{n-1} W_k\\right) \\frac{\\sum_{k=1}^{n-1} W_k G_k}{\\sum_{k=1}^{n-1} W_k}}{C_n} \\\\\n",
    "&= \\frac{W_n G_n + \\left(\\sum_{k=1}^{n-1} W_k\\right) V_n}{C_n} \\\\\n",
    "&= \\frac{W_n G_n + \\left(\\left(\\sum_{k=1}^{n-1} W_k\\right) + W_n - W_n\\right) V_n}{C_n} \\\\\n",
    "&= \\frac{W_n G_n + \\left(\\left(\\sum_{k=1}^n W_k\\right) - W_n\\right) V_n}{C_n} \\\\\n",
    "&= \\frac{W_n G_n + (C_n - W_n) V_n}{C_n} \\\\\n",
    "&= \\frac{W_n G_n + C_n V_n - W_n V_n}{C_n} \\\\\n",
    "&= V_n + \\frac{W_n}{C_n} [G_n - V_n], n \\geq 1 \\tag{5.8}\n",
    "\\end{align*}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
