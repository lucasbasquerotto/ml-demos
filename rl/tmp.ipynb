{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Source: http://web.archive.org/web/20160830014637/https://gym.openai.com/docs/rl#id16\n",
    "\n",
    "### Algorithm 1: Cross Entropy Method\n",
    "\n",
    ">Initialize $\\mu \\in \\mathbb{R}^d, \\sigma \\in \\mathbb{R}^d<br/>\n",
    ">For iteration = 1, 2, ...<br/>\n",
    ">\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Collect $n$ samples of $\\theta_i \\sim N(\\mu, diag(\\sigma))$<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Perform a noise evaluation $f(\\theta_i, \\zeta_i)$ on each one<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Select the top $p%$ of samples $(e.g. p = 20)$, which we'll call the \"elite set\"<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new $\\mu$, $\\sigma$.\n",
    ">\n",
    ">Return the final $\\mu$\n",
    "\n",
    "In the RL setting, we evaluate $f(θ_i, ζ_i)$ by executing the policy parameterized by $θ_i$ for one or more episodes, and computing the total return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Exercises\n",
    "\n",
    "### 1. Apply the cross-entropy method to the CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalParams:\n",
    "    def __init__(self, n_samples=100, elite_percent=0.2, iterations=50, noise_factor=0.1, seed: int | None = None):\n",
    "        self.n_samples = n_samples\n",
    "        self.elite_percent = elite_percent\n",
    "        self.iterations = iterations\n",
    "        self.noise_factor = noise_factor\n",
    "        self.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "\n",
    "def cross_entropy_method(dimension: int, evaluator: typing.Callable[[np.ndarray, float], float], params=EvalParams()) -> np.ndarray:\n",
    "    n_samples = params.n_samples\n",
    "    elite_percent = params.elite_percent\n",
    "    iterations = params.iterations\n",
    "    noise_factor = params.noise_factor\n",
    "    seed = params.seed\n",
    "\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Step 1: Initialize μ and σ\n",
    "    mu = np.zeros(dimension)  # Initial mean\n",
    "    sigma = np.ones(dimension)  # Initial standard deviation (diagonal of covariance)\n",
    "\n",
    "    n_elite = int(elite_percent * n_samples)  # Number of elite samples\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # Step 2: Collect n samples of θ_i ∼ N(μ, diag(σ))\n",
    "        samples = np.random.multivariate_normal(mu, np.diag(sigma), n_samples)\n",
    "\n",
    "        # Step 3: Perform a noisy evaluation f(θ_i, ζ_i) on each one\n",
    "        evaluations = np.array([evaluator(theta, noise_factor) for theta in samples])\n",
    "\n",
    "        # Step 4: Select the top p% of samples (elite set)\n",
    "        elite_indices = evaluations.argsort()[-n_elite:]  # Indices of top p% evaluations\n",
    "        elite_samples = samples[elite_indices]\n",
    "\n",
    "        # Step 5: Fit a new Gaussian distribution to the elite set (new μ, σ)\n",
    "        mu = np.mean(elite_samples, axis=0)\n",
    "        sigma = np.std(elite_samples, axis=0)\n",
    "\n",
    "        best_evaluation = np.max(evaluations)\n",
    "        worst_evaluation = np.min(evaluations)\n",
    "        mean_evaluation = np.mean(evaluations)\n",
    "        std_evaluation = np.std(evaluations)\n",
    "\n",
    "        # Print progress\n",
    "        iter_str = f\"Evaluation at iteration {iteration + 1}\"\n",
    "        best_str = f\"Best = {best_evaluation}\"\n",
    "        worst_str = f\"Worst = {worst_evaluation}\"\n",
    "        mean_str = f\"Mean = {mean_evaluation}\"\n",
    "        std_str = f\"Std = {std_evaluation}\"\n",
    "        print(f\"> {iter_str}: {best_str} | {worst_str} | {mean_str} | {std_str}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    evaluations = np.array([evaluator(theta, 0) for theta in samples])\n",
    "\n",
    "    best_str = f\"Best = {np.max(evaluations)}\"\n",
    "    worst_str = f\"Worst = {np.min(evaluations)}\"\n",
    "    mean_str = f\"Mean = {np.mean(evaluations)}\"\n",
    "    std_str = f\"Std = {np.std(evaluations)}\"\n",
    "    print(f\"Final evaluation (no noise): {best_str} | {worst_str} | {mean_str} | {std_str}\")\n",
    "\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Evaluation at iteration 1: Best = -0.7163898685709817 | Worst = -16.630537879032783 | Mean = -4.81642544861259 | Std = 3.0088976229443496\n",
      "> Evaluation at iteration 2: Best = -0.030286620483872893 | Worst = -8.395106551733688 | Mean = -2.695192797417966 | Std = 1.7424096212903402\n",
      "> Evaluation at iteration 3: Best = -0.183146818144696 | Worst = -5.784244448889563 | Mean = -1.8918651297640066 | Std = 1.142651509511421\n",
      "> Evaluation at iteration 4: Best = -0.01041125216644051 | Worst = -5.965552672451809 | Mean = -1.7013528648158394 | Std = 1.1336715866212825\n",
      "> Evaluation at iteration 5: Best = -0.09221563865514357 | Worst = -8.175666769391277 | Mean = -1.4474574674884966 | Std = 1.0886044912391468\n",
      "> Evaluation at iteration 6: Best = -0.14374786467820738 | Worst = -4.537120205517496 | Mean = -1.7124577548664752 | Std = 0.9337543613322696\n",
      "> Evaluation at iteration 7: Best = -0.016785042731554983 | Worst = -5.338396521998412 | Mean = -1.6470026657881511 | Std = 1.0951476764484096\n",
      "> Evaluation at iteration 8: Best = -0.005830718940566665 | Worst = -7.107788810801438 | Mean = -1.4746809746105902 | Std = 0.9725576042265411\n",
      "> Evaluation at iteration 9: Best = -0.028499345314620617 | Worst = -4.909239283421525 | Mean = -1.5547449584837962 | Std = 0.9564813920353793\n",
      "> Evaluation at iteration 10: Best = -0.04298503904160958 | Worst = -5.113196519843032 | Mean = -1.7449757496964906 | Std = 1.0999763296425238\n",
      "> Evaluation at iteration 11: Best = -0.21536731529083744 | Worst = -5.577431201705024 | Mean = -1.636654572953365 | Std = 0.9725406773983448\n",
      "> Evaluation at iteration 12: Best = -0.08287936571724519 | Worst = -6.3147425995506214 | Mean = -2.055902791423682 | Std = 1.1898687733334927\n",
      "> Evaluation at iteration 13: Best = -0.2049854500606691 | Worst = -6.601484243283388 | Mean = -1.9377511446914593 | Std = 1.3088739700771255\n",
      "> Evaluation at iteration 14: Best = 0.1129769556633517 | Worst = -7.505941005097122 | Mean = -1.9247556493472096 | Std = 1.4978426430722187\n",
      "> Evaluation at iteration 15: Best = 0.04455283449485701 | Worst = -4.189536490445539 | Mean = -1.5001320181634064 | Std = 0.9428876979275738\n",
      "> Evaluation at iteration 16: Best = -0.02934981988657967 | Worst = -3.155015525333171 | Mean = -1.181774676523329 | Std = 0.6588003090380705\n",
      "> Evaluation at iteration 17: Best = -0.07766934602399025 | Worst = -4.727517772636598 | Mean = -1.2384892436931907 | Std = 0.8048238825115225\n",
      "> Evaluation at iteration 18: Best = -0.10341331958340379 | Worst = -3.632804517061215 | Mean = -1.2128329543484453 | Std = 0.7553125589886305\n",
      "> Evaluation at iteration 19: Best = -0.07627704399005285 | Worst = -4.945231706287899 | Mean = -1.2100491756424316 | Std = 0.8567356857575457\n",
      "> Evaluation at iteration 20: Best = -0.06740652141808952 | Worst = -3.8107758970040075 | Mean = -1.3662296718952285 | Std = 0.8325958190383071\n",
      "> Evaluation at iteration 21: Best = -0.12950161871378058 | Worst = -3.5780151860538836 | Mean = -1.136211252507572 | Std = 0.6805820133864935\n",
      "> Evaluation at iteration 22: Best = 0.02437046943586363 | Worst = -3.8445143836986078 | Mean = -1.3830955442056074 | Std = 0.7697096458309993\n",
      "> Evaluation at iteration 23: Best = -0.11742042311388924 | Worst = -4.480052193529199 | Mean = -1.5463058382698682 | Std = 0.897858966272291\n",
      "> Evaluation at iteration 24: Best = -0.07172503251813368 | Worst = -7.267764241954888 | Mean = -1.7081315118668652 | Std = 1.1549708896806508\n",
      "> Evaluation at iteration 25: Best = -0.18342967446675368 | Worst = -6.274550254537798 | Mean = -1.6558533118838736 | Std = 1.0536789704738911\n",
      "> Evaluation at iteration 26: Best = -0.10574269962612902 | Worst = -6.465285029765386 | Mean = -1.6190176099179254 | Std = 1.042582981632895\n",
      "> Evaluation at iteration 27: Best = -0.17591431843319383 | Worst = -6.161978915576989 | Mean = -1.6261291497814625 | Std = 1.0361670642718175\n",
      "> Evaluation at iteration 28: Best = -0.006368283705795591 | Worst = -4.607863588334734 | Mean = -1.6880449741781833 | Std = 0.923966808143266\n",
      "> Evaluation at iteration 29: Best = -0.04103606924202596 | Worst = -3.872233823245373 | Mean = -1.6040041468968953 | Std = 1.019348828679352\n",
      "> Evaluation at iteration 30: Best = -0.24615297272076136 | Worst = -4.651834184400813 | Mean = -1.5200722210797972 | Std = 0.9377669729321296\n",
      "> Evaluation at iteration 31: Best = -0.09970576446519347 | Worst = -5.550300925563765 | Mean = -1.5558197756670733 | Std = 1.0732889251475763\n",
      "> Evaluation at iteration 32: Best = 0.019356572837626443 | Worst = -6.018933109289957 | Mean = -1.4704951951625265 | Std = 1.0196394842935297\n",
      "> Evaluation at iteration 33: Best = -0.06556582532995602 | Worst = -4.757053849327699 | Mean = -1.2927455932488439 | Std = 0.9023443463906192\n",
      "> Evaluation at iteration 34: Best = -0.21355903254528363 | Worst = -4.820621371720394 | Mean = -1.4872990414317522 | Std = 0.8921489393498189\n",
      "> Evaluation at iteration 35: Best = -0.02593950862441985 | Worst = -6.206140083438268 | Mean = -1.672793366975474 | Std = 1.0561867481671032\n",
      "> Evaluation at iteration 36: Best = -0.034633155634431256 | Worst = -3.876347292002529 | Mean = -1.47932737765818 | Std = 0.8966038152895757\n",
      "> Evaluation at iteration 37: Best = -0.24025632116978685 | Worst = -4.838196445778964 | Mean = -1.3875901780715023 | Std = 0.7636817191673212\n",
      "> Evaluation at iteration 38: Best = -0.15347249074306613 | Worst = -4.9478209367613815 | Mean = -1.5996687523568829 | Std = 0.9964589859680265\n",
      "> Evaluation at iteration 39: Best = -0.0844254671178735 | Worst = -4.790082811456535 | Mean = -1.6988982849513825 | Std = 0.9344767074533316\n",
      "> Evaluation at iteration 40: Best = -0.16386747915633781 | Worst = -3.703964223894205 | Mean = -1.3379724415944145 | Std = 0.7364705529384554\n",
      "> Evaluation at iteration 41: Best = 0.12963267989460253 | Worst = -4.563302867849263 | Mean = -1.2645882053369397 | Std = 0.8534914759989951\n",
      "> Evaluation at iteration 42: Best = 0.11716541474532469 | Worst = -4.669384293198709 | Mean = -1.1166844548948938 | Std = 0.7705894812790377\n",
      "> Evaluation at iteration 43: Best = -0.1481030650115568 | Worst = -3.7248162547283683 | Mean = -1.2941867165552505 | Std = 0.8593608337070772\n",
      "> Evaluation at iteration 44: Best = -0.08326924205449471 | Worst = -4.8171036355900165 | Mean = -1.4015935683156255 | Std = 0.8282381458041276\n",
      "> Evaluation at iteration 45: Best = -0.12499566070748533 | Worst = -4.237030747056169 | Mean = -1.59075576326375 | Std = 0.8951864272253887\n",
      "> Evaluation at iteration 46: Best = -0.11166338013592508 | Worst = -5.731032065992383 | Mean = -1.8003127384520925 | Std = 1.176207832053087\n",
      "> Evaluation at iteration 47: Best = 0.046802640333874906 | Worst = -5.837023680175564 | Mean = -1.8047413820192648 | Std = 1.1506938749657825\n",
      "> Evaluation at iteration 48: Best = -0.17353262697112742 | Worst = -6.473059210272861 | Mean = -1.6266612152096753 | Std = 1.0738505429448646\n",
      "> Evaluation at iteration 49: Best = -0.10133067350654526 | Worst = -3.98149891193264 | Mean = -1.4203619897211524 | Std = 0.7306949071439541\n",
      "> Evaluation at iteration 50: Best = 0.07007401631993743 | Worst = -5.258014635095309 | Mean = -1.7617489199249394 | Std = 1.1620032500142026\n",
      "Final evaluation (no noise): Best = -0.032838528175699426 | Worst = -5.458116908981243 | Mean = -1.760732509450331 | Std = 1.1518847291306449\n",
      "Final μ: [-0.00671388  0.03218686  0.00390431  0.03454659  0.07452639]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "# Noisy evaluation function (replace this with your real function)\n",
    "def evaluate(theta, noise_factor=0.1):\n",
    "    # Example evaluation: simple quadratic function with noise\n",
    "    noise = np.random.randn() * noise_factor\n",
    "    return -np.sum(theta**2) + noise\n",
    "\n",
    "dimension = 5  # Dimensionality of the problem\n",
    "final_mu = cross_entropy_method(dimension, evaluate, params=EvalParams(seed=42))\n",
    "print(\"Final μ:\", final_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "\n",
    "class EvalParams:\n",
    "    def __init__(self, n_samples=100, elite_percent=0.2, iterations=50, noise_factor=0.1, seed: int | None = None):\n",
    "        self.n_samples = n_samples\n",
    "        self.elite_percent = elite_percent\n",
    "        self.iterations = iterations\n",
    "        self.noise_factor = noise_factor\n",
    "        self.seed = seed\n",
    "\n",
    "def evaluate_cem(\n",
    "    env: Env,\n",
    "    action_selector: typing.Callable[[typing.Any, np.ndarray], typing.Any],\n",
    "    params=EvalParams(),\n",
    "):\n",
    "    def evaluate_episode(theta: np.ndarray, noise_factor: float):\n",
    "        total_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = action_selector(state, theta)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "        # Add noise to the total reward\n",
    "        noise = np.random.randn() * noise_factor\n",
    "        return total_reward + noise\n",
    "\n",
    "    dimension = env.observation_space.shape[0]  # Dimensionality of the problem\n",
    "    final_mu = cross_entropy_method(dimension, evaluate_episode, params=params)\n",
    "    print(\"Final μ:\", final_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Evaluation at iteration 1: Best = 499.99373209027266 | Worst = 7.793255789996012 | Mean = 61.86439955294896 | Std = 99.04554906229103\n",
      "> Evaluation at iteration 2: Best = 500.15033983017673 | Worst = 8.027902152577035 | Mean = 187.1227853824887 | Std = 158.3693825217052\n",
      "> Evaluation at iteration 3: Best = 500.26016831141806 | Worst = 8.858163426706696 | Mean = 309.9864407630564 | Std = 168.1395492628534\n",
      "> Evaluation at iteration 4: Best = 500.313774853366 | Worst = 9.065590077652976 | Mean = 335.8186637306612 | Std = 178.89979421950864\n",
      "> Evaluation at iteration 5: Best = 500.2363872494619 | Worst = 7.846889249744428 | Mean = 388.718769490725 | Std = 185.68580809325368\n",
      "> Evaluation at iteration 6: Best = 500.2411676675578 | Worst = 7.886993066095965 | Mean = 389.64420595602815 | Std = 166.70803510157654\n",
      "> Evaluation at iteration 7: Best = 500.2272434720447 | Worst = 7.978654322064957 | Mean = 335.74979201640286 | Std = 194.45285755128398\n",
      "> Evaluation at iteration 8: Best = 500.3152056734512 | Worst = 8.793491695374367 | Mean = 377.4665467852126 | Std = 182.0601252765819\n",
      "> Evaluation at iteration 9: Best = 500.1677377087805 | Worst = 9.01024846421582 | Mean = 367.0641563367678 | Std = 195.0034948538817\n",
      "> Evaluation at iteration 10: Best = 500.31129102010436 | Worst = 9.85279629269732 | Mean = 412.7003898606243 | Std = 152.97606496164954\n",
      "> Evaluation at iteration 11: Best = 500.208266032211 | Worst = 8.949980218290401 | Mean = 403.6425211449492 | Std = 171.0561194535014\n",
      "> Evaluation at iteration 12: Best = 500.22314989110276 | Worst = 9.017146931759038 | Mean = 415.7872701501128 | Std = 154.93673264876438\n",
      "> Evaluation at iteration 13: Best = 500.24984304837216 | Worst = 8.90933514548457 | Mean = 403.43061229048226 | Std = 164.68616189175688\n",
      "> Evaluation at iteration 14: Best = 500.266690522232 | Worst = 9.144656329125137 | Mean = 384.6737789624952 | Std = 178.85110848869644\n",
      "> Evaluation at iteration 15: Best = 500.279220886791 | Worst = 8.876352171340764 | Mean = 438.33150395103144 | Std = 125.98700443201088\n",
      "> Evaluation at iteration 16: Best = 500.24187777793134 | Worst = 10.046769010385889 | Mean = 410.85781532359795 | Std = 145.1520958285898\n",
      "> Evaluation at iteration 17: Best = 500.2214329942557 | Worst = 8.914881951755252 | Mean = 412.67609476816773 | Std = 164.08390572397738\n",
      "> Evaluation at iteration 18: Best = 500.22409265562055 | Worst = 8.90114709921341 | Mean = 445.3372968135596 | Std = 129.0051891045108\n",
      "> Evaluation at iteration 19: Best = 500.2026817116042 | Worst = 9.909267344062773 | Mean = 455.0675001686518 | Std = 116.14879044758781\n",
      "> Evaluation at iteration 20: Best = 500.2322609225263 | Worst = 8.80888406747395 | Mean = 461.6106509878236 | Std = 106.49926362414541\n",
      "> Evaluation at iteration 21: Best = 500.26323862046775 | Worst = 9.90111276523644 | Mean = 458.93043100549767 | Std = 108.02412099506488\n",
      "> Evaluation at iteration 22: Best = 500.22331159171324 | Worst = 10.063524553660395 | Mean = 471.3303269120181 | Std = 98.56016327452824\n",
      "> Evaluation at iteration 23: Best = 500.32877611873886 | Worst = 11.040219782491235 | Mean = 477.2454470819213 | Std = 83.29322348857353\n",
      "> Evaluation at iteration 24: Best = 500.24398744669855 | Worst = 142.0459575488311 | Mean = 469.80892394582565 | Std = 78.95468300823735\n",
      "> Evaluation at iteration 25: Best = 500.2184230601755 | Worst = 10.105705901135657 | Mean = 463.73550606639253 | Std = 117.41278531233925\n",
      "> Evaluation at iteration 26: Best = 500.28425937280105 | Worst = 9.917169746042312 | Mean = 475.74327467882904 | Std = 90.13216376560428\n",
      "> Evaluation at iteration 27: Best = 500.3186574911811 | Worst = 10.020103350371658 | Mean = 466.2202601250433 | Std = 104.80125758684491\n",
      "> Evaluation at iteration 28: Best = 500.3285723713682 | Worst = 10.026151859672582 | Mean = 471.10731021525737 | Std = 88.24702661226426\n",
      "> Evaluation at iteration 29: Best = 500.2317516231197 | Worst = 33.12129839285494 | Mean = 473.49278590348825 | Std = 80.43573796155009\n",
      "> Evaluation at iteration 30: Best = 500.3140422910102 | Worst = 9.924539561760673 | Mean = 461.2338411513225 | Std = 123.4093765615352\n",
      "> Evaluation at iteration 31: Best = 500.287408269801 | Worst = 9.951488998069594 | Mean = 485.1531816060697 | Std = 74.0061233121359\n",
      "> Evaluation at iteration 32: Best = 500.274878241591 | Worst = 9.066510605152148 | Mean = 449.02617308904775 | Std = 136.9401962514613\n",
      "> Evaluation at iteration 33: Best = 500.24254125918037 | Worst = 9.99022050029158 | Mean = 451.78096832139204 | Std = 132.81066728474266\n",
      "> Evaluation at iteration 34: Best = 500.2979854516737 | Worst = 9.960882183636429 | Mean = 452.01422681965175 | Std = 122.38307440206201\n",
      "> Evaluation at iteration 35: Best = 500.3096695513772 | Worst = 9.952946086047058 | Mean = 474.79478677001885 | Std = 85.79425457864299\n",
      "> Evaluation at iteration 36: Best = 500.2096161276051 | Worst = 9.049158650759047 | Mean = 458.2879254868858 | Std = 125.62465885100097\n",
      "> Evaluation at iteration 37: Best = 500.28470095889577 | Worst = 27.109252541467843 | Mean = 477.7747326922833 | Std = 83.2070776645261\n",
      "> Evaluation at iteration 38: Best = 500.16953634941797 | Worst = 10.028945107285066 | Mean = 490.11431530362483 | Std = 58.54226024950325\n",
      "> Evaluation at iteration 39: Best = 500.29303202854925 | Worst = 10.03621843221047 | Mean = 466.77974958291065 | Std = 110.56622800135821\n",
      "> Evaluation at iteration 40: Best = 500.2339996321932 | Worst = 10.029979617157293 | Mean = 488.63094408363725 | Std = 69.32596949536853\n",
      "> Evaluation at iteration 41: Best = 500.2338822941528 | Worst = 22.886488058058877 | Mean = 487.5123861297375 | Std = 55.599568493347846\n",
      "> Evaluation at iteration 42: Best = 500.2513643468209 | Worst = 10.09002056534505 | Mean = 484.0472492204636 | Std = 69.0144930185649\n",
      "> Evaluation at iteration 43: Best = 500.1887551350256 | Worst = 9.895862091147848 | Mean = 468.3770605820087 | Std = 95.11229197141809\n",
      "> Evaluation at iteration 44: Best = 500.22096286521906 | Worst = 230.09974209381244 | Mean = 486.553140325274 | Std = 50.0620377550223\n",
      "> Evaluation at iteration 45: Best = 500.2123320513281 | Worst = 27.11208186488308 | Mean = 478.36221874110686 | Std = 80.98525509375338\n",
      "> Evaluation at iteration 46: Best = 500.2275855881877 | Worst = 8.91172315216611 | Mean = 473.59680596377603 | Std = 95.5214841447008\n",
      "> Evaluation at iteration 47: Best = 500.2475064639169 | Worst = 10.013901318105898 | Mean = 473.74413514773494 | Std = 97.2322065247858\n",
      "> Evaluation at iteration 48: Best = 500.1944048293802 | Worst = 20.179346492487717 | Mean = 488.24628820246687 | Std = 59.317992781175846\n",
      "> Evaluation at iteration 49: Best = 500.1700774787541 | Worst = 16.98811791572709 | Mean = 483.4459845629843 | Std = 75.58708761624041\n",
      "> Evaluation at iteration 50: Best = 500.207679658533 | Worst = 37.79305607407858 | Mean = 474.8523205623738 | Std = 90.70706770563105\n",
      "Final evaluation (no noise): Best = 500.0 | Worst = 174.0 | Mean = 489.03 | Std = 54.093706657983795\n",
      "Final μ: [0.78462122 2.05881789 5.09606312 3.45118194]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def evaluate_cem_cartpole(params=EvalParams()):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    def action_selector(state, theta):\n",
    "        return 0 if np.dot(theta, state) < 0 else 1\n",
    "\n",
    "    evaluate_cem(env=env, action_selector=action_selector, params=params)\n",
    "\n",
    "# Test the function\n",
    "evaluate_cem_cartpole(EvalParams(seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (Practice *) Apply it to the Swimmer environment, which has a continuous action space. Try artificially increasing the variance and gradually lowering this noise to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_windows():\n",
    "    import platform\n",
    "    return platform.system() == \"Windows\"\n",
    "\n",
    "def evaluate_cem_swimmer(params=EvalParams()):\n",
    "    env = gym.make(\"Swimmer-v3\")\n",
    "\n",
    "    def action_selector(state, theta):\n",
    "        return np.clip(np.dot(theta, state), -1, 1)\n",
    "\n",
    "    evaluate_cem(env=env, action_selector=action_selector, params=params)\n",
    "\n",
    "if not is_windows():\n",
    "    # Test the function\n",
    "    evaluate_cem_swimmer(EvalParams(seed=42))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
