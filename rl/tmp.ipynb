{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnv:\n",
    "    def __init__(self, n_states: int, n_actions: int):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self, seed: int = 0) -> int:\n",
    "        self.steps = 0\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, env: BaseEnv):\n",
    "        self.env = env\n",
    "\n",
    "    def act(self, state: int) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, steps: int):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "from numpy.random import MT19937, Generator\n",
    "\n",
    "def random_generator(seed: int | None = None):\n",
    "    bg = MT19937(seed)\n",
    "    rg = Generator(bg)\n",
    "    return rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BranchingEnv(BaseEnv):\n",
    "    def __init__(self, n_states: int, b=1, mean=0, deviation=1, seed: int | None = None):\n",
    "        assert n_states >= b + 1, f\"The number of different states ({n_states}) must be more than b + 1 ({b + 1})\"\n",
    "        assert b >= 1, f\"The branching ({b}) must be 1 or higher\"\n",
    "\n",
    "        actions = [0, 1]\n",
    "        n_actions = len(actions)\n",
    "        rg = random_generator(seed)\n",
    "\n",
    "        super().__init__(\n",
    "            n_states=n_states,\n",
    "            n_actions=n_actions)\n",
    "\n",
    "        self.b = b\n",
    "        self.steps = 0\n",
    "        self.all_steps = 0\n",
    "        self.state: int | None = None\n",
    "        self.rg = rg\n",
    "        self.mean = mean\n",
    "        self.deviation = deviation\n",
    "        self.transitions = [\n",
    "            [\n",
    "                [\n",
    "                    (\n",
    "                        # random move to any spot that is not the same state,\n",
    "                        # and also not the terminal state (self.n_states-1)\n",
    "                        (s + rg.choice(range(1, self.n_states-1))) % (self.n_states-1),\n",
    "                        rg.normal(loc=self.mean, scale=self.deviation),\n",
    "                    )\n",
    "                    for _ in range(self.b)\n",
    "                ]\n",
    "                for _ in range(self.n_actions)\n",
    "            ]\n",
    "            for s in range(self.n_states)\n",
    "        ]\n",
    "\n",
    "    def reset(self, seed: int | None = None) -> int:\n",
    "        state = 0\n",
    "        rg = random_generator(seed)\n",
    "        self.steps = 0\n",
    "        self.state = state\n",
    "        self.rg = rg\n",
    "        return state\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        steps = self.steps + 1\n",
    "        state = self.state\n",
    "\n",
    "        assert state is not None, \"The environment was not initialized\"\n",
    "        assert state != (self.n_states - 1), \"The environment is in a terminal state\"\n",
    "\n",
    "        b_chosen = self.rg.choice(range(self.b))\n",
    "        transition = self.transitions[state][action][b_chosen]\n",
    "        next_state, reward = transition\n",
    "        terminated = self.rg.random() < 0.1\n",
    "        next_state = (self.n_states - 1) if terminated else next_state\n",
    "        truncated = False\n",
    "\n",
    "        self.steps = steps\n",
    "        self.state = next_state\n",
    "        self.all_steps += 1\n",
    "\n",
    "        return next_state, reward, terminated, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class BaseAgentParams:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state: int,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: int,\n",
    "        next_action: int,\n",
    "        terminated: bool,\n",
    "        truncated: bool,\n",
    "    ):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.next_state = next_state\n",
    "        self.next_action = next_action\n",
    "        self.reward = reward\n",
    "        self.terminated = terminated\n",
    "        self.truncated = truncated\n",
    "\n",
    "class BaseDynaAgent(BaseAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: BaseEnv,\n",
    "        n_plan: int | None = None, # None when the planning is over the trajectory or all known state-action pairs\n",
    "        plan_all: bool = False, # n_plan must be None, True for planning over the trajectory, False for all known state-action pairs\n",
    "        alpha: float | None = None, # None for expected updates\n",
    "        gamma: float = 1, # 1 for undiscounted task\n",
    "        epsilon: float = 0.1,\n",
    "        q_learning: bool = True, # True to update based on the maximum value of Q\n",
    "        max_updates: int | None = None,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        super().__init__(env=env)\n",
    "        self.n_plan = n_plan\n",
    "        self.plan_all = plan_all\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_learning = q_learning\n",
    "        self.Q = np.zeros((env.n_states, env.n_actions), dtype=float)\n",
    "        self.TA: dict[tuple[int, int], int] = defaultdict(int) # amount of transitions from (S, A)\n",
    "        self.SP: dict[tuple[int, int], dict[int, int]] = defaultdict(lambda: defaultdict(int)) # amount of times in which (S, A) -> S'\n",
    "        self.T: dict[tuple[int, int, int], int] = defaultdict(int) # amount of times in which (S, A) -> S'\n",
    "        self.M: dict[tuple[int, int], dict[int, float]] = defaultdict(dict) # probability of (S, A) -> S'\n",
    "        self.R: dict[tuple[int, int, int], float] = defaultdict(float) # mean reward of (S, A, S') (based on SP)\n",
    "        self.max_updates = max_updates\n",
    "        self.seed = seed\n",
    "        self.rg = random_generator(seed)\n",
    "        self.updates = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.updates = 0\n",
    "\n",
    "    def initial_state(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def is_terminal(self, state: int) -> bool:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state: int) -> int:\n",
    "        n_actions = self.env.n_actions\n",
    "        epsilon = self.epsilon\n",
    "        qs = self.Q[state]\n",
    "        probs = np.ones(n_actions, dtype=float) * epsilon / n_actions\n",
    "        probs[np.argmax(qs)] += 1 - epsilon\n",
    "        action = self.rg.choice(len(probs), p=probs)\n",
    "        return action\n",
    "\n",
    "    def act_plan(self, state: int) -> int | None:\n",
    "        n_actions = self.env.n_actions\n",
    "        epsilon = self.epsilon\n",
    "        allowed_actions = [a for a in range(n_actions) if self.TA[(state, a)]]\n",
    "\n",
    "        if not allowed_actions:\n",
    "            return None\n",
    "\n",
    "        qs = [self.Q[state, a] for a in allowed_actions]\n",
    "        n_actions = len(qs)\n",
    "        probs = np.ones(n_actions, dtype=float) * epsilon / n_actions\n",
    "        probs[np.argmax(qs)] += 1 - epsilon\n",
    "        action = self.rg.choice(allowed_actions, p=probs)\n",
    "        return action\n",
    "\n",
    "    # called after a real action\n",
    "    def update(self, params: BaseAgentParams) -> None:\n",
    "        self.update_model(params)\n",
    "        self.update_value(params)\n",
    "        self.plan()\n",
    "\n",
    "    # called after a real action\n",
    "    def update_model(self, params: BaseAgentParams):\n",
    "        state = params.state\n",
    "        action = params.action\n",
    "        next_state = params.next_state\n",
    "        reward = params.reward\n",
    "        state_action = (state, action)\n",
    "        san = (state, action, next_state)\n",
    "        self.TA[state_action] += 1\n",
    "        self.T[san] += 1\n",
    "        self.R[san] = ((self.T[san] - 1) * self.R[san] + reward) / self.T[san]\n",
    "        self.SP[state_action][next_state] += 1\n",
    "        for sp in self.SP[state_action]:\n",
    "            self.M[state_action][sp] = self.T[(state, action, sp)] / self.TA[state_action]\n",
    "\n",
    "    def sample_cause(self) -> tuple[int, int]:\n",
    "        keys = [key for key in self.TA if self.TA[key]]\n",
    "        idx = self.rg.choice(len(keys))\n",
    "        state, action = keys[idx]\n",
    "        return state, action\n",
    "\n",
    "    def sample_effect(self, state_action: tuple[int, int]) -> tuple[int, float] | None:\n",
    "        probs_dict = self.M[state_action]\n",
    "        probs_states = [s for s in probs_dict]\n",
    "        probs = [probs_dict[s] for s in probs_dict]\n",
    "        idx = self.rg.choice(len(probs_states), p=probs)\n",
    "        next_state = probs_states[idx]\n",
    "        state, action = state_action\n",
    "        expected_r = self.R[(state, action, next_state)]\n",
    "        return next_state, expected_r\n",
    "\n",
    "    def next_value(self, params: BaseAgentParams, q_learning: bool) -> float:\n",
    "        next_state = params.next_state\n",
    "        next_action = params.next_action\n",
    "        terminated = params.terminated\n",
    "        Q = self.Q\n",
    "\n",
    "        if terminated:\n",
    "            return 0\n",
    "\n",
    "        return max(Q[next_state]) if q_learning else Q[next_state, next_action]\n",
    "\n",
    "    def single_update_value(self) -> None:\n",
    "        self.updates += 1\n",
    "\n",
    "    # called both in real actions and simulated actions\n",
    "    def update_value(self, params: BaseAgentParams) -> float:\n",
    "        state = params.state\n",
    "        action = params.action\n",
    "        reward = params.reward\n",
    "\n",
    "        Q = self.Q\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "\n",
    "        if alpha is not None:\n",
    "            next_value = self.next_value(params=params, q_learning=self.q_learning)\n",
    "            Q[state, action] += alpha * (reward + gamma * next_value - Q[state, action])\n",
    "            self.single_update_value()\n",
    "        else:\n",
    "            # Expected Updates\n",
    "            probs = self.M[(state, action)]\n",
    "            Q[state, action] = 0\n",
    "            for sp in probs:\n",
    "                reward = self.R[(state, action, sp)]\n",
    "                next_value = self.next_value(\n",
    "                    params=BaseAgentParams(\n",
    "                        state=state,\n",
    "                        action=action,\n",
    "                        reward=reward,\n",
    "                        next_state=sp,\n",
    "                        next_action=None,\n",
    "                        terminated=False,\n",
    "                        truncated=False,\n",
    "                    ),\n",
    "                    q_learning=True,\n",
    "                )\n",
    "                Q[state, action] += probs[sp] * (reward + gamma * next_value - Q[state, action])\n",
    "            self.single_update_value()\n",
    "\n",
    "    def plan(self):\n",
    "        # True for planning over all known state-action pairs for every step,\n",
    "        # otherwise it uses the on-police trajectory\n",
    "        plan_all = self.plan_all\n",
    "        n_plan = self.n_plan\n",
    "        idx_plan_all = 0\n",
    "        ta_keys = [key for key in self.TA if self.TA[key] > 0]\n",
    "        terminated = False\n",
    "        count = 0\n",
    "\n",
    "        if plan_all:\n",
    "            s, a = ta_keys[idx_plan_all]\n",
    "        else:\n",
    "            s = self.initial_state()\n",
    "            a = self.act_plan(s)\n",
    "\n",
    "        while True:\n",
    "            if self.max_updates is not None:\n",
    "                if self.updates >= self.max_updates:\n",
    "                    break\n",
    "            sp, r = self.sample_effect((s, a))\n",
    "            ap = self.act_plan(sp)\n",
    "            terminated = self.is_terminal(sp)\n",
    "            truncated = (ap is None)\n",
    "            self.update_value(BaseAgentParams(\n",
    "                state=s,\n",
    "                action=a,\n",
    "                reward=r,\n",
    "                next_state=sp,\n",
    "                next_action=ap,\n",
    "                terminated=terminated,\n",
    "                truncated=False,\n",
    "            ))\n",
    "\n",
    "            if n_plan is not None:\n",
    "                count += 1\n",
    "                if count >= n_plan:\n",
    "                    break\n",
    "\n",
    "            if plan_all:\n",
    "                idx_plan_all += 1\n",
    "                if idx_plan_all >= len(ta_keys):\n",
    "                    if n_plan is not None:\n",
    "                        idx_plan_all = 0\n",
    "                    else:\n",
    "                        break\n",
    "                s, a = ta_keys[idx_plan_all]\n",
    "            elif terminated or truncated:\n",
    "                if n_plan is not None:\n",
    "                    idx_plan_all = 0\n",
    "                    s = self.initial_state()\n",
    "                    a = self.act_plan(s)\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                s = sp\n",
    "                a = ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchingDynaAgent(BaseDynaAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: BranchingEnv,\n",
    "        plan_all: bool = False,\n",
    "        alpha: float | None = None, # None for expected updates\n",
    "        gamma: float = 1, # 1 for undiscounted task\n",
    "        epsilon: float = 0.1,\n",
    "        q_learning: bool = True, # True to update based on the maximum value of Q\n",
    "        max_updates: int | None = None,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            env=env,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            epsilon=epsilon,\n",
    "            q_learning=q_learning,\n",
    "            max_updates=max_updates,\n",
    "            seed=seed)\n",
    "        self.plan_all = plan_all\n",
    "        self.values_history: list[float] = []\n",
    "\n",
    "    def reset(self) -> float:\n",
    "        super().reset()\n",
    "        self.values_history = [self.initial_value()]\n",
    "\n",
    "    def initial_value(self) -> float:\n",
    "        return np.mean(self.Q[0])\n",
    "\n",
    "    def initial_state(self) -> int:\n",
    "        return 0\n",
    "\n",
    "    def is_terminal(self, state: int) -> bool:\n",
    "        return state == self.env.n_states - 1\n",
    "\n",
    "    def single_update_value(self) -> None:\n",
    "        super().single_update_value()\n",
    "        self.values_history.append(self.initial_value())\n",
    "\n",
    "    def train(self) -> list[float]:\n",
    "        self.reset()\n",
    "\n",
    "        rewards = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        state = self.env.reset(self.seed)\n",
    "        action = self.act(state)\n",
    "\n",
    "        while self.updates < self.max_updates:\n",
    "            next_state, r, terminated, truncated = self.env.step(action)\n",
    "            rewards += r\n",
    "            next_action = self.act(next_state)\n",
    "\n",
    "            self.update(BaseAgentParams(\n",
    "                state=state,\n",
    "                action=action,\n",
    "                reward=r,\n",
    "                next_state=next_state,\n",
    "                next_action=next_action,\n",
    "                terminated=terminated,\n",
    "                truncated=truncated,\n",
    "            ))\n",
    "\n",
    "            if terminated or truncated:\n",
    "                state = self.env.reset()\n",
    "                action = self.act(state)\n",
    "            else:\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "        return self.values_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case(n_states: int, plan_all: bool, b: int, max_updates: int, seed: int | None) -> list[float]:\n",
    "    env = BranchingEnv(n_states=n_states, b=b, seed=seed)\n",
    "    agent = BranchingDynaAgent(env=env, plan_all=plan_all, max_updates=max_updates, seed=seed)\n",
    "    q_values = agent.train()\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_branching(title: str, cases: list[tuple[str, typing.Callable[[], list[float]]]]):\n",
    "    print('-' * 80)\n",
    "    print('Last value of start state')\n",
    "    results: list[tuple[str, list[float]]] = []\n",
    "    for name, fn in cases:\n",
    "        q_values = fn()\n",
    "        results.append((name, q_values))\n",
    "        print(f'{name}: {q_values[-1]}')\n",
    "    print('-' * 80)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    for name, q_values in results:\n",
    "        plt.plot(q_values, label=name)\n",
    "\n",
    "    plt.xlabel('Computation time, in expected updates')\n",
    "    plt.ylabel('Value of start state under greedy policy')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "max_updates = 20000\n",
    "n_states = 1000\n",
    "show_branching(title=f'{n_states} states', cases=[\n",
    "    ('on-policy, b=1', lambda: test_case(n_states=n_states, plan_all=False, b=1, max_updates=max_updates, seed=seed)),\n",
    "    ('on-policy, b=3', lambda: test_case(n_states=n_states, plan_all=False, b=3, max_updates=max_updates, seed=seed)),\n",
    "    ('on-policy, b=10', lambda: test_case(n_states=n_states, plan_all=False, b=10, max_updates=max_updates, seed=seed)),\n",
    "\n",
    "    ('uniform, b=1', lambda: test_case(n_states=n_states, plan_all=True, b=1, max_updates=max_updates, seed=seed)),\n",
    "    ('uniform, b=3', lambda: test_case(n_states=n_states, plan_all=True, b=3, max_updates=max_updates, seed=seed)),\n",
    "    ('uniform, b=10', lambda: test_case(n_states=n_states, plan_all=True, b=10, max_updates=max_updates, seed=seed)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "max_updates = 200000\n",
    "n_states = 10000\n",
    "show_branching(title=f'{n_states} states', cases=[\n",
    "    ('on-policy, b=1', lambda: test_case(n_states=n_states, plan_all=False, b=1, max_updates=max_updates, seed=seed)),\n",
    "    ('on-policy, b=3', lambda: test_case(n_states=n_states, plan_all=False, b=3, max_updates=max_updates, seed=seed)),\n",
    "\n",
    "    ('uniform, b=1', lambda: test_case(n_states=n_states, plan_all=True, b=1, max_updates=max_updates, seed=seed)),\n",
    "    ('uniform, b=3', lambda: test_case(n_states=n_states, plan_all=True, b=3, max_updates=max_updates, seed=seed)),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
