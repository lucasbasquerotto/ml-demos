{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.4 (programming) \n",
    "\n",
    "**Q**\n",
    "\n",
    "The exploration bonus described above actually changes the estimated values of states and actions. Is this necessary? Suppose the bonus $\\kappa \\sqrt{\\tau}$ was used not in updates, but solely in action selection. That is, suppose the action selected was always that for which $Q(S_t, a) + \\kappa \\sqrt{\\tau(S_t, a)}$ was maximal. Carry out a gridworld experiment that tests and illustrates the strengths and weaknesses of this alternate approach.\n",
    "\n",
    "**A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnv:\n",
    "    def __init__(self, n_states: int, n_actions: int):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self, seed: int = 0) -> int:\n",
    "        self.steps = 0\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, env: BaseEnv):\n",
    "        self.env = env\n",
    "\n",
    "    def act(self, state: int) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, steps: int):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "NORMAL_COLOR = np.array([1., 1., 1.])\n",
    "START_COLOR = np.array([255, 99, 71]) / 255\n",
    "FINISH_COLOR = np.array([152, 249, 152]) / 255\n",
    "BARRIER_COLOR = np.array([0.5, 0.5, 0.5])\n",
    "AGENT_COLOR = np.array([39, 116, 218]) / 255\n",
    "\n",
    "TYPE_NORMAL = 0\n",
    "TYPE_START = 1\n",
    "TYPE_FINISH = 2\n",
    "TYPE_BARRIER = 3\n",
    "\n",
    "class GridWorldEnv(BaseEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: tuple[int, int],\n",
    "        start: tuple[int, int],\n",
    "        finish: tuple[int, int],\n",
    "        barriers_over_time: list[tuple[int, list[tuple[int, int]]]],\n",
    "        max_steps: int | None = None,\n",
    "    ):\n",
    "        rows, cols = size\n",
    "        start_x, start_y = start\n",
    "        s_start = start_y * cols + start_x\n",
    "        finish_x, finish_y = finish\n",
    "        s_finish = finish_y * cols + finish_x\n",
    "\n",
    "        assert rows > 2 and cols > 2, \"Invalid grid size\"\n",
    "        assert (start_y >= 0 and start_y < rows), f\"Invalid start row: {start_y} (row should be in [0, {rows}))\"\n",
    "        assert (start_x >= 0 and start_x < cols), f\"Invalid start column: {start_x} (column should be in [0, {cols}))\"\n",
    "        assert (finish_y >= 0 and finish_y < rows), f\"Invalid finish row: {finish_y} (row should be in [0, {rows}))\"\n",
    "        assert (finish_x >= 0 and finish_x < cols), f\"Invalid finish column: {finish_x} (column should be in [0, {cols}))\"\n",
    "\n",
    "        for time_step, barriers in barriers_over_time:\n",
    "            for barrier in barriers:\n",
    "                x, y = barrier\n",
    "                assert (y >= 0 and y < rows), f\"Invalid barrier row at time-step {time_step}: {y} (row should be in [0, {rows}))\"\n",
    "                assert (x >= 0 and x < cols), f\"Invalid barrier column at time-step {time_step}: {x} (column should be in [0, {cols}))\"\n",
    "                assert barrier != start, f\"Invalid barrier at time-step {time_step}: should not be in the start cell\"\n",
    "                assert barrier != finish, f\"Invalid barrier at time-step {time_step}: should not be in the finish cell\"\n",
    "\n",
    "        grid = np.full((rows, cols), TYPE_NORMAL)\n",
    "        grid[start_y, start_x] = TYPE_START\n",
    "        grid[finish_y, finish_x] = TYPE_FINISH\n",
    "\n",
    "        actions = [\n",
    "            (0, 1),\n",
    "            (1, 0),\n",
    "            (0, -1),\n",
    "            (-1, 0),\n",
    "        ]\n",
    "\n",
    "        n_states = rows * cols\n",
    "        n_actions = len(actions)\n",
    "\n",
    "        super().__init__(\n",
    "            n_states=n_states,\n",
    "            n_actions=n_actions)\n",
    "\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start = start\n",
    "        self.s_start = s_start\n",
    "        self.finish = finish\n",
    "        self.s_finish = s_finish\n",
    "        self.actions = actions\n",
    "        self.grid = grid\n",
    "        self.max_steps = max_steps\n",
    "        self.seed: int | None = None\n",
    "        self.barriers_over_time = barriers_over_time\n",
    "        self.barrier_time_step = -1\n",
    "        self.barriers: list[tuple[int, int]] = []\n",
    "\n",
    "        self.steps = 0\n",
    "        self.all_steps = 0\n",
    "        self.state: int | None = None\n",
    "\n",
    "        self.update_barriers()\n",
    "\n",
    "    def update_barriers(self):\n",
    "        rows = self.rows\n",
    "        cols = self.cols\n",
    "        all_steps = self.all_steps\n",
    "        barriers_over_time = self.barriers_over_time\n",
    "        grid = self.grid\n",
    "\n",
    "        time_step = 0\n",
    "        barriers: list[tuple[int, int]] = []\n",
    "\n",
    "        for t, b in barriers_over_time:\n",
    "            if t > all_steps:\n",
    "                break\n",
    "\n",
    "            time_step = t\n",
    "            barriers = b\n",
    "\n",
    "        if time_step > self.barrier_time_step:\n",
    "            self.barrier_time_step = time_step\n",
    "            self.barriers = barriers\n",
    "\n",
    "            for row in range(rows):\n",
    "                for col in range(cols):\n",
    "                    if grid[row, col] == TYPE_BARRIER:\n",
    "                        grid[row, col] = TYPE_NORMAL\n",
    "\n",
    "            for x, y in barriers:\n",
    "                grid[y, x] = TYPE_BARRIER\n",
    "\n",
    "    def reset(self, seed: int = 0) -> int:\n",
    "        random.seed(seed)\n",
    "        state = self.s_start\n",
    "        self.steps = 0\n",
    "        self.state = state\n",
    "        return state\n",
    "\n",
    "    def reset_all_steps(self, seed: int = 0) -> int:\n",
    "        state = self.reset(seed=seed)\n",
    "        self.all_steps = 0\n",
    "        self.update_barriers()\n",
    "        return state\n",
    "\n",
    "    def invalid_position(self, row: int, col: int) -> bool:\n",
    "        return row < 0 or row >= self.rows or col < 0 or col >= self.cols or self.grid[row, col] == TYPE_BARRIER\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        steps = self.steps + 1\n",
    "        state = self.state\n",
    "\n",
    "        assert state is not None, \"The environment was not initialized\"\n",
    "        assert state != self.s_finish, \"The environment is in a terminal state\"\n",
    "\n",
    "        row = state // self.cols\n",
    "        col = state % self.cols\n",
    "\n",
    "        action_move = self.actions[action]\n",
    "        move_h, move_v = action_move\n",
    "\n",
    "        new_row = row + move_v\n",
    "        new_col = col + move_h\n",
    "        new_state = state\n",
    "        reward = 0\n",
    "        terminated = False\n",
    "\n",
    "        if not self.invalid_position(row=new_row, col=new_col):\n",
    "            new_state = new_row * self.cols + new_col\n",
    "            terminated = new_state = self.s_finish\n",
    "\n",
    "            if terminated:\n",
    "                reward = 1\n",
    "\n",
    "        truncated = (not terminated) and (self.max_steps is not None) and (self.max_steps <= steps)\n",
    "\n",
    "        self.steps = steps\n",
    "        self.state = new_state\n",
    "        self.all_steps += 1\n",
    "\n",
    "        self.update_barriers()\n",
    "\n",
    "        return new_state, reward, terminated, truncated\n",
    "\n",
    "    def plot(self, title: str | None = None, history: list[int] = []):\n",
    "        rows, cols = self.rows, self.cols\n",
    "\n",
    "        color_grid = np.full((rows, cols, 3), NORMAL_COLOR)\n",
    "\n",
    "        for row in range(rows):\n",
    "            inv_row = rows - row - 1\n",
    "            for col in range(cols):\n",
    "                if self.grid[row, col] == TYPE_START:\n",
    "                    color_grid[inv_row, col] = START_COLOR\n",
    "                elif self.grid[row, col] == TYPE_FINISH:\n",
    "                    color_grid[inv_row, col] = FINISH_COLOR\n",
    "                elif self.grid[row, col] == TYPE_BARRIER:\n",
    "                    color_grid[inv_row, col] = BARRIER_COLOR\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(color_grid, aspect='auto')\n",
    "        plt.grid(which='both', color='#333', linestyle='-', linewidth=1)\n",
    "        plt.xticks(np.arange(.5, self.cols, 1), [])\n",
    "        plt.yticks(np.arange(.5, self.rows, 1), [])\n",
    "        plt.tick_params(bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)\n",
    "\n",
    "        for i, state in enumerate(history):\n",
    "            row = state // self.cols\n",
    "            col = state % self.cols\n",
    "            row = rows - row - 1\n",
    "            # color starts black and ends as AGENT_COLOR\n",
    "            shade = i/len(history)\n",
    "            color = np.array(AGENT_COLOR) * shade\n",
    "            plt.plot(col, row, 'o', color=color, markersize=20)\n",
    "\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def blocking_maze(cls, max_steps: int | None = None):\n",
    "        rows, cols = 6, 9\n",
    "        start = (8, 5)\n",
    "        finish = (3, 0)\n",
    "        barriers_over_time = [\n",
    "            (0, [(x, 2) for x in range(8)]),\n",
    "            (1000, [(x, 2) for x in range(1, 9)]),\n",
    "        ]\n",
    "        return cls(\n",
    "            size=(rows, cols),\n",
    "            start=start,\n",
    "            finish=finish,\n",
    "            barriers_over_time=barriers_over_time,\n",
    "            max_steps=max_steps)\n",
    "\n",
    "    @classmethod\n",
    "    def shortcut_maze(cls, max_steps: int | None = None):\n",
    "        rows, cols = 6, 9\n",
    "        start = (8, 5)\n",
    "        finish = (3, 0)\n",
    "        barriers_over_time = [\n",
    "            (0, [(x, 2) for x in range(1, 9)]),\n",
    "            (3000, [(x, 2) for x in range(1, 8)]),\n",
    "        ]\n",
    "        return cls(\n",
    "            size=(rows, cols),\n",
    "            start=start,\n",
    "            finish=finish,\n",
    "            barriers_over_time=barriers_over_time,\n",
    "            max_steps=max_steps)\n",
    "\n",
    "class GridWorldAgentParams:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state: int,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: int,\n",
    "        terminated: bool,\n",
    "        truncated: bool,\n",
    "    ):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.terminated = terminated\n",
    "        self.truncated = truncated\n",
    "\n",
    "\n",
    "class GridWorldAgent(BaseAgent):\n",
    "    def __init__(self, env: GridWorldEnv, name: str, seed: int | None = None):\n",
    "        super().__init__(env)\n",
    "        self.grid_env = env\n",
    "        self.name = name\n",
    "        self.seed = seed\n",
    "\n",
    "    def train(self, steps: int):\n",
    "        self.train_with_rewards(steps=steps, show=False)\n",
    "\n",
    "    def update(self, params: GridWorldAgentParams) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_with_rewards(self, steps: int, plot=True) -> list[float]:\n",
    "        seed = self.seed\n",
    "\n",
    "        state = self.grid_env.reset_all_steps(seed=seed)\n",
    "        all_steps = 0\n",
    "        rewards = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        history = [state]\n",
    "        last_full_history: tuple[list[int], bool, bool] = []\n",
    "        reward_history: list[float] = [0]\n",
    "\n",
    "        while all_steps < steps:\n",
    "            all_steps += 1\n",
    "            action = self.act(state)\n",
    "            next_state, r, c, t = self.env.step(action)\n",
    "            rewards += r\n",
    "            terminated = c\n",
    "            truncated = t\n",
    "\n",
    "            self.update(GridWorldAgentParams(\n",
    "                state=state,\n",
    "                action=action,\n",
    "                reward=r,\n",
    "                next_state=next_state,\n",
    "                terminated=terminated,\n",
    "                truncated=truncated,\n",
    "            ))\n",
    "\n",
    "            history.append(next_state)\n",
    "            reward_history.append(rewards)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                state = self.env.reset(seed=seed)\n",
    "                last_full_history = [history, terminated, truncated]\n",
    "                history = [state]\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        if not last_full_history:\n",
    "            last_full_history = [history, terminated, truncated]\n",
    "\n",
    "        if plot:\n",
    "            self.grid_env.plot(title=self.name, history=history)\n",
    "\n",
    "        return reward_history\n",
    "\n",
    "class TestAgent(GridWorldAgent):\n",
    "    def __init__(self, env: GridWorldEnv, name: str, seed: int | None = None):\n",
    "        super().__init__(env=env, name=name, seed=seed)\n",
    "\n",
    "    def act(self, state: int) -> int:\n",
    "        return 0 # Always North\n",
    "\n",
    "    def update(self, params: GridWorldAgentParams) -> None:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "env = GridWorldEnv.blocking_maze(max_steps=300)\n",
    "agent = TestAgent(env, name='Blocking Maze (999 steps)')\n",
    "agent.train_with_rewards(steps=999)\n",
    "state = env.reset()\n",
    "action = agent.act(state)\n",
    "env.step(action)\n",
    "env.plot(title='Blocking Maze (1000 steps)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "env = GridWorldEnv.shortcut_maze(max_steps=300)\n",
    "agent = TestAgent(env, name='Shortcut Maze (2999 steps)')\n",
    "agent.train_with_rewards(steps=2999)\n",
    "state = env.reset()\n",
    "action = agent.act(state)\n",
    "env.step(action)\n",
    "env.plot(title='Shortcut Maze (3000 steps)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaAgent(GridWorldAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: GridWorldEnv,\n",
    "        name: str,\n",
    "        n_plan: int = 0,\n",
    "        alpha: float = 0.2,\n",
    "        gamma: float = 0.9,\n",
    "        epsilon: float = 0.1,\n",
    "        kappa: float = 0,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        super().__init__(env=env, name=name, seed=seed)\n",
    "        self.n_plan = n_plan\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.kappa = kappa\n",
    "        self.Q = np.zeros((env.n_states, env.n_actions), dtype=float)\n",
    "        self.M: dict[tuple[int, int], tuple[int, int]] = dict()\n",
    "        self.actions = 0\n",
    "        self.T: dict[tuple[int, int], int] = dict()\n",
    "\n",
    "    def train_with_rewards(self, steps: int, plot=True) -> list[float]:\n",
    "        self.actions = 0\n",
    "        self.tau = dict()\n",
    "        return super().train_with_rewards(steps=steps, plot=plot)\n",
    "\n",
    "    def act(self, state: int) -> int:\n",
    "        n_actions = self.env.n_actions\n",
    "        epsilon = self.epsilon\n",
    "        kappa = self.kappa\n",
    "\n",
    "        if kappa > 0:\n",
    "            action = next((a for a in range(n_actions) if (state, a) not in self.T), None)\n",
    "\n",
    "            if action is None:\n",
    "                action: int = np.argmax(self.actions - self.T[(state, a)] for a in range(n_actions))\n",
    "\n",
    "            self.T[(state, action)] = self.actions\n",
    "            self.actions += 1\n",
    "            return action\n",
    "        else:\n",
    "            qs = self.Q[state]\n",
    "            probs = np.ones(n_actions, dtype=float) * epsilon / n_actions\n",
    "            probs[np.argmax(qs)] += 1 - epsilon\n",
    "            action = np.random.choice(len(probs), p=probs)\n",
    "            return action\n",
    "\n",
    "    def sample(self) -> tuple[int, int]:\n",
    "        M = self.M\n",
    "        keys = list(M.keys())\n",
    "        idx = np.random.choice(len(keys))\n",
    "        state, action = keys[idx]\n",
    "        return state, action\n",
    "\n",
    "    def update(self, params: GridWorldAgentParams) -> None:\n",
    "        state = params.state\n",
    "        action = params.action\n",
    "        reward = params.reward\n",
    "        next_state = params.next_state\n",
    "\n",
    "        Q = self.Q\n",
    "        n_plan = self.n_plan\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "\n",
    "        Q[state, action] += alpha * (reward + gamma * max(Q[next_state]) - Q[state, action])\n",
    "        self.M[(state, action)] = (reward, next_state)\n",
    "\n",
    "        for _ in range(n_plan):\n",
    "            s, a = self.sample()\n",
    "            r, sp = self.M[(s, a)]\n",
    "            Q[s, a] += alpha * (r + gamma * max(Q[sp]) - Q[s, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gridworld(agents: list[DynaAgent]):\n",
    "    rewards: list[tuple[str, list[float]]] = []\n",
    "\n",
    "    for agent in agents:\n",
    "        agent = DynaAgent(\n",
    "            env,\n",
    "            name=agent.name,\n",
    "            n_plan=10,\n",
    "            kappa=0,\n",
    "        )\n",
    "        r = agent.train_with_rewards(steps=6000)\n",
    "        rewards.append((agent.name, r))\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    for name, r in rewards:\n",
    "        plt.plot(r, label=name)\n",
    "\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Cumulative reward')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gridworld(name: str, env: GridWorldEnv):\n",
    "    agents: list[DynaAgent] = [\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name=f'{name} - No Exploration Bonus - n=10',\n",
    "            n_plan=10,\n",
    "            kappa=0,\n",
    "        ),\n",
    "        # DynaAgent(\n",
    "        #     env,\n",
    "        #     name=f'{name} - No Exploration Bonus - n=50',\n",
    "        #     n_plan=50,\n",
    "        #     kappa=0,\n",
    "        # ),\n",
    "        DynaAgent(\n",
    "            env,\n",
    "            name=f'{name} - Exploration 10% - n=10',\n",
    "            n_plan=10,\n",
    "            kappa=0.1,\n",
    "        ),\n",
    "        # DynaAgent(\n",
    "        #     env,\n",
    "        #     name=f'{name} - Exploration 10% - n=50',\n",
    "        #     n_plan=50,\n",
    "        #     kappa=0.1,\n",
    "        # ),\n",
    "        # DynaAgent(\n",
    "        #     env,\n",
    "        #     name=f'{name} - Exploration 20% - n=10',\n",
    "        #     n_plan=10,\n",
    "        #     kappa=0.2,\n",
    "        # ),\n",
    "        # DynaAgent(\n",
    "        #     env,\n",
    "        #     name=f'{name} - Exploration 50% - n=50',\n",
    "        #     n_plan=50,\n",
    "        #     kappa=0.2,\n",
    "        # ),\n",
    "    ]\n",
    "    plot_gridworld(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv.blocking_maze(max_steps=300)\n",
    "test_gridworld(name='Blocking Maze', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv.shortcut_maze(max_steps=300)\n",
    "test_gridworld(name='Shortcut Maze', env=env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
