{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnv:\n",
    "    def __init__(self, n_states: int, n_actions: int):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.state: int | None = None\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self, seed: int = 0) -> int:\n",
    "        self.steps = 0\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, env: BaseEnv):\n",
    "        self.env = env\n",
    "\n",
    "    def act(self, state: int) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, steps: int):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "from numpy.random import MT19937, Generator\n",
    "\n",
    "def random_generator(seed: int | None = None):\n",
    "    bg = MT19937(seed)\n",
    "    rg = Generator(bg)\n",
    "    return rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BranchingEnv(BaseEnv):\n",
    "    def __init__(self, n_states: int, b=1, mean=0, deviation=1, seed: int | None = None, verbose: bool = False):\n",
    "        assert n_states >= b + 1, f\"The number of different states ({n_states}) must be more than b + 1 ({b + 1})\"\n",
    "        assert b >= 1, f\"The branching ({b}) must be 1 or higher\"\n",
    "\n",
    "        actions = [0, 1]\n",
    "        n_actions = len(actions)\n",
    "        rg = random_generator(seed)\n",
    "\n",
    "        super().__init__(\n",
    "            n_states=n_states,\n",
    "            n_actions=n_actions)\n",
    "\n",
    "        self.b = b\n",
    "        self.steps = 0\n",
    "        self.state: int | None = None\n",
    "        self.rg = rg\n",
    "        self.mean = mean\n",
    "        self.deviation = deviation\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.transitions = self.create_transitions()\n",
    "\n",
    "    def create_transitions(self):\n",
    "        return [\n",
    "            [\n",
    "                [\n",
    "                    (\n",
    "                        # random move to any spot that is not the same state,\n",
    "                        # and also not the terminal state (self.n_states-1)\n",
    "                        (s + self.rg.choice(range(1, self.n_states-1))) % (self.n_states-1),\n",
    "                        self.rg.normal(loc=self.mean, scale=self.deviation),\n",
    "                    )\n",
    "                    for _ in range(self.b)\n",
    "                ]\n",
    "                for _ in range(self.n_actions)\n",
    "            ]\n",
    "            for s in range(self.n_states)\n",
    "        ]\n",
    "\n",
    "    def reset(self, seed: int | None = None) -> int:\n",
    "        state = 0\n",
    "        rg = random_generator(seed)\n",
    "        self.steps = 0\n",
    "        self.state = state\n",
    "        self.rg = rg\n",
    "        return state\n",
    "\n",
    "    def step(self, action: int) -> tuple[int, float, bool, bool]:\n",
    "        steps = self.steps + 1\n",
    "        state = self.state\n",
    "\n",
    "        assert state is not None, \"The environment was not initialized\"\n",
    "        assert state != (self.n_states - 1), \"The environment is in a terminal state\"\n",
    "\n",
    "        b_chosen = self.rg.choice(range(self.b))\n",
    "        next_state, reward = self.transitions[state][action][b_chosen]\n",
    "        terminated = self.rg.random() < 0.1 or (next_state == (self.n_states - 1))\n",
    "        next_state = (self.n_states - 1) if terminated else next_state\n",
    "        reward = 0 if terminated else reward\n",
    "        truncated = False\n",
    "\n",
    "        self.steps = steps\n",
    "        self.state = next_state\n",
    "\n",
    "        if self.verbose:\n",
    "            prefix = f'{state} ' if steps == 1 else ''\n",
    "            print(f'{prefix}-> {next_state} ({reward:.2f})', end=' ' if not terminated else '\\n')\n",
    "\n",
    "        return next_state, reward, terminated, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class EnhancedBaseAgentParams:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state: int,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: int,\n",
    "        next_action: int,\n",
    "        terminated: bool,\n",
    "        truncated: bool,\n",
    "    ):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.next_state = next_state\n",
    "        self.next_action = next_action\n",
    "        self.reward = reward\n",
    "        self.terminated = terminated\n",
    "        self.truncated = truncated\n",
    "\n",
    "class EnhancedBaseAgent(BaseAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: BaseEnv,\n",
    "        n_plan: int | None = None, # None when the planning is over the trajectory or all known state-action pairs\n",
    "        plan_all: bool = False, # n_plan must be None; True for planning over the trajectory, False for all known state-action pairs\n",
    "        alpha: float | None = None, # None for expected updates\n",
    "        gamma: float = 1, # 1 for undiscounted task\n",
    "        epsilon: float = 0.1, # exploration rate (epsilon-greedy policy)\n",
    "        q_learning: bool = True, # True to update based on the maximum value of Q\n",
    "        max_updates: int | None = None,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        super().__init__(env=env)\n",
    "        self.n_plan = n_plan\n",
    "        self.plan_all = plan_all\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_learning = q_learning\n",
    "        self.Q = np.zeros((env.n_states, env.n_actions), dtype=float)\n",
    "        self.TA: dict[tuple[int, int], int] = defaultdict(int) # amount of transitions from (S, A)\n",
    "        self.SP: dict[tuple[int, int], dict[int, int]] = defaultdict(lambda: defaultdict(int)) # amount of times in which (S, A) -> S'\n",
    "        self.T: dict[tuple[int, int, int], int] = defaultdict(int) # amount of times in which (S, A) -> S', flattened with key (S, A, S')\n",
    "        self.M: dict[tuple[int, int], dict[int, float]] = defaultdict(dict) # probability of (S, A) -> S'\n",
    "        self.R: dict[tuple[int, int, int], float] = defaultdict(float) # mean reward of (S, A, S') (based on the amount defined in SP)\n",
    "        self.max_updates = max_updates\n",
    "        self.seed = seed\n",
    "        self.rg = random_generator(seed)\n",
    "        self.updates = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.updates = 0\n",
    "\n",
    "    def initial_state(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def is_terminal(self, state: int) -> bool:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state: int) -> int:\n",
    "        n_actions = self.env.n_actions\n",
    "        epsilon = self.epsilon\n",
    "        qs = self.Q[state]\n",
    "        probs = np.ones(n_actions, dtype=float) * epsilon / n_actions\n",
    "        probs[np.argmax(qs)] += 1 - epsilon\n",
    "        action = self.rg.choice(len(probs), p=probs)\n",
    "        return action\n",
    "\n",
    "    def act_plan(self, state: int) -> int | None:\n",
    "        n_actions = self.env.n_actions\n",
    "        epsilon = self.epsilon\n",
    "        allowed_actions = [a for a in range(n_actions) if self.TA[(state, a)]]\n",
    "\n",
    "        if state != self.env.state:\n",
    "            # every state accessed during planning must have already transitioned\n",
    "            # to another state, except the current state in the real run, that may\n",
    "            # not have made any transition yet\n",
    "            assert len(allowed_actions) > 0\n",
    "\n",
    "        if not allowed_actions:\n",
    "            return None\n",
    "\n",
    "        qs = [self.Q[state, a] for a in allowed_actions]\n",
    "        n_actions = len(qs)\n",
    "        probs = np.ones(n_actions, dtype=float) * epsilon / n_actions\n",
    "        probs[np.argmax(qs)] += 1 - epsilon\n",
    "        action = self.rg.choice(allowed_actions, p=probs)\n",
    "        return action\n",
    "\n",
    "    def sample_cause(self) -> tuple[int, int]:\n",
    "        keys = [key for key in self.TA if self.TA[key]]\n",
    "        idx = self.rg.choice(len(keys))\n",
    "        state, action = keys[idx]\n",
    "        return state, action\n",
    "\n",
    "    def sample_effect(self, state_action: tuple[int, int]) -> tuple[int, float] | None:\n",
    "        probs_dict = self.M[state_action]\n",
    "        probs_states = [s for s in probs_dict]\n",
    "        probs = [probs_dict[s] for s in probs_dict]\n",
    "        next_state = self.rg.choice(probs_states, p=probs)\n",
    "        state, action = state_action\n",
    "        expected_r = self.R[(state, action, next_state)]\n",
    "        return next_state, expected_r\n",
    "\n",
    "    def next_value(self, params: EnhancedBaseAgentParams, q_learning: bool) -> float:\n",
    "        next_state = params.next_state\n",
    "        next_action = params.next_action\n",
    "        terminated = params.terminated\n",
    "        Q = self.Q\n",
    "\n",
    "        if terminated:\n",
    "            return 0\n",
    "\n",
    "        if q_learning:\n",
    "            return max(Q[next_state])\n",
    "\n",
    "        assert next_action is not None\n",
    "\n",
    "        return Q[next_state, next_action]\n",
    "\n",
    "    def single_update_value(self) -> None:\n",
    "        self.updates += 1\n",
    "\n",
    "    # called after a real action\n",
    "    def update(self, params: EnhancedBaseAgentParams) -> None:\n",
    "        self.update_model(params)\n",
    "        self.update_value(params)\n",
    "        self.plan()\n",
    "\n",
    "    # called after a real action\n",
    "    def update_model(self, params: EnhancedBaseAgentParams):\n",
    "        state = params.state\n",
    "        action = params.action\n",
    "        next_state = params.next_state\n",
    "        reward = params.reward\n",
    "        state_action = (state, action)\n",
    "        san = (state, action, next_state)\n",
    "        self.TA[state_action] += 1\n",
    "        self.T[san] += 1\n",
    "        self.R[san] = ((self.T[san] - 1) * self.R[san] + reward) / self.T[san]\n",
    "        self.SP[state_action][next_state] += 1\n",
    "        for sp in self.SP[state_action]:\n",
    "            self.M[state_action][sp] = self.T[(state, action, sp)] / self.TA[state_action]\n",
    "\n",
    "    # called both in real actions and simulated actions\n",
    "    def update_value(self, params: EnhancedBaseAgentParams) -> float:\n",
    "        state = params.state\n",
    "        action = params.action\n",
    "        reward = params.reward\n",
    "\n",
    "        Q = self.Q\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "\n",
    "        if alpha is not None:\n",
    "            next_value = self.next_value(params=params, q_learning=self.q_learning)\n",
    "            Q[state, action] += alpha * (reward + gamma * next_value - Q[state, action])\n",
    "        else:\n",
    "            # Expected Updates\n",
    "            probs = self.M[(state, action)]\n",
    "\n",
    "            before_q = Q[state, action]\n",
    "\n",
    "            Q[state, action] = 0\n",
    "            info = []\n",
    "            for sp in probs:\n",
    "                reward = self.R[(state, action, sp)]\n",
    "                terminated = self.is_terminal(sp)\n",
    "                next_value = self.next_value(\n",
    "                    params=EnhancedBaseAgentParams(\n",
    "                        state=state,\n",
    "                        action=action,\n",
    "                        reward=reward,\n",
    "                        next_state=sp,\n",
    "                        next_action=None,\n",
    "                        terminated=terminated,\n",
    "                        truncated=False,\n",
    "                    ),\n",
    "                    q_learning=True,\n",
    "                )\n",
    "                info += [(sp, probs[sp], reward, next_value, Q[state, action], Q[sp])]\n",
    "                Q[state, action] += probs[sp] * (reward + gamma * next_value - Q[state, action])\n",
    "\n",
    "            if Q[state, action] - before_q > 20 or Q[state, action] > 100:\n",
    "                print()\n",
    "                print('---')\n",
    "                print(f'> Q={Q[state, action]:.2f} from {before_q:.2f} (Updates: {self.updates}) - state={state}, action={action}')\n",
    "                print('---')\n",
    "                for i in info:\n",
    "                    print('>> ', i)\n",
    "                raise Exception('stop')\n",
    "\n",
    "        self.single_update_value()\n",
    "\n",
    "    def plan(self):\n",
    "        # True for planning over all known state-action pairs for every step,\n",
    "        # otherwise it uses the on-police trajectory\n",
    "        plan_all = self.plan_all\n",
    "        n_plan = self.n_plan\n",
    "        idx_plan_all = 0\n",
    "        ta_keys = [key for key in self.TA if self.TA[key] > 0]\n",
    "        terminated = False\n",
    "        count = 0\n",
    "\n",
    "        if plan_all:\n",
    "            s, a = ta_keys[idx_plan_all]\n",
    "        else:\n",
    "            s = self.initial_state()\n",
    "            a = self.act_plan(s)\n",
    "\n",
    "        # store the states already planned when plan_all is False, to avoid infinite loops\n",
    "        planned_states = set([s])\n",
    "\n",
    "        while True:\n",
    "            if self.max_updates is not None:\n",
    "                if self.updates >= self.max_updates:\n",
    "                    break\n",
    "            sp, r = self.sample_effect((s, a))\n",
    "            terminated = self.is_terminal(sp)\n",
    "            ap = None if terminated else self.act_plan(sp)\n",
    "            truncated = (ap is None) and not terminated\n",
    "\n",
    "            self.update_value(EnhancedBaseAgentParams(\n",
    "                state=s,\n",
    "                action=a,\n",
    "                reward=r,\n",
    "                next_state=sp,\n",
    "                next_action=ap,\n",
    "                terminated=terminated,\n",
    "                truncated=truncated,\n",
    "            ))\n",
    "\n",
    "            if n_plan is not None:\n",
    "                count += 1\n",
    "                if count >= n_plan:\n",
    "                    break\n",
    "\n",
    "            if plan_all:\n",
    "                # iterate over all pairs (state, action)\n",
    "                idx_plan_all += 1\n",
    "                if idx_plan_all >= len(ta_keys):\n",
    "                    if n_plan is not None:\n",
    "                        idx_plan_all = 0\n",
    "                    else:\n",
    "                        break\n",
    "                s, a = ta_keys[idx_plan_all]\n",
    "            elif terminated or truncated:\n",
    "                if n_plan is not None:\n",
    "                    # only if episode anded and have an explicit number of simulations per step\n",
    "                    s = self.initial_state()\n",
    "                    a = self.act_plan(s)\n",
    "                    planned_states.clear()\n",
    "                else:\n",
    "                    break\n",
    "            elif sp not in planned_states:\n",
    "                # continue trajectory, avoiding cycles\n",
    "                planned_states.add(sp)\n",
    "                s = sp\n",
    "                a = ap\n",
    "            elif n_plan is not None:\n",
    "                # only if have an explicit number of simulations per step\n",
    "                s = self.initial_state()\n",
    "                a = self.act_plan(s)\n",
    "                planned_states.clear()\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchingDynaAgent(EnhancedBaseAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: BranchingEnv,\n",
    "        plan_all: bool = False,\n",
    "        alpha: float | None = None, # None for expected updates\n",
    "        gamma: float = 1, # 1 for undiscounted task\n",
    "        epsilon: float = 0.1,\n",
    "        q_learning: bool = True, # True to update based on the maximum value of Q\n",
    "        max_updates: int | None = None,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            env=env,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            epsilon=epsilon,\n",
    "            q_learning=q_learning,\n",
    "            max_updates=max_updates,\n",
    "            seed=seed)\n",
    "        self.plan_all = plan_all\n",
    "        self.values_history: list[float] = []\n",
    "\n",
    "    def reset(self) -> float:\n",
    "        super().reset()\n",
    "        self.values_history = [self.initial_value()]\n",
    "\n",
    "    def initial_state(self) -> int:\n",
    "        return 0\n",
    "\n",
    "    def initial_value(self) -> float:\n",
    "        n_actions = self.env.n_actions\n",
    "        epsilon = self.epsilon\n",
    "        qs = self.Q[self.initial_state()]\n",
    "        probs = np.ones(n_actions, dtype=float) * epsilon / n_actions\n",
    "        probs[np.argmax(qs)] += 1 - epsilon\n",
    "        v_s0 = sum([p*q for p, q in zip(probs, qs)])\n",
    "        return v_s0\n",
    "\n",
    "    def is_terminal(self, state: int) -> bool:\n",
    "        return state == self.env.n_states - 1\n",
    "\n",
    "    def single_update_value(self) -> None:\n",
    "        super().single_update_value()\n",
    "        self.values_history.append(self.initial_value())\n",
    "\n",
    "    def train(self, verbose=False) -> list[float]:\n",
    "        self.reset()\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        max_seed = self.env.n_states * 100\n",
    "        # the reset seed is to make sure that a sequence of episodes\n",
    "        # don't terminate (with 10% chance) after the exact same steps,\n",
    "        # if the the same actions are performed over several episodes\n",
    "        # (if the same seed was used for different resets, a sequency\n",
    "        # of the same actions would produce the same results, because\n",
    "        # the chosen branches would be the same, and the termination\n",
    "        # would happen after the exact same steps, which could cause\n",
    "        # some state-action values end up with extremely large values)\n",
    "        reset_seed = self.rg.integers(0, max_seed)\n",
    "        state = self.env.reset(reset_seed)\n",
    "        action = self.act(state)\n",
    "        episode = 0\n",
    "        steps = 0\n",
    "        rewards = 0\n",
    "\n",
    "        while self.updates < self.max_updates:\n",
    "            next_state, r, terminated, truncated = self.env.step(action)\n",
    "            steps += 1\n",
    "            rewards += r\n",
    "            next_action = self.act(next_state)\n",
    "\n",
    "            self.update(EnhancedBaseAgentParams(\n",
    "                state=state,\n",
    "                action=action,\n",
    "                reward=r,\n",
    "                next_state=next_state,\n",
    "                next_action=next_action,\n",
    "                terminated=terminated,\n",
    "                truncated=truncated,\n",
    "            ))\n",
    "\n",
    "            if terminated or truncated:\n",
    "                episode += 1\n",
    "                if verbose:\n",
    "                    print(f'[{episode}] [Q0={self.initial_value():.2f}] Steps: {steps} - Cumulative Reward: {rewards} - Updates: {self.updates}')\n",
    "                steps = 0\n",
    "                rewards = 0\n",
    "\n",
    "                reset_seed = self.rg.integers(0, max_seed)\n",
    "                state = self.env.reset(reset_seed)\n",
    "                action = self.act(state)\n",
    "            else:\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "        return self.values_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case(n_states: int, plan_all: bool, b: int, max_updates: int, seed: int | None, verbose: bool = False) -> list[float]:\n",
    "    if verbose or True:\n",
    "        print('seed', seed)\n",
    "    env = BranchingEnv(n_states=n_states, b=b, seed=seed, verbose=verbose)\n",
    "    agent = BranchingDynaAgent(env=env, plan_all=plan_all, max_updates=max_updates, seed=seed)\n",
    "    values_history = agent.train(verbose=verbose)\n",
    "    if verbose:\n",
    "        print()\n",
    "        print('-' * 80)\n",
    "        print(agent.Q[:20])\n",
    "        print('...')\n",
    "        print(agent.Q[-20:])\n",
    "    return values_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_branching(title: str, tasks: int, cases: list[tuple[str, typing.Callable[[int], list[float]]]], seed: int | None = None):\n",
    "    print('=' * 80)\n",
    "    print('Last value of start state')\n",
    "    results: list[tuple[str, list[float]]] = []\n",
    "    for i, (name, fn) in enumerate(cases):\n",
    "        rg = random_generator(seed)\n",
    "        if i > 0:\n",
    "            print('-' * 80)\n",
    "        avg_q: list[float] | None = None\n",
    "        for task in range(tasks):\n",
    "            task_seed = rg.integers(low=0, high=100*tasks)\n",
    "            q_values = fn(task_seed)\n",
    "            print(f'[task: {task+1}] {name}: {q_values[-1]}')\n",
    "            avg_q = [\n",
    "                (avg_q[i] if avg_q is not None else 0) + (q_values[i]/tasks)\n",
    "                for i in range(len(q_values))\n",
    "            ]\n",
    "        assert avg_q is not None\n",
    "        results.append((name, avg_q))\n",
    "        print(f'[avg] {name}: {avg_q[-1]}')\n",
    "    print('=' * 80)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    for name, q_values in results:\n",
    "        plt.plot(q_values, label=name)\n",
    "\n",
    "    plt.xlabel('Computation time, in expected updates')\n",
    "    plt.ylabel('Value of start state under greedy policy')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 6603\n",
    "seed = 7183\n",
    "_ = test_case(n_states=1000, plan_all=False, b=1, max_updates=20000, seed=seed, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 302\n",
    "_ = test_case(n_states=1000, plan_all=True, b=1, max_updates=20000, seed=seed, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "max_updates = 20000\n",
    "n_states = 1000\n",
    "tasks = 20\n",
    "show_branching(\n",
    "    title=f'{n_states} states',\n",
    "    tasks=tasks,\n",
    "    seed=seed,\n",
    "    cases=[\n",
    "        ('on-policy, b=1', lambda task_seed: test_case(n_states=n_states, plan_all=False, b=1, max_updates=max_updates, seed=task_seed)),\n",
    "        # ('on-policy, b=3', lambda task_seed: test_case(n_states=n_states, plan_all=False, b=3, max_updates=max_updates, seed=task_seed)),\n",
    "        # ('on-policy, b=10', lambda task_seed: test_case(n_states=n_states, plan_all=False, b=10, max_updates=max_updates, seed=task_seed)),\n",
    "\n",
    "        ('uniform, b=1', lambda task_seed: test_case(n_states=n_states, plan_all=True, b=1, max_updates=max_updates, seed=task_seed)),\n",
    "        # ('uniform, b=3', lambda task_seed: test_case(n_states=n_states, plan_all=True, b=3, max_updates=max_updates, seed=task_seed)),\n",
    "        # ('uniform, b=10', lambda task_seed: test_case(n_states=n_states, plan_all=True, b=10, max_updates=max_updates, seed=task_seed)),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "max_updates = 200000\n",
    "n_states = 10000\n",
    "tasks = 1\n",
    "show_branching(\n",
    "    title=f'{n_states} states',\n",
    "    tasks=tasks,\n",
    "    seed=seed,\n",
    "    cases=[\n",
    "        ('on-policy, b=1', lambda task_seed: test_case(n_states=n_states, plan_all=False, b=1, max_updates=max_updates, seed=task_seed)),\n",
    "        ('on-policy, b=3', lambda task_seed: test_case(n_states=n_states, plan_all=False, b=3, max_updates=max_updates, seed=task_seed)),\n",
    "\n",
    "        ('uniform, b=1', lambda task_seed: test_case(n_states=n_states, plan_all=True, b=1, max_updates=max_updates, seed=task_seed)),\n",
    "        ('uniform, b=3', lambda task_seed: test_case(n_states=n_states, plan_all=True, b=3, max_updates=max_updates, seed=task_seed)),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
