{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "This section has presented options for the discounted case, but discounting is arguably inappropriate for control when using function approximation (Section 10.4). What is the natural Bellman equation for a hierarchical policy, analogous to (17.4), but for the average reward setting (Section 10.3)? What are the two parts of the option model, analogous to (17.2) and (17.3), for the average reward setting?\n",
    "\n",
    "**A**\n",
    "\n",
    "Equation 17.2 is:\n",
    "\n",
    "$$\n",
    "r(s, \\omega) \\doteq \\mathbb{E}[R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... + \\gamma^{\\tau - 1} R_{\\tau} | S_0 = s, A_{0:\\tau - 1} \\sim \\pi_{\\omega}, \\tau \\sim \\gamma_{\\omega}] \\tag{17.2}\n",
    "$$\n",
    "\n",
    "Equation 17.3 is:\n",
    "\n",
    "$$\n",
    "p(s' | s, \\omega) \\doteq \\sum_{k=1}^{\\infty} \\gamma^k Pr\\{ S_k = s', \\tau = k | S_0 = s, A_{0:k-1} \\sim \\pi_{\\omega}, \\tau \\sim \\gamma_{\\omega} \\} \\tag{17.3}\n",
    "$$\n",
    "\n",
    "Equation 17.4 is:\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_{\\omega \\in \\Omega(s)} \\pi(\\omega | s) \\left[ r(s, \\omega) + \\sum_{s'} p(s' | s, \\omega) v_{\\pi}(s') \\right] \\tag{17.4}\n",
    "$$\n",
    "\n",
    "The natural Bellman equation for a hierarchical policy, analogous to (17.4), but for the average reward setting (Section 10.3) is:\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_{\\omega \\in \\Omega(s)} \\pi(\\omega | s) \\left[ r(s, \\omega) - r(\\pi) + \\sum_{s'} p(s' | s, \\omega) v_{\\pi}(s') \\right]\n",
    "$$\n",
    "\n",
    "where $r(\\pi)$ is the average reward while following the policy $\\pi$.\n",
    "\n",
    "The part of the option model, analogous to (17.2), and based on (10.9), is:\n",
    "\n",
    "$$\n",
    "r(s, \\omega) \\doteq \\mathbb{E}[R_1 - r(\\pi) + R_2 - r(\\pi) + R_3 - r(\\pi) + ... + R_{\\tau} - r(\\pi) | S_0 = s, A_{0:\\tau - 1} \\sim \\pi_{\\omega}, \\tau \\sim \\gamma_{\\omega}]\n",
    "$$\n",
    "\n",
    "or, alternatively:\n",
    "\n",
    "$$\n",
    "r(s, \\omega) \\doteq \\mathbb{E}\\left[\\sum_{k=1}^{\\tau} (R_k - r(\\pi)) | S_0 = s, A_{0:\\tau - 1} \\sim \\pi_{\\omega}, \\tau \\sim \\gamma_{\\omega}\\right]\n",
    "$$\n",
    "\n",
    "The part of the option model, analogous to (17.3), is the actual transition probability of the environment (the dynamics of the environment regarding the state transitions does not depend on the average reward, only on the environment itself, the policy $\\pi_{\\omega}$ that chooses the action, and $\\gamma_{\\omega}$ that determines when the sequence of steps ends and defines $\\tau$), that is:\n",
    "\n",
    "$$\n",
    "p(s' | s, \\omega) \\doteq \\sum_{k=1}^{\\infty} Pr\\{ S_k = s', \\tau = k | S_0 = s, A_{0:k-1} \\sim \\pi_{\\omega}, \\tau \\sim \\gamma_{\\omega} \\}\n",
    "$$\n",
    "\n",
    "which is the probability of state $s$ reach state $s'$ after any number of time steps (the probability of reaching any state after any amount of time steps is 1, so $\\sum_{k=1}^{\\infty} Pr\\{ \\tau = k | S_0 = s, A_{0:k-1} \\sim \\pi_{\\omega}, \\tau \\sim \\gamma_{\\omega} \\} = 1$ and, consequently, $p(s' | s, \\omega) \\leq 1$; it will be 1 if state $s$ can only transition to state $s'$ under the policy $\\pi_{\\omega}$)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
