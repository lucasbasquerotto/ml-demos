{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "| s              | a              | | s'             | r              | | p(s', r\\|s, a) |\n",
    "| -------------- | -------------- |-| -------------- | -------------- |-| -------------- |\n",
    "| high           | search         | | high           | $r_{search}$   | | $\\alpha$       |\n",
    "| high           | search         | | low            | $r_{search}$   | | $1 - \\alpha$   |\n",
    "| low            | search         | | high           | -3             | | $1 - \\beta$    |\n",
    "| low            | search         | | low            | $r_{search}$   | | $\\beta$        |\n",
    "| high           | wait           | | high           | $r_{wait}$     | | 1              |\n",
    "| low            | wait           | | low            | $r_{wait}$     | | 1              |\n",
    "| low            | recharge       | | high           | 0              | | 1              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5\n",
    "\n",
    "Given: \n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1 \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "\\end{align*}\n",
    "\n",
    "In episodic tasks, $\\mathcal{S}$ represents the set of all states *minus* the terminal state. \n",
    "\n",
    "To make the equation apply to episodic tasks, the terminal state have to be considered as a possible state of s'.\n",
    "\n",
    "The modified version of the equation is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s' \\in \\mathcal{S^+}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1 \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "\\end{align*}\n",
    "\n",
    "The only change was from $\\mathcal{S}$ to $\\mathcal{S^+}$ for the possible values of s', but the possible values of s must still be in $\\mathcal{S}$ (and not in $\\mathcal{S^+}$), because a terminal state can't have a next state (s')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.7\n",
    "\n",
    "The reward will be the same no matter if the robot escape the maze in 10, 100, or any number of steps. The robot should receive a greater reward if it reaches in less time steps (or penalized if it reaches in more time steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.8\n",
    "\n",
    " **Q**\n",
    " \n",
    " Suppose $\\gamma = 0.5$ and the following sequence of rewards is received $R_1 = -1$, $R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0$, $G_1$, ..., $G_5$? Hint: Work backwards.\n",
    "\n",
    " **A**\n",
    "\n",
    " The return at time step t is:\n",
    " \n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "\n",
    "Considering $g = k - 1$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t &= R_{t+1} + \\sum_{k=1}^{\\infty} \\gamma^k R_{t+k+1} \\\\\n",
    "&= R_{t+1} + \\sum_{g=0}^{\\infty} \\gamma^{g+1} R_{t+[g+1]+1} \\\\\n",
    "&= R_{t+1} + \\gamma \\sum_{g=0}^{\\infty} \\gamma^g R_{[t+1]+g+1} \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{align*}\n",
    "\n",
    "Valid for all $t < T$, with $G_T = 0$\n",
    "\n",
    "So, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_5 = 0 \\\\\n",
    "G_4 = R_5 + \\gamma G_5 = 2 + 0.5 \\cdot 0 = 2 \\\\\n",
    "G_3 = R_4 + \\gamma G_4 = 3 + 0.5 \\cdot 2 = 4 \\\\\n",
    "G_2 = R_3 + \\gamma G_3 = 6 + 0.5 \\cdot 4 = 8 \\\\\n",
    "G_1 = R_2 + \\gamma G_2 = 2 + 0.5 \\cdot 8 = 6 \\\\\n",
    "G_0 = R_1 + \\gamma G_1 = -1 + 0.5 \\cdot 6 = 2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.9\n",
    "\n",
    "**Q**\n",
    "\n",
    "Suppose γ = 0.9 and the reward sequence is R1 = 2 followed by an infinite sequence of 7s. What are G1 and G0?\n",
    "\n",
    "**A**\n",
    "\n",
    "\\begin{align*}\n",
    "G_1 &= \\sum_{k=0}^{\\infty} \\gamma^k R_{1+k+1} \\\\\n",
    "&= \\sum_{k=0}^{\\infty} (\\gamma^k \\cdot 7) \\\\\n",
    "&= 7 \\sum_{k=0}^{\\infty} \\gamma^k \\\\\n",
    "&= 7 \\frac{1}{1 - \\gamma} \\\\\n",
    "&= \\frac{7}{1 - 0.9} \\\\\n",
    "&= \\frac{7}{0.1} \\\\\n",
    "&= 70\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "G_0 &= R_1 + \\gamma G_1 \\\\\n",
    "&= 2 + 0.9 \\cdot 70 \\\\\n",
    "&= 2 + 63 \\\\\n",
    "&= 65\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.10\n",
    "\n",
    "**Q**\n",
    "\n",
    "Prove the second equality in:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "**A**\n",
    "\n",
    "Initially, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\gamma^0 + \\gamma^1 + \\gamma^2 + ...\n",
    "\\end{align*}\n",
    "\n",
    "For $0 < \\gamma < 1$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - \\gamma G_t &= [\\gamma^0 + \\gamma^1 + \\gamma^2 + ...] - [\\gamma^1 + \\gamma^2 + \\gamma^3 + ...] \\\\\n",
    "&= \\gamma^0 + [\\gamma^1 - \\gamma^1] + [\\gamma^2 - \\gamma^2] + ... \\\\\n",
    "&= \\gamma^0 \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "(it's valid only for $\\gamma < 1$ because $\\gamma^k$ converges to 0 as k goes to $\\infty$ in this case, but not otherwise)\n",
    "\n",
    "Isolating $G_t$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - \\gamma G_t = G_t \\cdot (1 - \\gamma) = 1\n",
    "\\end{align*}\n",
    "\n",
    "And the solution:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.11\n",
    "\n",
    "**Q**\n",
    "\n",
    "If the current state is $S_t$, and actions are selected according to stochastic policy π, then what is the\n",
    "expectation of $R_{t+1}$ in terms of π and the four-argument function p (3.2)?\n",
    "\n",
    "**A**\n",
    "\n",
    "The four-argument function is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(s', r|s, a) \\stackrel{\\cdot}{=} Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\}\n",
    "\\end{align*}\n",
    "\n",
    "The probability of choosing the action a, given the state s, is $\\pi(a|s)$, and the probability of reward r (and state s'), given the state s and action a is $p(s', r|s, a)$, so the probability of a given state generate a specific action a, a new state s' and the reward r is $\\pi(a|s) \\cdot p(s', r|s, a)$.\n",
    "\n",
    "It's also important to note that the sum of these probabilities over all possible actions in state $S_t=s$ (all $a \\in \\mathcal{A}(s)$), for all possible new states and rewards, is 1:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r|s, a) = 1\\text{, with } S_t=s\n",
    "\\end{align*}\n",
    "\n",
    "The expectation of $R_{t+1}$ is the sum of the probability of each action a, given the state s, multiplied by the sum of each possible reward r multiplied by its probability of occurence given action a, so:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [R_{t+1}|S_t=s] = \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Which is the sum of the weighted rewards of each action in state $S_t=s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.12\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $v_\\pi$ in terms of $q_\\pi$ and $\\pi$.\n",
    "\n",
    "**A**\n",
    "\n",
    "For a state $S_t=s$, the probability of $A_t=a$ is $\\pi(a|s)$ (by definition). The expectation of $G_t$ given s, is the sum of the expectations of $G_t$ for every possible action in state s, multiplied for the probability of that action happening:\n",
    "\n",
    "\\begin{align*}\n",
    "v_\\pi(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s \\right] \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\right) \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\cdot q_{\\pi}(s, a)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.13\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $q_\\pi$ in terms of $v_\\pi$ and the four-argument p.\n",
    "\n",
    "**A**\n",
    "\n",
    "The four-argument function is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(s', r|s, a) \\stackrel{\\cdot}{=} Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\}\n",
    "\\end{align*}\n",
    "\n",
    "$v_{\\pi}$ is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "v_\\pi(s) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s] = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s \\right]\n",
    "\\end{align*}\n",
    "\n",
    "$q_{\\pi}$ is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s, A_t=a] = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right]\n",
    "\\end{align*}\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) &= \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^0 R_{t+1} + \\sum_{k=1}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^{k+1} R_{[t+1]+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma \\cdot \\sum_{k=0}^{\\infty} \\gamma^k R_{[t+1]+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right]\n",
    "\\end{align*}\n",
    "\n",
    "Solving the left side:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a)\n",
    "\\end{align*}\n",
    "\n",
    "Because, according to *Exercise 3.11*:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [R_{t+1}|S_t=s] = \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "and in this case we already have a, so the probability of choosing it is 1, and every other action in $\\mathcal{A}(s)$ has probability 0:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [ R_{t+1} | S_t=s, A_t=a ] = \\sum_{x \\in \\mathcal{A}(s)} \\left( \\mathbb{1}_{x = a} \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, x) \\right) = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a)\n",
    "\\end{align*}\n",
    "\n",
    "Solving the right side:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right] &= \\gamma \\cdot \\mathbb{E}_{\\pi} \\left[ G_{t+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\gamma \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) \\cdot \\mathbb{E}_{\\pi} \\left[ G_{t+1} | S_{t+1}=s' \\right] \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s')\n",
    "\\end{align*}\n",
    "\n",
    "($v_{\\pi}(s')$ can only be considered when state s and action a causes the new state s', but this can be considered for every new state, so we multiply the probability of every new state ocurring, which can be calculated with $p(s', r | s, a)$ for every $s' \\in \\mathcal{S}$ and $r \\in \\mathcal{R}$)\n",
    "\n",
    "Finally:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) &=  \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\left[ \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right] + \\left[ \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s') \\right] \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) + \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s') \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r|s, a) \\cdot (r + \\gamma \\cdot v_{\\pi}(s'))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.14\n",
    "\n",
    "**Q**\n",
    "\n",
    "The Bellman equation (3.14) must hold for each state for the value function $v_π$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighbouring states, valued at +2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n",
    "\n",
    "**A**\n",
    "\n",
    "The Bellman equation is:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G{t+1} | S_t=s] \\\\\n",
    "&= \\sum_a \\pi (a|s) \\sum_{s'} \\sum_r p(s', r | s, a) \\left[ r + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s'] \\right] \\\\\n",
    "&= \\sum_a \\pi (a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] \\text{, for all } s \\in \\mathcal{S} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The center state, values at +0.7 (s), has the following possible actions:\n",
    "\n",
    "- Go North: direct reward 0, v(s') = 2.3\n",
    "- Go East: direct reward 0, v(s') = 0.4\n",
    "- Go South: direct reward 0, v(s') = -0.4\n",
    "- Go West: direct reward 0, v(s') = 0.7\n",
    "\n",
    "We have $\\gamma = 0.9$ and $\\pi (a|s) = 0.25$ (1/4) for each of the 4 possible actions (go north, east, south or west), because the agent selects all four actions with equal probability in all states (as stated in the Gridworld example definition).\n",
    "\n",
    "Also, for each action, there's only one next state possible (and reward), so $p(s', r | s, a) = 1$ for every $s in \\{a_n, a_e, a_s, a_w\\}$ (all possible actions). \n",
    "\n",
    "So, we have only a single next state s' and reward r for a given state s and action a:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] = r + \\gamma v_{\\pi}(s') \\text{, given } S_t=s \\text{ and } A_t=a\n",
    "\\end{align*}\n",
    "\n",
    "Considering the possible actions and state-values ($v_{\\pi}$) relative to the center position in the example (s, with $v_{\\pi}(s) = 0.7$):\n",
    "\n",
    "- $a_n$: go north, gives $s_n'$, with $v_{\\pi}(s_n') = 2.3$ and $r_n = 0$\n",
    "- $a_e$: go east, gives $s_e'$, with $v_{\\pi}(s_e') = 0.4$ and $r_e = 0$\n",
    "- $a_s$: go south, gives $s_s'$, with $v_{\\pi}(s_s') = -0.4$ and $r_s = 0$\n",
    "- $a_w$: go west, gives $s_w'$, with $v_{\\pi}(s_w') = 0.7$ and $r_w = 0$\n",
    "\n",
    "Applying the Bellman equation:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &= \\sum_a \\pi (a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] \\\\\n",
    "&= \\sum_{a \\in \\{a_n, a_e, a_s, a_w\\}} \\pi (a|s) \\cdot 1 \\cdot [ r + \\gamma v_{\\pi}(s') ] \\\\ \n",
    "&= \\sum_{a \\in \\{a_n, a_e, a_s, a_w\\}} \\pi (a|s) [ r + \\gamma v_{\\pi}(s') ] \\\\ \n",
    "&= \\pi (a_n|s) [ r_n + \\gamma v_{\\pi}(s_n') ] + \\pi (a_e|s) [ r_e + \\gamma v_{\\pi}(s_e') ] + \\pi (a_s|s) [ r_s + \\gamma v_{\\pi}(s_s') ] + \\pi (a_w|s) [ r_w + \\gamma v_{\\pi}(s_w') ] \\\\ \n",
    "&= 0.25 \\cdot [ r_n + \\gamma v_{\\pi}(s_n') ] + 0.25 \\cdot [ r_e + \\gamma v_{\\pi}(s_e') ] + 0.25 \\cdot [ r_s + \\gamma v_{\\pi}(s_s') ] + 0.25 \\cdot [ r_w + \\gamma v_{\\pi}(s_w') ] \\\\ \n",
    "&= 0.25 \\cdot [ r_n + \\gamma v_{\\pi}(s_n') + r_e + \\gamma v_{\\pi}(s_e') + r_s + \\gamma v_{\\pi}(s_s') + r_w + \\gamma v_{\\pi}(s_w') ]\n",
    "\\end{align*}\n",
    "\n",
    "The above solution works for every cell in the gridworld. \n",
    "\n",
    "For any cell that is not A or B, and is not in the edge of the grid (which is the case of the cell in the center), all direct rewards are 0, so:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) = 0.25 \\cdot [ \\gamma v_{\\pi}(s_n') + \\gamma v_{\\pi}(s_e') + \\gamma v_{\\pi}(s_s') + \\gamma v_{\\pi}(s_w') ]\n",
    "\\end{align*}\n",
    "\n",
    "With $\\gamma = 0.9$, for the cell in the center, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &= 0.25 \\cdot [ \\gamma v_{\\pi}(s_n') + \\gamma v_{\\pi}(s_e') + \\gamma v_{\\pi}(s_s') + \\gamma v_{\\pi}(s_w') ] \\\\\n",
    "&= 0.25 \\cdot [ 0.9 \\cdot 2.3 + 0.9 \\cdot -0.4 + 0.9 \\cdot 0.4 + 0.9 \\cdot 0.7 ] \\\\\n",
    "&= 0.25 \\cdot 0.9 \\cdot [ 2.3 - 0.4 + 0.4 + 0.7 ] \\\\\n",
    "&= 0.225 \\cdot 3.0 \\\\\n",
    "&= 0.675 \\approx 0.7 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.15\n",
    "\n",
    "**Q**\n",
    "\n",
    "In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant c to all the rewards adds a constant, $v_c$, to the values of all states, and thus does not affect the relative values of any states under any policies. What is $v_c$ in terms of c and γ?\n",
    "\n",
    "**A**\n",
    "\n",
    "The equation 3.8 is as follows, for $0 <= \\gamma <= 1$:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t \\stackrel{.}{=} R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "\n",
    "Assuming the new value function $G_c$ ($G_{c0}, G_{c1}, G_{c1}, ..., G_{ct}, ...$) and the corresponding reward $R_c$ ($R_{ct}$ at time step t), with $R_{ct} = R_t + c$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} &\\stackrel{.}{=} R_{c[t+1]} + \\gamma R_{c[t+2]} + \\gamma^2 R_{c[t+3]} + ... \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\gamma^k R_{c[t+k+1]} \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\gamma^k [c + R_{t+k+1}] \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\gamma^k \\cdot c + \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\\\\n",
    "&= c \\cdot \\sum_{k=0}^{\\infty} \\gamma^k + G_t\n",
    "\\end{align*}\n",
    "\n",
    "And we have, as shown in a previous exercise (3.10):\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} &= c \\cdot \\sum_{k=0}^{\\infty} \\gamma^k + G_t \\\\\n",
    "&= \\frac{c}{1 - \\gamma} + G_t \\\\\n",
    "&= v_c + G_t\n",
    "\\end{align*}\n",
    "\n",
    "With:\n",
    "\n",
    "\\begin{align*}\n",
    "v_c = \\frac{c}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "And also:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{c \\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_{ct} | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[v_c + G_t | S_t=s] \\\\\n",
    "&= v_c + \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= v_c + v_{\\pi}(s) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "($\\mathbb{E}_{\\pi}[v_c | S_t=s] = v_c$, because $\\gamma$, and consequently $v_c$, is a constant)\n",
    "\n",
    "Based on what was defined and proven previously, the signs of the rewards are not important, as long as the difference between them is kept, because adding a constant c to all rewards adds the constant $v_c = \\frac{c}{1-\\gamma}$ to the values of all states (this means that states with higher values will remain with higher values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.16\n",
    "\n",
    "**Q**\n",
    "\n",
    "Now consider adding a constant c to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.\n",
    "\n",
    "**A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous exercise, we had:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} = c \\cdot \\sum_{k=0}^{\\infty} \\gamma^k + G_t\n",
    "\\end{align*}\n",
    "\n",
    "And:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "Resulting in:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} = \\frac{1}{1 - \\gamma} + G_t\n",
    "\\end{align*}\n",
    "\n",
    "In an episodic task, there's a finite number of states, with a terminal state T.\n",
    "\n",
    "We would have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} = c \\cdot \\sum_{k=0}^{T-t} \\gamma^k + G_t\n",
    "\\end{align*}\n",
    "\n",
    "And\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{k=0}^{T-t} \\gamma^k - \\gamma \\sum_{k=0}^{T-t} \\gamma^k &= \\sum_{k=0}^{T-t} \\gamma^k - \\sum_{k=0}^{T-t} \\gamma^{k+1} \\\\\n",
    "(1 - \\gamma) \\sum_{k=0}^{T-t} \\gamma^k &= \\gamma^0 - \\gamma^{T-t+1} \\\\\n",
    "\\sum_{k=0}^{T-t} &= \\frac{1 - \\gamma^{T-t+1}}{(1 - \\gamma)}\n",
    "\\end{align*}\n",
    "\n",
    "And similarly:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{c \\pi}(s) = c \\cdot \\frac{1 - \\gamma^{T-t+1}}{(1 - \\gamma)} + v_{\\pi}(s) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This value is not constant, because t is increased by 1 after every time step.\n",
    "\n",
    "The difference between a state value and the next starts with $c \\cdot \\frac{1 - \\gamma^{T+1}}{(1 - \\gamma)}$ (an absolute value higher than c) in the first time step and decreases the absolute value added after each time step, ending in $c \\cdot \\frac{1 - \\gamma^{T-T+1}}{(1 - \\gamma)} = c \\cdot \\frac{1 - \\gamma}{(1 - \\gamma)} = c$ at the last time step T.\n",
    "\n",
    "In other words, this would have an effect because the difference of the values are not the same.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Consider:\n",
    "\n",
    "- $\\gamma = 0.5$\n",
    "- 4 time steps t = 0, 1, 2, 3 (T = 3)\n",
    "- 4 states $S_0$, $S_1$, $S_2$, $S_3$, with $S_t$ as the state at time step t ($S_3$ is the final state at t=T=3)\n",
    "- $R_t$ the reward at time step t, t > 0\n",
    "- $A_t$ is the action at time t, t < 4, that changes $S_t$ to $S_{t+1}$ and returns the reward $R_{t+1}$\n",
    "- $v_{\\pi}(s)$ the state value of state s under some policy $\\pi$, with (for the sake of the example) $v_{\\pi}(S_0) = 2$, $v_{\\pi}(S_1) = -3$, $v_{\\pi}(S_2) = 0$, $v_{\\pi}(S_3) = 5$\n",
    "\n",
    "Adding a constant c=10 to the rewards will change the state values to:\n",
    "\n",
    "- $v_{c \\pi}(S_0) = 10 \\cdot \\frac{1 - 0.5^{3-0+1}}{(1 - 0.9)} + 2 = 10 \\cdot \\frac{\\frac{15}{16}}{0.1} + 2 = 100 \\cdot \\frac{15}{16} + 2 = 1500 / 16 + 2 = 93.75 + 2 = 95.75$\n",
    "- $v_{c \\pi}(S_1) = 10 \\cdot \\frac{1 - 0.5^{3-1+1}}{(1 - 0.9)} - 3 = 10 \\cdot \\frac{\\frac{7}{8}}{0.1} - 3 = 100 \\cdot \\frac{14}{16} - 3 = 1400 / 16 - 3 = 87.5 - 3 = 90.5$\n",
    "- $v_{c \\pi}(S_2) = 10 \\cdot \\frac{1 - 0.5^{3-2+1}}{(1 - 0.9)} + 0 = 10 \\cdot \\frac{\\frac{3}{4}}{0.1} + 0 = 100 \\cdot \\frac{12}{16} + 0 = 1200 / 16 + 0 = 75 + 0 = 75$\n",
    "- $v_{c \\pi}(S_3) = 10 \\cdot \\frac{1 - 0.5^{3-3+1}}{(1 - 0.9)} + 5 = 10 \\cdot \\frac{\\frac{1}{2}}{0.1} + 5 = 100 \\cdot \\frac{8}{16} + 5 = 800 / 16 + 5 = 50 + 5 = 55$\n",
    "\n",
    "Initially: \n",
    "\n",
    "- $v_{\\pi}(S_0) = 2$\n",
    "- $v_{\\pi}(S_1) = -3$\n",
    "- $v_{\\pi}(S_2) = 0$\n",
    "- $v_{\\pi}(S_3) = 5$\n",
    "\n",
    "After adding c=10 to the rewards:\n",
    "\n",
    "- $v_{c \\pi}(S_0) = 95.75$\n",
    "- $v_{c \\pi}(S_1) = 90.5$\n",
    "- $v_{c \\pi}(S_2) = 75$\n",
    "- $v_{c \\pi}(S_3) = 55$\n",
    "\n",
    "The difference of the state values changed, as well as the order:\n",
    "\n",
    "- $v_{\\pi}(S_1) < v_{\\pi}(S_2) < _{\\pi}(S_0) < v_{\\pi}(S_3)$\n",
    "- $v_{c \\pi}(S_3) < v_{c \\pi}(S_2) < v_{c \\pi}(S_1) < v_{c \\pi}(S_0)$ (states in the initial steps were more affected by adding c to the rewards, considering the actions and new states the same; in practice, the agent will try to choose other actions to procrastinate and receive more rewards when c > 0, and to end faster when c < 0, but that will depend on the possible actions and new states for each state)\n",
    "\n",
    "The previous example considered only the state values, keeping all actions, new states and number of time steps the same, to show that the state values changed and have not kept the same order as before.\n",
    "\n",
    "To explain how the agent may choose different actions, a simpler example:\n",
    "\n",
    "- 2 states $S_0$ and $S_1$\n",
    "- 2 actions $A_0$ and $A_1$\n",
    "- Reward 5 if chooses $A_1$, going to state $S_1$ and ending the episode\n",
    "- Reward -1 if chooses $A_0$, ending in the same state $S_0$ (non-terminal)\n",
    "- The agent will try maximize as a function of T: $\\gamma^T \\cdot (c+5) + \\sum_{t=0}^{T-1} \\gamma^t \\cdot (c-1) = \\gamma^T \\cdot (c+5) + (c-1) \\cdot \\frac{1-\\gamma^T}{1-\\gamma}$ (it will choose $A_0$ repeatedly until it is not beneficial anymore)\n",
    "- For c=0 (the original case), the best value of T is 0 (will only choose $A_1$ and end the episode)\n",
    "\n",
    "If $\\gamma = 0.9$ and $c=10$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\gamma^T \\cdot (c+5) + (c-1) \\cdot \\frac{1-\\gamma^T}{1-\\gamma} &= 0.9^T \\cdot (10+5) + (10-1) \\cdot \\frac{1-0.9^T}{1-0.9} \\\\\n",
    "&= 15 \\cdot 0.9^T + 90 \\cdot (1-0.9^T) \\\\\n",
    "&= 15 \\cdot 0.9^T + 90 - 90 \\cdot 0.9^T \\\\\n",
    "&= 90 - 75 \\cdot 0.9^T\n",
    "\\end{align*}\n",
    "\n",
    "As T approaches $\\infty$, $90 - 75 \\cdot 0.9^T$ approaches 90 (it increases as T increases), so the agent will instead choose $A_0$ infinitely (in the original case, for $c=0$, it will choose $A_1$ instead and end the episode).\n",
    "\n",
    "As a general case:\n",
    "\n",
    "\\begin{align*}\n",
    "\\gamma^T \\cdot (c+5) + (c-1) \\cdot \\frac{1-\\gamma^T}{1-\\gamma} &= \\gamma^T \\cdot (c+5) + (1-\\gamma^T) \\cdot \\frac{c-1}{1-\\gamma} \\\\\n",
    "&= (c+5) \\cdot \\gamma^T + \\frac{c-1}{1-\\gamma} - \\frac{c-1}{1-\\gamma} \\cdot \\gamma^T \\\\\n",
    "&= \\frac{c-1}{1-\\gamma} + [c + 5 - \\frac{c-1}{1-\\gamma}] \\cdot \\gamma^T \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The objective would be to maximize $\\frac{c-1}{1-\\gamma} + [c + 5 - \\frac{c-1}{1-\\gamma}] \\cdot \\gamma^T$\n",
    "\n",
    "For given values of c and $\\gamma$ (0 < $\\gamma$ < 1), the objective is actually to maximize $[c + 5 - \\frac{c-1}{1-\\gamma}] \\cdot \\gamma^T$ over T. The optimal values of T are:\n",
    "\n",
    "- $T=0$ if $c + 5 - \\frac{c-1}{1-\\gamma} > 0$, because $\\gamma^T$ decreases as T increases, and the expression would reduce its value as T increases\n",
    "- $T=\\infty$ if $c + 5 - \\frac{c-1}{1-\\gamma} < 0$, because $\\gamma^T$ decreases as T increases, and the expression would increase its value as T increases (the negative part would decrease in its absolute value, increasing the result)\n",
    "- T can be anything if $c + 5 - \\frac{c-1}{1-\\gamma} = 0$ (no matter how many times you can $A_0$, and then $A_1$, the resulting cumulative reward would be the same; this is because the cummulative reward choosing $A_0$ until T-1, and then $A_1$, is the same for any T, because the reduction in the part of the reward of $A_1$ due to the increase of T, is equally compensated by calling $A_0$ more times)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
