{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "| s              | a              | | s'             | r              | | p(s', r\\|s, a) |\n",
    "| -------------- | -------------- |-| -------------- | -------------- |-| -------------- |\n",
    "| high           | search         | | high           | $r_{search}$   | | $\\alpha$       |\n",
    "| high           | search         | | low            | $r_{search}$   | | $1 - \\alpha$   |\n",
    "| low            | search         | | high           | -3             | | $1 - \\beta$    |\n",
    "| low            | search         | | low            | $r_{search}$   | | $\\beta$        |\n",
    "| high           | wait           | | high           | $r_{wait}$     | | 1              |\n",
    "| low            | wait           | | low            | $r_{wait}$     | | 1              |\n",
    "| low            | recharge       | | high           | 0              | | 1              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5\n",
    "\n",
    "Given: \n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1 \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "\\end{align*}\n",
    "\n",
    "In episodic tasks, $\\mathcal{S}$ represents the set of all states *minus* the terminal state. \n",
    "\n",
    "To make the equation apply to episodic tasks, the terminal state have to be considered as a possible state of s'.\n",
    "\n",
    "The modified version of the equation is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s' \\in \\mathcal{S^+}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1 \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "\\end{align*}\n",
    "\n",
    "The only change was from $\\mathcal{S}$ to $\\mathcal{S^+}$ for the possible values of s', but the possible values of s must still be in $\\mathcal{S}$ (and not in $\\mathcal{S^+}$), because a terminal state can't have a next state (s')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.7\n",
    "\n",
    "The reward will be the same no matter if the robot escape the maze in 10, 100, or any number of steps. The robot should receive a greater reward if it reaches in less time steps (or penalized if it reaches in more time steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.8\n",
    "\n",
    " **Q**\n",
    " \n",
    " Suppose $\\gamma = 0.5$ and the following sequence of rewards is received $R_1 = -1$, $R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0$, $G_1$, ..., $G_5$? Hint: Work backwards.\n",
    "\n",
    " **A**\n",
    "\n",
    " The return at time step t is:\n",
    " \n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "\n",
    "Considering $g = k - 1$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t &= R_{t+1} + \\sum_{k=1}^{\\infty} \\gamma^k R_{t+k+1} \\\\\n",
    "&= R_{t+1} + \\sum_{g=0}^{\\infty} \\gamma^{g+1} R_{t+[g+1]+1} \\\\\n",
    "&= R_{t+1} + \\gamma \\sum_{g=0}^{\\infty} \\gamma^g R_{[t+1]+g+1} \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{align*}\n",
    "\n",
    "Valid for all $t < T$, with $G_T = 0$\n",
    "\n",
    "So, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_5 = 0 \\\\\n",
    "G_4 = R_5 + \\gamma G_5 = 2 + 0.5 \\cdot 0 = 2 \\\\\n",
    "G_3 = R_4 + \\gamma G_4 = 3 + 0.5 \\cdot 2 = 4 \\\\\n",
    "G_2 = R_3 + \\gamma G_3 = 6 + 0.5 \\cdot 4 = 8 \\\\\n",
    "G_1 = R_2 + \\gamma G_2 = 2 + 0.5 \\cdot 8 = 6 \\\\\n",
    "G_0 = R_1 + \\gamma G_1 = -1 + 0.5 \\cdot 6 = 2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.9\n",
    "\n",
    "**Q**\n",
    "\n",
    "Suppose γ = 0.9 and the reward sequence is R1 = 2 followed by an infinite sequence of 7s. What are G1 and G0?\n",
    "\n",
    "**A**\n",
    "\n",
    "\\begin{align*}\n",
    "G_1 &= \\sum_{k=0}^{\\infty} \\gamma^k R_{1+k+1} \\\\\n",
    "&= \\sum_{k=0}^{\\infty} (\\gamma^k \\cdot 7) \\\\\n",
    "&= 7 \\sum_{k=0}^{\\infty} \\gamma^k \\\\\n",
    "&= 7 \\frac{1}{1 - \\gamma} \\\\\n",
    "&= \\frac{7}{1 - 0.9} \\\\\n",
    "&= \\frac{7}{0.1} \\\\\n",
    "&= 70\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "G_0 &= R_1 + \\gamma G_1 \\\\\n",
    "&= 2 + 0.9 \\cdot 70 \\\\\n",
    "&= 2 + 63 \\\\\n",
    "&= 65\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.10\n",
    "\n",
    "**Q**\n",
    "\n",
    "Prove the second equality in:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "**A**\n",
    "\n",
    "Initially, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\gamma^0 + \\gamma^1 + \\gamma^2 + ...\n",
    "\\end{align*}\n",
    "\n",
    "For $0 < \\gamma < 1$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - \\gamma G_t &= [\\gamma^0 + \\gamma^1 + \\gamma^2 + ...] - [\\gamma^1 + \\gamma^2 + \\gamma^3 + ...] \\\\\n",
    "&= \\gamma^0 + [\\gamma^1 - \\gamma^1] + [\\gamma^2 - \\gamma^2] + ... \\\\\n",
    "&= \\gamma^0 \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "(it's valid only for $\\gamma < 1$ because $\\gamma^k$ converges to 0 as k goes to $\\infty$ in this case, but not otherwise)\n",
    "\n",
    "Isolating $G_t$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - \\gamma G_t = G_t \\cdot (1 - \\gamma) = 1\n",
    "\\end{align*}\n",
    "\n",
    "And the solution:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.11\n",
    "\n",
    "**Q**\n",
    "\n",
    "If the current state is $S_t$, and actions are selected according to stochastic policy π, then what is the\n",
    "expectation of $R_{t+1}$ in terms of π and the four-argument function p (3.2)?\n",
    "\n",
    "**A**\n",
    "\n",
    "The four-argument function is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(s', r|s, a) \\stackrel{\\cdot}{=} Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\}\n",
    "\\end{align*}\n",
    "\n",
    "The probability of choosing the action a, given the state s, is $\\pi(a|s)$, and the probability of reward r (and state s'), given the state s and action a is $p(s', r|s, a)$, so the probability of a given state generate a specific action a, a new state s' and the reward r is $\\pi(a|s) \\cdot p(s', r|s, a)$.\n",
    "\n",
    "It's also important to note that the sum of these probabilities over all possible actions in state $S_t=s$ (all $a \\in \\mathcal{A}(s)$), for all possible new states and rewards, is 1:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r|s, a) = 1\\text{, with } S_t=s\n",
    "\\end{align*}\n",
    "\n",
    "The expectation of $R_{t+1}$ is the sum of the probability of each action a, given the state s, multiplied by the sum of each possible reward r multiplied by its probability of occurence given action a, so:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [R_{t+1}|S_t=s] = \\sum_{a \\in \\mathcal{A}(s)} \\left[ \\pi(a|s) \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "Which is the sum of the weighted rewards of each action in state $S_t=s$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
