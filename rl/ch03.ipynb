{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "| s              | a              | | s'             | r              | | p(s', r\\|s, a) |\n",
    "| -------------- | -------------- |-| -------------- | -------------- |-| -------------- |\n",
    "| high           | search         | | high           | $r_{search}$   | | $\\alpha$       |\n",
    "| high           | search         | | low            | $r_{search}$   | | $1 - \\alpha$   |\n",
    "| low            | search         | | high           | -3             | | $1 - \\beta$    |\n",
    "| low            | search         | | low            | $r_{search}$   | | $\\beta$        |\n",
    "| high           | wait           | | high           | $r_{wait}$     | | 1              |\n",
    "| low            | wait           | | low            | $r_{wait}$     | | 1              |\n",
    "| low            | recharge       | | high           | 0              | | 1              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5\n",
    "\n",
    "Given: \n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1 \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "\\end{align*}\n",
    "\n",
    "In episodic tasks, $\\mathcal{S}$ represents the set of all states *minus* the terminal state. \n",
    "\n",
    "To make the equation apply to episodic tasks, the terminal state have to be considered as a possible state of s'.\n",
    "\n",
    "The modified version of the equation is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s' \\in \\mathcal{S^+}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1 \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "\\end{align*}\n",
    "\n",
    "The only change was from $\\mathcal{S}$ to $\\mathcal{S^+}$ for the possible values of s', but the possible values of s must still be in $\\mathcal{S}$ (and not in $\\mathcal{S^+}$), because a terminal state can't have a next state (s')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.7\n",
    "\n",
    "The reward will be the same no matter if the robot escape the maze in 10, 100, or any number of steps. The robot should receive a greater reward if it reaches in less time steps (or penalized if it reaches in more time steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.8\n",
    "\n",
    " **Q**\n",
    " \n",
    " Suppose $\\gamma = 0.5$ and the following sequence of rewards is received $R_1 = -1$, $R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0$, $G_1$, ..., $G_5$? Hint: Work backwards.\n",
    "\n",
    " **A**\n",
    "\n",
    " The return at time step t is:\n",
    " \n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "\n",
    "Considering $g = k - 1$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t &= R_{t+1} + \\sum_{k=1}^{\\infty} \\gamma^k R_{t+k+1} \\\\\n",
    "&= R_{t+1} + \\sum_{g=0}^{\\infty} \\gamma^{g+1} R_{t+[g+1]+1} \\\\\n",
    "&= R_{t+1} + \\gamma \\sum_{g=0}^{\\infty} \\gamma^g R_{[t+1]+g+1} \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{align*}\n",
    "\n",
    "Valid for all $t < T$, with $G_T = 0$\n",
    "\n",
    "So, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_5 = 0 \\\\\n",
    "G_4 = R_5 + \\gamma G_5 = 2 + 0.5 \\cdot 0 = 2 \\\\\n",
    "G_3 = R_4 + \\gamma G_4 = 3 + 0.5 \\cdot 2 = 4 \\\\\n",
    "G_2 = R_3 + \\gamma G_3 = 6 + 0.5 \\cdot 4 = 8 \\\\\n",
    "G_1 = R_2 + \\gamma G_2 = 2 + 0.5 \\cdot 8 = 6 \\\\\n",
    "G_0 = R_1 + \\gamma G_1 = -1 + 0.5 \\cdot 6 = 2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.9\n",
    "\n",
    "**Q**\n",
    "\n",
    "Suppose γ = 0.9 and the reward sequence is R1 = 2 followed by an infinite sequence of 7s. What are G1 and G0?\n",
    "\n",
    "**A**\n",
    "\n",
    "\\begin{align*}\n",
    "G_1 &= \\sum_{k=0}^{\\infty} \\gamma^k R_{1+k+1} \\\\\n",
    "&= \\sum_{k=0}^{\\infty} (\\gamma^k \\cdot 7) \\\\\n",
    "&= 7 \\sum_{k=0}^{\\infty} \\gamma^k \\\\\n",
    "&= 7 \\frac{1}{1 - \\gamma} \\\\\n",
    "&= \\frac{7}{1 - 0.9} \\\\\n",
    "&= \\frac{7}{0.1} \\\\\n",
    "&= 70\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "G_0 &= R_1 + \\gamma G_1 \\\\\n",
    "&= 2 + 0.9 \\cdot 70 \\\\\n",
    "&= 2 + 63 \\\\\n",
    "&= 65\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.10\n",
    "\n",
    "**Q**\n",
    "\n",
    "Prove the second equality in:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "**A**\n",
    "\n",
    "Initially, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\gamma^0 + \\gamma^1 + \\gamma^2 + ...\n",
    "\\end{align*}\n",
    "\n",
    "For $0 < \\gamma < 1$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - \\gamma G_t &= [\\gamma^0 + \\gamma^1 + \\gamma^2 + ...] - [\\gamma^1 + \\gamma^2 + \\gamma^3 + ...] \\\\\n",
    "&= \\gamma^0 + [\\gamma^1 - \\gamma^1] + [\\gamma^2 - \\gamma^2] + ... \\\\\n",
    "&= \\gamma^0 \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "(it's valid only for $\\gamma < 1$ because $\\gamma^k$ converges to 0 as k goes to $\\infty$ in this case, but not otherwise)\n",
    "\n",
    "Isolating $G_t$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - \\gamma G_t = G_t \\cdot (1 - \\gamma) = 1\n",
    "\\end{align*}\n",
    "\n",
    "And the solution:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.11\n",
    "\n",
    "**Q**\n",
    "\n",
    "If the current state is $S_t$, and actions are selected according to stochastic policy π, then what is the\n",
    "expectation of $R_{t+1}$ in terms of π and the four-argument function p (3.2)?\n",
    "\n",
    "**A**\n",
    "\n",
    "The four-argument function is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(s', r|s, a) \\stackrel{\\cdot}{=} Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\}\n",
    "\\end{align*}\n",
    "\n",
    "The probability of choosing the action a, given the state s, is $\\pi(a|s)$, and the probability of reward r (and state s'), given the state s and action a is $p(s', r|s, a)$, so the probability of a given state generate a specific action a, a new state s' and the reward r is $\\pi(a|s) \\cdot p(s', r|s, a)$.\n",
    "\n",
    "It's also important to note that the sum of these probabilities over all possible actions in state $S_t=s$ (all $a \\in \\mathcal{A}(s)$), for all possible new states and rewards, is 1:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r|s, a) = 1\\text{, with } S_t=s\n",
    "\\end{align*}\n",
    "\n",
    "The expectation of $R_{t+1}$ is the sum of the probability of each action a, given the state s, multiplied by the sum of each possible reward r multiplied by its probability of occurence given action a, so:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [R_{t+1}|S_t=s] = \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Which is the sum of the weighted rewards of each action in state $S_t=s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.12\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $v_\\pi$ in terms of $q_\\pi$ and $\\pi$.\n",
    "\n",
    "**A**\n",
    "\n",
    "For a state $S_t=s$, the probability of $A_t=a$ is $\\pi(a|s)$ (by definition). The expectation of $G_t$ given s, is the sum of the expectations of $G_t$ for every possible action in state s, multiplied for the probability of that action happening:\n",
    "\n",
    "\\begin{align*}\n",
    "v_\\pi(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s \\right] \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\right) \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\cdot q_{\\pi}(s, a)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.13\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $q_\\pi$ in terms of $v_\\pi$ and the four-argument p.\n",
    "\n",
    "**A**\n",
    "\n",
    "The four-argument function is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(s', r|s, a) \\stackrel{\\cdot}{=} Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\}\n",
    "\\end{align*}\n",
    "\n",
    "$v_{\\pi}$ is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "v_\\pi(s) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s] = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s \\right]\n",
    "\\end{align*}\n",
    "\n",
    "$v_{\\pi}$ is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s, A_t=a] = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right]\n",
    "\\end{align*}\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) &= \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^0 R_{t+1} + \\sum_{k=1}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^{k+1} R_{[t+1]+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma \\cdot \\sum_{k=0}^{\\infty} \\gamma^k R_{[t+1]+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right]\n",
    "\\end{align*}\n",
    "\n",
    "Solving the left side:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a)\n",
    "\\end{align*}\n",
    "\n",
    "Because, according to *Exercise 3.11*:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [R_{t+1}|S_t=s] = \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "and in this case we already have a, so the probability of choosing it is 1, and every other action in $\\mathcal{A}(s)$ has probability 0:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [ R_{t+1} | S_t=s, A_t=a ] = \\sum_{x \\in \\mathcal{A}(s)} \\left( \\mathbb{1}_{x = a} \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, x) \\right) = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a)\n",
    "\\end{align*}\n",
    "\n",
    "Solving the right side:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right] &= \\gamma \\cdot \\mathbb{E}_{\\pi} \\left[ G_{t+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\gamma \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) \\cdot \\mathbb{E}_{\\pi} \\left[ G_{t+1} | S_{t+1}=s' \\right] \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s')\n",
    "\\end{align*}\n",
    "\n",
    "($v_{\\pi}(s')$ can only be considered when state s and action a causes the new state s', but this can be considered for every new state, so we multiply the probability of every new state ocurring, which can be calculated with $p(s', r | s, a)$ for every $s' \\in \\mathcal{S}$ and $r \\in \\mathcal{R}$)\n",
    "\n",
    "Finally:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) &=  \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\left[ \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right] + \\left[ \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s') \\right] \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) + \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s') \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r|s, a) \\cdot (r + \\gamma \\cdot v_{\\pi}(s'))\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
