{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "| s              | a              | | s'             | r              | | p(s', r\\|s, a) |\n",
    "| -------------- | -------------- |-| -------------- | -------------- |-| -------------- |\n",
    "| high           | search         | | high           | $r_{search}$   | | $\\alpha$       |\n",
    "| high           | search         | | low            | $r_{search}$   | | $1 - \\alpha$   |\n",
    "| low            | search         | | high           | -3             | | $1 - \\beta$    |\n",
    "| low            | search         | | low            | $r_{search}$   | | $\\beta$        |\n",
    "| high           | wait           | | high           | $r_{wait}$     | | 1              |\n",
    "| low            | wait           | | low            | $r_{wait}$     | | 1              |\n",
    "| low            | recharge       | | high           | 0              | | 1              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5\n",
    "\n",
    "Given: \n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1 \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "\\end{align*}\n",
    "\n",
    "In episodic tasks, $\\mathcal{S}$ represents the set of all states *minus* the terminal state. \n",
    "\n",
    "To make the equation apply to episodic tasks, the terminal state have to be considered as a possible state of s'.\n",
    "\n",
    "The modified version of the equation is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s' \\in \\mathcal{S^+}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1 \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "\\end{align*}\n",
    "\n",
    "The only change was from $\\mathcal{S}$ to $\\mathcal{S^+}$ for the possible values of s', but the possible values of s must still be in $\\mathcal{S}$ (and not in $\\mathcal{S^+}$), because a terminal state can't have a next state (s')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.7\n",
    "\n",
    "The reward will be the same no matter if the robot escape the maze in 10, 100, or any number of steps. The robot should receive a greater reward if it reaches in less time steps (or penalized if it reaches in more time steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.8\n",
    "\n",
    " **Q**\n",
    " \n",
    " Suppose $\\gamma = 0.5$ and the following sequence of rewards is received $R_1 = -1$, $R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0$, $G_1$, ..., $G_5$? Hint: Work backwards.\n",
    "\n",
    " **A**\n",
    "\n",
    " The return at time step t is:\n",
    " \n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "\n",
    "Considering $g = k - 1$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t &= R_{t+1} + \\sum_{k=1}^{\\infty} \\gamma^k R_{t+k+1} \\\\\n",
    "&= R_{t+1} + \\sum_{g=0}^{\\infty} \\gamma^{g+1} R_{t+[g+1]+1} \\\\\n",
    "&= R_{t+1} + \\gamma \\sum_{g=0}^{\\infty} \\gamma^g R_{[t+1]+g+1} \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{align*}\n",
    "\n",
    "Valid for all $t < T$, with $G_T = 0$\n",
    "\n",
    "So, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_5 = 0 \\\\\n",
    "G_4 = R_5 + \\gamma G_5 = 2 + 0.5 \\cdot 0 = 2 \\\\\n",
    "G_3 = R_4 + \\gamma G_4 = 3 + 0.5 \\cdot 2 = 4 \\\\\n",
    "G_2 = R_3 + \\gamma G_3 = 6 + 0.5 \\cdot 4 = 8 \\\\\n",
    "G_1 = R_2 + \\gamma G_2 = 2 + 0.5 \\cdot 8 = 6 \\\\\n",
    "G_0 = R_1 + \\gamma G_1 = -1 + 0.5 \\cdot 6 = 2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.9\n",
    "\n",
    "**Q**\n",
    "\n",
    "Suppose γ = 0.9 and the reward sequence is R1 = 2 followed by an infinite sequence of 7s. What are G1 and G0?\n",
    "\n",
    "**A**\n",
    "\n",
    "\\begin{align*}\n",
    "G_1 &= \\sum_{k=0}^{\\infty} \\gamma^k R_{1+k+1} \\\\\n",
    "&= \\sum_{k=0}^{\\infty} (\\gamma^k \\cdot 7) \\\\\n",
    "&= 7 \\sum_{k=0}^{\\infty} \\gamma^k \\\\\n",
    "&= 7 \\frac{1}{1 - \\gamma} \\\\\n",
    "&= \\frac{7}{1 - 0.9} \\\\\n",
    "&= \\frac{7}{0.1} \\\\\n",
    "&= 70\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "G_0 &= R_1 + \\gamma G_1 \\\\\n",
    "&= 2 + 0.9 \\cdot 70 \\\\\n",
    "&= 2 + 63 \\\\\n",
    "&= 65\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.10\n",
    "\n",
    "**Q**\n",
    "\n",
    "Prove the second equality in:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "**A**\n",
    "\n",
    "Initially, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\gamma^0 + \\gamma^1 + \\gamma^2 + ...\n",
    "\\end{align*}\n",
    "\n",
    "For $0 < \\gamma < 1$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - \\gamma G_t &= [\\gamma^0 + \\gamma^1 + \\gamma^2 + ...] - [\\gamma^1 + \\gamma^2 + \\gamma^3 + ...] \\\\\n",
    "&= \\gamma^0 + [\\gamma^1 - \\gamma^1] + [\\gamma^2 - \\gamma^2] + ... \\\\\n",
    "&= \\gamma^0 \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "(it's valid only for $\\gamma < 1$ because $\\gamma^k$ converges to 0 as k goes to $\\infty$ in this case, but not otherwise)\n",
    "\n",
    "Isolating $G_t$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - \\gamma G_t = G_t \\cdot (1 - \\gamma) = 1\n",
    "\\end{align*}\n",
    "\n",
    "And the solution:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.11\n",
    "\n",
    "**Q**\n",
    "\n",
    "If the current state is $S_t$, and actions are selected according to stochastic policy π, then what is the\n",
    "expectation of $R_{t+1}$ in terms of π and the four-argument function p (3.2)?\n",
    "\n",
    "**A**\n",
    "\n",
    "The four-argument function is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(s', r|s, a) \\stackrel{\\cdot}{=} Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\}\n",
    "\\end{align*}\n",
    "\n",
    "The probability of choosing the action a, given the state s, is $\\pi(a|s)$, and the probability of reward r (and state s'), given the state s and action a is $p(s', r|s, a)$, so the probability of a given state generate a specific action a, a new state s' and the reward r is $\\pi(a|s) \\cdot p(s', r|s, a)$.\n",
    "\n",
    "It's also important to note that the sum of these probabilities over all possible actions in state $S_t=s$ (all $a \\in \\mathcal{A}(s)$), for all possible new states and rewards, is 1:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r|s, a) = 1\\text{, with } S_t=s\n",
    "\\end{align*}\n",
    "\n",
    "The expectation of $R_{t+1}$ is the sum of the probability of each action a, given the state s, multiplied by the sum of each possible reward r multiplied by its probability of occurence given action a, so:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [R_{t+1}|S_t=s] = \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Which is the sum of the weighted rewards of each action in state $S_t=s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.12\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $v_\\pi$ in terms of $q_\\pi$ and $\\pi$.\n",
    "\n",
    "**A**\n",
    "\n",
    "For a state $S_t=s$, the probability of $A_t=a$ is $\\pi(a|s)$ (by definition). The expectation of $G_t$ given s, is the sum of the expectations of $G_t$ for every possible action in state s, multiplied for the probability of that action happening:\n",
    "\n",
    "\\begin{align*}\n",
    "v_\\pi(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s \\right] \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\right) \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\cdot q_{\\pi}(s, a)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.13\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $q_\\pi$ in terms of $v_\\pi$ and the four-argument p.\n",
    "\n",
    "**A**\n",
    "\n",
    "The four-argument function is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(s', r|s, a) \\stackrel{\\cdot}{=} Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\}\n",
    "\\end{align*}\n",
    "\n",
    "$v_{\\pi}$ is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "v_\\pi(s) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s] = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s \\right]\n",
    "\\end{align*}\n",
    "\n",
    "$q_{\\pi}$ is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s, A_t=a] = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right]\n",
    "\\end{align*}\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) &= \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^0 R_{t+1} + \\sum_{k=1}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^{k+1} R_{[t+1]+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma \\cdot \\sum_{k=0}^{\\infty} \\gamma^k R_{[t+1]+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right]\n",
    "\\end{align*}\n",
    "\n",
    "Solving the left side:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a)\n",
    "\\end{align*}\n",
    "\n",
    "Because, according to *Exercise 3.11*:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [R_{t+1}|S_t=s] = \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "and in this case we already have a, so the probability of choosing it is 1, and every other action in $\\mathcal{A}(s)$ has probability 0:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [ R_{t+1} | S_t=s, A_t=a ] = \\sum_{x \\in \\mathcal{A}(s)} \\left( \\mathbb{1}_{x = a} \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, x) \\right) = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a)\n",
    "\\end{align*}\n",
    "\n",
    "Solving the right side:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right] &= \\gamma \\cdot \\mathbb{E}_{\\pi} \\left[ G_{t+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\gamma \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) \\cdot \\mathbb{E}_{\\pi} \\left[ G_{t+1} | S_{t+1}=s' \\right] \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s')\n",
    "\\end{align*}\n",
    "\n",
    "($v_{\\pi}(s')$ can only be considered when state s and action a causes the new state s', but this can be considered for every new state, so we multiply the probability of every new state ocurring, which can be calculated with $p(s', r | s, a)$ for every $s' \\in \\mathcal{S}$ and $r \\in \\mathcal{R}$)\n",
    "\n",
    "Finally:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) &=  \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\left[ \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right] + \\left[ \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s') \\right] \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) + \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s') \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r|s, a) \\cdot (r + \\gamma \\cdot v_{\\pi}(s'))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.14\n",
    "\n",
    "**Q**\n",
    "\n",
    "The Bellman equation (3.14) must hold for each state for the value function $v_π$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighbouring states, valued at +2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n",
    "\n",
    "**A**\n",
    "\n",
    "The Bellman equation is:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G{t+1} | S_t=s] \\\\\n",
    "&= \\sum_a \\pi (a|s) \\sum_{s'} \\sum_r p(s', r | s, a) \\left[ r + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s'] \\right] \\\\\n",
    "&= \\sum_a \\pi (a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] \\text{, for all } s \\in \\mathcal{S} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The center state, values at +0.7 (s), has the following possible actions:\n",
    "\n",
    "- Go North: direct reward 0, v(s') = 2.3\n",
    "- Go East: direct reward 0, v(s') = 0.4\n",
    "- Go South: direct reward 0, v(s') = -0.4\n",
    "- Go West: direct reward 0, v(s') = 0.7\n",
    "\n",
    "We have $\\gamma = 0.9$ and $\\pi (a|s) = 0.25$ (1/4) for each of the 4 possible actions (go north, east, south or west), because the agent selects all four actions with equal probability in all states (as stated in the Gridworld example definition).\n",
    "\n",
    "Also, for each action, there's only one next state possible (and reward), so $p(s', r | s, a) = 1$ for every $s in \\{a_n, a_e, a_s, a_w\\}$ (all possible actions). \n",
    "\n",
    "So, we have only a single next state s' and reward r for a given state s and action a:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] = r + \\gamma v_{\\pi}(s') \\text{, given } S_t=s \\text{ and } A_t=a\n",
    "\\end{align*}\n",
    "\n",
    "Considering the possible actions and state-values ($v_{\\pi}$) relative to the center position in the example (s, with $v_{\\pi}(s) = 0.7$):\n",
    "\n",
    "- $a_n$: go north, gives $s_n'$, with $v_{\\pi}(s_n') = 2.3$ and $r_n = 0$\n",
    "- $a_e$: go east, gives $s_e'$, with $v_{\\pi}(s_e') = 0.4$ and $r_e = 0$\n",
    "- $a_s$: go south, gives $s_s'$, with $v_{\\pi}(s_s') = -0.4$ and $r_s = 0$\n",
    "- $a_w$: go west, gives $s_w'$, with $v_{\\pi}(s_w') = 0.7$ and $r_w = 0$\n",
    "\n",
    "Applying the Bellman equation:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &= \\sum_a \\pi (a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] \\\\\n",
    "&= \\sum_{a \\in \\{a_n, a_e, a_s, a_w\\}} \\pi (a|s) \\cdot 1 \\cdot [ r + \\gamma v_{\\pi}(s') ] \\\\ \n",
    "&= \\sum_{a \\in \\{a_n, a_e, a_s, a_w\\}} \\pi (a|s) [ r + \\gamma v_{\\pi}(s') ] \\\\ \n",
    "&= \\pi (a_n|s) [ r_n + \\gamma v_{\\pi}(s_n') ] + \\pi (a_e|s) [ r_e + \\gamma v_{\\pi}(s_e') ] + \\pi (a_s|s) [ r_s + \\gamma v_{\\pi}(s_s') ] + \\pi (a_w|s) [ r_w + \\gamma v_{\\pi}(s_w') ] \\\\ \n",
    "&= 0.25 \\cdot [ r_n + \\gamma v_{\\pi}(s_n') ] + 0.25 \\cdot [ r_e + \\gamma v_{\\pi}(s_e') ] + 0.25 \\cdot [ r_s + \\gamma v_{\\pi}(s_s') ] + 0.25 \\cdot [ r_w + \\gamma v_{\\pi}(s_w') ] \\\\ \n",
    "&= 0.25 \\cdot [ r_n + \\gamma v_{\\pi}(s_n') + r_e + \\gamma v_{\\pi}(s_e') + r_s + \\gamma v_{\\pi}(s_s') + r_w + \\gamma v_{\\pi}(s_w') ]\n",
    "\\end{align*}\n",
    "\n",
    "The above solution works for every cell in the gridworld. \n",
    "\n",
    "For any cell that is not A or B, and is not in the edge of the grid (which is the case of the cell in the center), all direct rewards are 0, so:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) = 0.25 \\cdot [ \\gamma v_{\\pi}(s_n') + \\gamma v_{\\pi}(s_e') + \\gamma v_{\\pi}(s_s') + \\gamma v_{\\pi}(s_w') ]\n",
    "\\end{align*}\n",
    "\n",
    "With $\\gamma = 0.9$, for the cell in the center, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &= 0.25 \\cdot [ \\gamma v_{\\pi}(s_n') + \\gamma v_{\\pi}(s_e') + \\gamma v_{\\pi}(s_s') + \\gamma v_{\\pi}(s_w') ] \\\\\n",
    "&= 0.25 \\cdot [ 0.9 \\cdot 2.3 + 0.9 \\cdot -0.4 + 0.9 \\cdot 0.4 + 0.9 \\cdot 0.7 ] \\\\\n",
    "&= 0.25 \\cdot 0.9 \\cdot [ 2.3 - 0.4 + 0.4 + 0.7 ] \\\\\n",
    "&= 0.225 \\cdot 3.0 \\\\\n",
    "&= 0.675 \\approx 0.7 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.15\n",
    "\n",
    "**Q**\n",
    "\n",
    "In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant c to all the rewards adds a constant, $v_c$, to the values of all states, and thus does not affect the relative values of any states under any policies. What is $v_c$ in terms of c and γ?\n",
    "\n",
    "**A**\n",
    "\n",
    "The equation 3.8 is as follows, for $0 <= \\gamma <= 1$:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t \\stackrel{.}{=} R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "\n",
    "Assuming the new value function $G_c$ ($G_{c0}, G_{c1}, G_{c1}, ..., G_{ct}, ...$) and the corresponding reward $R_c$ ($R_{ct}$ at time step t), with $R_{ct} = R_t + c$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} &\\stackrel{.}{=} R_{c[t+1]} + \\gamma R_{c[t+2]} + \\gamma^2 R_{c[t+3]} + ... \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\gamma^k R_{c[t+k+1]} \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\gamma^k [c + R_{t+k+1}] \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\gamma^k \\cdot c + \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\\\\n",
    "&= c \\cdot \\sum_{k=0}^{\\infty} \\gamma^k + G_t\n",
    "\\end{align*}\n",
    "\n",
    "And we have, as shown in a previous exercise (3.10):\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} &= c \\cdot \\sum_{k=0}^{\\infty} \\gamma^k + G_t \\\\\n",
    "&= \\frac{c}{1 - \\gamma} + G_t \\\\\n",
    "&= v_c + G_t\n",
    "\\end{align*}\n",
    "\n",
    "With:\n",
    "\n",
    "\\begin{align*}\n",
    "v_c = \\frac{c}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "And also:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{c \\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_{ct} | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[v_c + G_t | S_t=s] \\\\\n",
    "&= v_c + \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= v_c + v_{\\pi}(s) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "($\\mathbb{E}_{\\pi}[v_c | S_t=s] = v_c$, because $\\gamma$, and consequently $v_c$, is a constant)\n",
    "\n",
    "Based on what was defined and proven previously, the signs of the rewards are not important, as long as the difference between them is kept, because adding a constant c to all rewards adds the constant $v_c = \\frac{c}{1-\\gamma}$ to the values of all states (this means that states with higher values will remain with higher values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.16\n",
    "\n",
    "**Q**\n",
    "\n",
    "Now consider adding a constant c to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.\n",
    "\n",
    "**A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous exercise, we had:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} = c \\cdot \\sum_{k=0}^{\\infty} \\gamma^k + G_t\n",
    "\\end{align*}\n",
    "\n",
    "And:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "Resulting in:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} = \\frac{1}{1 - \\gamma} + G_t\n",
    "\\end{align*}\n",
    "\n",
    "In an episodic task, there's a finite number of states, with a terminal state T.\n",
    "\n",
    "We would have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} = c \\cdot \\sum_{k=0}^{T-t} \\gamma^k + G_t\n",
    "\\end{align*}\n",
    "\n",
    "And\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{k=0}^{T-t} \\gamma^k - \\gamma \\sum_{k=0}^{T-t} \\gamma^k &= \\sum_{k=0}^{T-t} \\gamma^k - \\sum_{k=0}^{T-t} \\gamma^{k+1} \\\\\n",
    "(1 - \\gamma) \\sum_{k=0}^{T-t} \\gamma^k &= \\gamma^0 - \\gamma^{T-t+1} \\\\\n",
    "\\sum_{k=0}^{T-t} &= \\frac{1 - \\gamma^{T-t+1}}{(1 - \\gamma)}\n",
    "\\end{align*}\n",
    "\n",
    "And similarly:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{c \\pi}(s) = c \\cdot \\frac{1 - \\gamma^{T-t+1}}{(1 - \\gamma)} + v_{\\pi}(s) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This value is not constant, because t is increased by 1 after every time step.\n",
    "\n",
    "The difference between a state value and the next starts with $c \\cdot \\frac{1 - \\gamma^{T+1}}{(1 - \\gamma)}$ (an absolute value higher than c) in the first time step and decreases the absolute value added after each time step, ending in $c \\cdot \\frac{1 - \\gamma^{T-T+1}}{(1 - \\gamma)} = c \\cdot \\frac{1 - \\gamma}{(1 - \\gamma)} = c$ at the last time step T.\n",
    "\n",
    "In other words, this would have an effect because the difference of the values are not the same.\n",
    "\n",
    "**Example 01**\n",
    "\n",
    "Consider:\n",
    "\n",
    "- $\\gamma = 0.5$\n",
    "- 4 time steps t = 0, 1, 2, 3 (T = 3)\n",
    "- 4 states $S_0$, $S_1$, $S_2$, $S_3$, with $S_t$ as the state at time step t ($S_3$ is the final state at t=T=3)\n",
    "- $R_t$ the reward at time step t, t > 0\n",
    "- $A_t$ is the action at time t, t < 4, that changes $S_t$ to $S_{t+1}$ and returns the reward $R_{t+1}$\n",
    "- $v_{\\pi}(s)$ the state value of state s under some policy $\\pi$, with (for the sake of the example) $v_{\\pi}(S_0) = 2$, $v_{\\pi}(S_1) = -3$, $v_{\\pi}(S_2) = 0$, $v_{\\pi}(S_3) = 5$\n",
    "\n",
    "Adding a constant c=10 to the rewards will change the state values to:\n",
    "\n",
    "- $v_{c \\pi}(S_0) = 10 \\cdot \\frac{1 - 0.5^{3-0+1}}{(1 - 0.9)} + 2 = 10 \\cdot \\frac{\\frac{15}{16}}{0.1} + 2 = 100 \\cdot \\frac{15}{16} + 2 = 1500 / 16 + 2 = 93.75 + 2 = 95.75$\n",
    "- $v_{c \\pi}(S_1) = 10 \\cdot \\frac{1 - 0.5^{3-1+1}}{(1 - 0.9)} - 3 = 10 \\cdot \\frac{\\frac{7}{8}}{0.1} - 3 = 100 \\cdot \\frac{14}{16} - 3 = 1400 / 16 - 3 = 87.5 - 3 = 90.5$\n",
    "- $v_{c \\pi}(S_2) = 10 \\cdot \\frac{1 - 0.5^{3-2+1}}{(1 - 0.9)} + 0 = 10 \\cdot \\frac{\\frac{3}{4}}{0.1} + 0 = 100 \\cdot \\frac{12}{16} + 0 = 1200 / 16 + 0 = 75 + 0 = 75$\n",
    "- $v_{c \\pi}(S_3) = 10 \\cdot \\frac{1 - 0.5^{3-3+1}}{(1 - 0.9)} + 5 = 10 \\cdot \\frac{\\frac{1}{2}}{0.1} + 5 = 100 \\cdot \\frac{8}{16} + 5 = 800 / 16 + 5 = 50 + 5 = 55$\n",
    "\n",
    "Initially: \n",
    "\n",
    "- $v_{\\pi}(S_0) = 2$\n",
    "- $v_{\\pi}(S_1) = -3$\n",
    "- $v_{\\pi}(S_2) = 0$\n",
    "- $v_{\\pi}(S_3) = 5$\n",
    "\n",
    "After adding c=10 to the rewards:\n",
    "\n",
    "- $v_{c \\pi}(S_0) = 95.75$\n",
    "- $v_{c \\pi}(S_1) = 90.5$\n",
    "- $v_{c \\pi}(S_2) = 75$\n",
    "- $v_{c \\pi}(S_3) = 55$\n",
    "\n",
    "The difference of the state values changed, as well as the order:\n",
    "\n",
    "- $v_{\\pi}(S_1) < v_{\\pi}(S_2) < _{\\pi}(S_0) < v_{\\pi}(S_3)$\n",
    "- $v_{c \\pi}(S_3) < v_{c \\pi}(S_2) < v_{c \\pi}(S_1) < v_{c \\pi}(S_0)$ (states in the initial steps were more affected by adding c to the rewards, considering the actions and new states the same; in practice, the agent will try to choose other actions to procrastinate and receive more rewards when c > 0, and to end faster when c < 0, but that will depend on the possible actions and new states for each state)\n",
    "\n",
    "The previous example considered only the state values, keeping all actions, new states and number of time steps the same, to show that the state values changed and have not kept the same order as before.\n",
    "\n",
    "To explain how the agent may choose different actions, below is a simpler example (from the perspective of states and possible actions).\n",
    "\n",
    "\n",
    "**Example 02**\n",
    "\n",
    "- 2 states $S_0$ and $S_1$\n",
    "- 2 actions $A_0$ and $A_1$\n",
    "- Reward 5 if chooses $A_1$, going to state $S_1$ and ending the episode\n",
    "- Reward -1 if chooses $A_0$, ending in the same state $S_0$ (non-terminal)\n",
    "- The agent will try maximize as a function of T: $\\gamma^T \\cdot (c+5) + \\sum_{t=0}^{T-1} \\gamma^t \\cdot (c-1) = \\gamma^T \\cdot (c+5) + (c-1) \\cdot \\frac{1-\\gamma^T}{1-\\gamma}$ (it will choose $A_0$ repeatedly until it is not beneficial anymore)\n",
    "- For c=0 (the original case), the best value of T is 0 (will only choose $A_1$ and end the episode)\n",
    "\n",
    "If $\\gamma = 0.9$ and $c=10$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\gamma^T \\cdot (c+5) + (c-1) \\cdot \\frac{1-\\gamma^T}{1-\\gamma} &= 0.9^T \\cdot (10+5) + (10-1) \\cdot \\frac{1-0.9^T}{1-0.9} \\\\\n",
    "&= 15 \\cdot 0.9^T + 90 \\cdot (1-0.9^T) \\\\\n",
    "&= 15 \\cdot 0.9^T + 90 - 90 \\cdot 0.9^T \\\\\n",
    "&= 90 - 75 \\cdot 0.9^T\n",
    "\\end{align*}\n",
    "\n",
    "As T approaches $\\infty$, $90 - 75 \\cdot 0.9^T$ approaches 90 (it increases as T increases), so the agent will instead choose $A_0$ infinitely (in the original case, for $c=0$, it will choose $A_1$ instead and end the episode).\n",
    "\n",
    "As a general case:\n",
    "\n",
    "\\begin{align*}\n",
    "\\gamma^T \\cdot (c+5) + (c-1) \\cdot \\frac{1-\\gamma^T}{1-\\gamma} &= \\gamma^T \\cdot (c+5) + (1-\\gamma^T) \\cdot \\frac{c-1}{1-\\gamma} \\\\\n",
    "&= (c+5) \\cdot \\gamma^T + \\frac{c-1}{1-\\gamma} - \\frac{c-1}{1-\\gamma} \\cdot \\gamma^T \\\\\n",
    "&= \\frac{c-1}{1-\\gamma} + [c + 5 - \\frac{c-1}{1-\\gamma}] \\cdot \\gamma^T \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The objective would be to maximize $\\frac{c-1}{1-\\gamma} + \\left[c + 5 - \\frac{c-1}{1-\\gamma}\\right] \\cdot \\gamma^T$\n",
    "\n",
    "For given values of c and $\\gamma$ (0 < $\\gamma$ < 1), the objective is actually to maximize $\\left[c + 5 - \\frac{c-1}{1-\\gamma}\\right] \\cdot \\gamma^T$ over T (because $\\frac{c-1}{1-\\gamma}$ would be constant). The optimal values of T are:\n",
    "\n",
    "- $T=0$ if $c + 5 - \\frac{c-1}{1-\\gamma} > 0$, because $\\gamma^T$ decreases as T increases, and the expression would reduce its value as T increases (the original case, c=0, gives result 5, resulting in T=0 no matter the value of $\\gamma$, which is expected)\n",
    "- $T=\\infty$ if $c + 5 - \\frac{c-1}{1-\\gamma} < 0$, because $\\gamma^T$ decreases as T increases, and the expression would increase its value as T increases (the negative part would decrease in its absolute value, increasing the result)\n",
    "- T can be anything if $c + 5 - \\frac{c-1}{1-\\gamma} = 0$, (no matter how many times you can $A_0$, and then $A_1$, the resulting cumulative reward would be the same; this is because the cumulative reward of choosing $A_0$ until T-1, and then $A_1$, is the same for any T, because the reduction in the part of the reward of $A_1$ due to the increase of T, is equally compensated by calling $A_0$ more times)\n",
    "\n",
    "*Note:* the difference in the cumulative reward of changing $T=T_1$ to $T=T_2$, in the example above, when $c + 5 - \\frac{c-1}{1-\\gamma} = 0$, is 0, as shown below:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left[(c+5) \\cdot \\gamma^{T_2} + (c-1) \\cdot \\frac{1-\\gamma^{T_2}}{1-\\gamma}\\right] - \\left[(c+5) \\cdot \\gamma^{T_1} + (c-1) \\cdot \\frac{1-\\gamma^{T_1}}{1-\\gamma}\\right] &= \\\\\n",
    "(c+5) \\cdot (\\gamma^{T_2} - \\gamma^{T_1}) + \\frac{c-1}{1-\\gamma} \\cdot ((1-\\gamma^{T_2}) - (1-\\gamma^{T_1})) &= \\\\\n",
    "(c+5) \\cdot (\\gamma^{T_2} - \\gamma^{T_1}) + \\frac{c-1}{1-\\gamma} \\cdot (\\gamma^{T_1} - \\gamma^{T_2}) &= \\\\\n",
    "(\\gamma^{T_2} - \\gamma^{T_1})\\left((c+5) - \\frac{c-1}{1-\\gamma}\\right) &= 0\n",
    "\\end{align*}\n",
    "\n",
    "with $(c+5) \\cdot \\gamma^T$ due the reward of the last action ($A_1$, imediate reward $c+5$), and $(c-1) \\cdot \\frac{1-\\gamma^T}{1-\\gamma}$ due to the reward of all the actions ($A_0$, imediate reward $c-1$), except the last.\n",
    "\n",
    "This means that there's no change in the cumulative reward in the case $c + 5 - \\frac{c-1}{1-\\gamma} = 0$, and any number of actions $A_0$ folowed by the last action $A_1$ will always give the same cumulative reward.\n",
    "\n",
    "In this case, for a given $\\gamma$, the value of c is:\n",
    "\n",
    "\\begin{align*}\n",
    "c + 5 - \\frac{c-1}{1-\\gamma} &= 0 \\\\\n",
    "c + 5 &= \\frac{c-1}{1-\\gamma} \\\\\n",
    "c + 5 - c \\cdot \\gamma - 5 \\gamma &= c - 1 \\\\\n",
    "5 + 1 - 5 \\gamma &= c \\cdot \\gamma \\\\\n",
    "c \\cdot \\gamma &= 6 - 5 \\gamma \\\\\n",
    "c &= \\frac{6}{\\gamma} - 5\n",
    "\\end{align*}\n",
    "\n",
    "This means that, for $c = \\frac{6}{\\gamma} - 5$, choosing any number of actions $A_0$ followed by the final action $A_1$ gives the same cumulative result (this can be seen as a special case). It's valid to note that because $0 < \\gamma < 1$, the value of c will be necessarily positive. \n",
    "\n",
    "For lower values of c, it will behave as the original case (with $c = 0$ being the original case).\n",
    "\n",
    "For higher values of c, it will call $A_0$ infinitely, never ending the episode.\n",
    "\n",
    "So, for the states, actions and rewards defined previously, choosing $c >= \\frac{6}{\\gamma} - 5$ changes the state values and the choices the agent will make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.17\n",
    "\n",
    "**Q**\n",
    "\n",
    "What is the Bellman equation for action values, that is, for $q_π$? It must give the action value $q_π(s, a)$ in terms of the action values, $q_π(s', a')$, of possible successors to the state–action pair (s, a). Hint: The backup diagram to the right corresponds to this equation. Show the sequence of equations analogous to (3.14), but for action values.\n",
    "\n",
    "**A**\n",
    "\n",
    "The Bellman equation for state values is:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G{t+1} | S_t=s] \\\\\n",
    "&= \\sum_a \\pi (a|s) \\sum_{s'} \\sum_r p(s', r | s, a) \\left[ r + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s'] \\right] \\\\\n",
    "&= \\sum_a \\pi (a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] \\text{, for all } s \\in \\mathcal{S} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The Bellman equation for action values is:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_{s'} \\sum_r p(s', r | s, a) \\left[ r + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s'] \\right] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a' | s') \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s', A_{t+1}=a'] \\right] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a' | s') q_{\\pi}(s', a') \\right] \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.18\n",
    "\n",
    "**A**\n",
    "\n",
    "By definition:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a]\n",
    "\\end{align*}\n",
    "\n",
    "The equation of $v_{\\pi}(s)$ in terms of the expectation of $q_{\\pi}(s, a)$ is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[q_{\\pi}(s, a) | S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "The equation of $v_{\\pi}(s)$ in terms of $q_{\\pi}(s, a)$ with no expected value notation is:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[q_{\\pi}(s, a) | S_t=s] \\\\\n",
    "&= \\sum_a \\pi(a | s) \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_a \\pi(a | s) q_{\\pi}(s, a)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.19\n",
    "\n",
    "**A**\n",
    "\n",
    "By definition:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a]\n",
    "\\end{align*}\n",
    "\n",
    "The equation of $q_{\\pi}(s, a)$ in terms of the expectation of $R_{t+1}$ and $v_{\\pi}(S_{t+1})$, but not conditioned on following the policy ($\\pi$), is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The equation of $q_{\\pi}(s, a)$ explicitly in terms of $p(s', r | s, a)$ with no expected value notation is:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_{\\pi}(s')]\n",
    "\\end{align*}\n",
    "\n",
    "because:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[R_{t+1} | S_t=s, A_t=a] = \\sum_{s', r} p(s', r | s, a) \\cdot r\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] &= \\mathbb{E}[G_{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) \\mathbb{E}[G_{t+1} | S_{t+1}=s'] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) v_{\\pi}(s')\n",
    "\\end{align*}\n",
    "\n",
    "(this can also be seen in the answer of Exercise 3.13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.20\n",
    "\n",
    "**Q**\n",
    "\n",
    "Draw or describe the optimal state-value function for the golf example.\n",
    "\n",
    "**A**\n",
    "\n",
    "Considering $\\mathcal{S} = {-3, -2, green}$ as all possible states, more specifically, $s = -3$, $s = -2$ and $s = green$ as the regions between the contour line labeled -3 and -2, between -2 and the green area and the green area, respectively, the optimal actions are: \n",
    "\n",
    "- To call the driver at $s = -3$ and $s = -2$; and \n",
    "- To call the putter at $s = green$ \n",
    "\n",
    "(I haven't added $s = -1$ as another state, where you could either use the putter or the driver with the same result, for simplicity, and consider only the green area, that contains the area $s = -1$, as a single state, because at this point the putter can be used with better or equal results than the driver)\n",
    "\n",
    "Considering $v_*(s)$ as the optimal state-value at state s, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_*(-3) &= -3 \\\\\n",
    "v_*(-2) &= -2 \\\\\n",
    "v_*(green) &= -1\n",
    "\\end{align*}\n",
    "\n",
    "Because at $s = -3$ we need exactly 3 strokes to complete the hole in the optimal case (2 drivers and 1 putter), at $s = -2$ we need exactly 2 strokes, and at $s = green$ we need exactly 1 stroke (using the putter, although if added another state $s = -1$ we could consider the driver too, but the state value would still remain as -1, so that's not necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.21\n",
    "\n",
    "**Q**\n",
    "\n",
    "Draw or describe the contours of the optimal action-value function for putting, $q_∗(s, putter)$, for the golf example.\n",
    "\n",
    "**A**\n",
    "\n",
    "The optimal policy is being followed, so even if using the putter is not the best action at the given state, the following actions must be optimals.\n",
    "\n",
    "The actions will then be:\n",
    "\n",
    "1. Use the putter the first time (because it is considering $q_*(s, putter)$ initially, even if putter is not the best action). If it's in the green area, it will reach the hole (it will start at step 3 already).\n",
    "2. Use the driver until it reaches the green area. It might not need to use the driver tough, if the ball was close enough to the green area; in the picture with the countors of $v_{putt}$, $q_*(-2, putter) = -2$. Otherwise, the value will depend based on the contours of the driver (second picture). \n",
    "3. Use the putter in the green area to complete the hole, so $q_*(green, putter) = -1$.\n",
    "\n",
    "If using the putter the first time takes the ball to the next region of the driver, or to the green area, or the state is already in the green area initially, then the total number of strokes will be equal to the optimal case (driver until green, then putter), and $q_*(s, putter) = v_*(s)$, otherwise it will be $q_*(s, putter) = v_*(s) - 1$ (it will need one more stroke than the case of choosing an optimal first action / driver).\n",
    "\n",
    "In terms of the equation:\n",
    "\n",
    "\\begin{align*}\n",
    "q_*(s, a) = \\sum_{s', r} p(s', r | s, a) \\left[r + \\gamma \\cdot \\operatorname*{argmax}_{a'} q(s', a')\\right]\n",
    "\\end{align*}\n",
    "\n",
    "In the golf example, $r = -1$ (each stroke penalizes with -1) and $\\gamma = 1$ (each value is directly added to the reward, as can be seen by the values of v and q in the example).\n",
    "\n",
    "In this specific case, using the putter in the green area will complete the hole, otherwise, considering the driver regions, it will either stay in the same region ($s' = s_{same} = s$, worse than the driver) or go to the next ($s' = s_{next}$, same as the driver), so we have the 3 following cases (for a given $S_{t+1}=s'$, the reward r is -1 and $p(s', r | s, a) = 1$):\n",
    "\n",
    "1. $q_*(green, putter) = 1 \\cdot [-1 + 1 * 0] = -1 = v_*(green) = v_*(s)$ (note: $q(s', a') = 0$ in this case, because s' is the final state)\n",
    "2. $q_*(s, putter) = 1 \\cdot [-1 + v_*(s_{same})] = v_*(s_{same}) - 1 = v_*(s) - 1$ (suboptimal action; the driver was a better first action)\n",
    "3. $q_*(s, putter) = 1 \\cdot [-1 + v_*(s_{next})] = v_*(s_{next}) - 1 = v_*(s)$ (optimal action; equal value as the driver, because the next state is the same as it would be if the driver was used; note: $v_*(s_{next}) - 1 = v_*(s)$ because, in the optimal policy, each next state is separated by exactly one stroke, with a reward of -1)\n",
    "\n",
    "The actual value of $q_*(s, putter)$ will depend on the initial state s, but it will end up being one of the 3 cases above.\n",
    "\n",
    "These solutions are the same as what was explained above:\n",
    "\n",
    "- When the initial state is the green area, the value is $q_*(s, putter) = -1 = v_*(green) = v_*(s)$;\n",
    "- Otherwise, it will be either $v_*(s)$ or $v_*(s) - 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.22\n",
    "\n",
    "**A**\n",
    "\n",
    "The state values of the policies are:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{left}(s) &= \\sum_{s', r} p(s', r | s, left) [r + \\gamma \\cdot v_{left}(s')] \\\\\n",
    "&= 1 \\cdot [r_{left} + \\gamma \\cdot v_{left}(s')] \\\\\n",
    "&= r_{left} + \\gamma \\cdot v_{left}(s') \\\\\n",
    "&= r_{left} + \\gamma \\cdot [r' + \\gamma \\cdot v_{left}(s)] \\\\\n",
    "&= r_{left} + \\gamma \\cdot [0 + \\gamma \\cdot v_{left}(s)] \\\\\n",
    "&= r_{left} + \\gamma^2 \\cdot v_{left}(s) \\\\\n",
    "&= 1 + \\gamma^2 \\cdot v_{left}(s)\n",
    "\\end{align*}\n",
    "\n",
    "**Important:** This is a continuing MDP, so there's no final state, but it happens in cycles, starting at the top state s and returning to it, to repeat the cycle again.\n",
    "\n",
    "*Note:* $p(s', r | s, left) = 1$ because there's only one possible next state s', the reward of choosing left is $r_{left} = 1$, and the next reward $r' = 0$, returning to the initial state s.\n",
    "\n",
    "Solving for $v_{left}$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{left}(s) &= 1 + \\gamma^2 \\cdot v_{left}(s) \\\\\n",
    "v_{left}(s) - \\gamma^2 \\cdot v_{left}(s) &= 1 \\\\\n",
    "v_{left}(s) &= \\frac{1}{1 - \\gamma^2}\n",
    "\\end{align*}\n",
    "\n",
    "And similarly for $v_{right}$:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{right}(s) &= \\sum_{s', r} p(s', r | s, right) [r + \\gamma \\cdot v_{right}(s')] \\\\\n",
    "&= 1 \\cdot [r_{right} + \\gamma \\cdot v_{right}(s')] \\\\\n",
    "&= r_{right} + \\gamma \\cdot v_{right}(s') \\\\\n",
    "&= r_{right} + \\gamma \\cdot [r' + \\gamma \\cdot v_{right}(s)] \\\\\n",
    "&= 0 + \\gamma \\cdot [2 + \\gamma \\cdot v_{right}(s)] \\\\\n",
    "&= 2 \\gamma + \\gamma^2 \\cdot v_{right}(s) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "*Note:* $p(s', r | s, right) = 1$ because there's only one possible next state s', the reward of choosing right is $r_{right} = 0$, and the next reward $r' = 2$, returning to the initial state s.\n",
    "\n",
    "Solving for $v_{right}$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{right}(s) &= 2 \\gamma + \\gamma^2 \\cdot v_{right}(s) \\\\\n",
    "v_{right}(s) - \\gamma^2 \\cdot v_{right}(s) &= 2 \\gamma \\\\\n",
    "v_{right}(s) &= \\frac{2 \\gamma}{1 - \\gamma^2}\n",
    "\\end{align*}\n",
    "\n",
    "The best policy, for a given $\\gamma$, is the policy that has the best state value in the initial (top) state s.\n",
    "\n",
    "**Q:** What policy is optimal if $\\gamma = 0$?\n",
    "\n",
    "**A:** We have $v_{left}(s) = \\frac{1}{1 - 0^2} = 1$ and $v_{right}(s) = \\frac{2 \\cdot 0}{1 - 0^2} = 0$. So, the best policy is $\\pi_{left}$ (*note:* $\\gamma = 0$ means that only the direct reward is considered).\n",
    "\n",
    "**Q:** If $\\gamma = 0.9$?\n",
    "\n",
    "**A:** We have $v_{left}(s) = \\frac{1}{1 - 0.9^2} = \\frac{1}{1 - 0.81} = \\frac{1}{0.19} \\approx 5.263$ and $v_{right}(s) = \\frac{2 \\cdot 0.9}{1 - 0.9^2} = \\frac{1.8}{0.19} \\approx 9.474$. So, the best policy is $\\pi_{right}$.\n",
    "\n",
    "**Q:** If $\\gamma = 0.5$?\n",
    "\n",
    "**A:** We have $v_{left}(s) = \\frac{1}{1 - 0.5^2} = \\frac{1}{1 - 0.25} = \\frac{1}{0.75} = \\frac{4}{3} \\approx 1.333$ and $v_{right}(s) = \\frac{2 \\cdot 0.5}{1 - 0.5^2} = \\frac{1}{0.75} = \\frac{4}{3} \\approx 1.333$. So, using both policies will give the same value, and any of the policies $\\pi_{left}$ and $\\pi_{right}$ can be choosen."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
