{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "**Q**\n",
    "\n",
    "Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as *different* from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.\n",
    "\n",
    "**A**\n",
    "\n",
    "Here are three very different examples of tasks that can fit the MDP framework:\n",
    "\n",
    "1. **Warehouse Robot**  \n",
    "   - **States:** The robot's location in the warehouse, the location of obstacles, and the status of shelves (whether they need restocking).\n",
    "   - **Actions:** Move forward, turn left, turn right, pick up item, drop item.\n",
    "   - **Rewards:** +10 for successfully stocking a shelf, -1 for each move taken (to encourage efficiency), and -20 for hitting an obstacle.\n",
    "   - **Stretch Factor:** Here, the robot’s environment changes frequently with moving obstacles, pushing the MDP to handle dynamic states.\n",
    "\n",
    "2. **Stock Trading Bot**  \n",
    "   - **States:** Stock prices, current holdings, account balance, and recent trends (like a basic moving average).\n",
    "   - **Actions:** Buy a stock, sell a stock, hold.\n",
    "   - **Rewards:** Change in account balance, with bonuses for profitable trades and penalties for holding onto losses.\n",
    "   - **Stretch Factor:** The continuous nature of stock prices means a discrete time step MDP might miss some timing-based nuances, but it still works if simplified to periodic checks.\n",
    "\n",
    "3. **Medical Treatment Planning**  \n",
    "   - **States:** Patient’s current health status (vitals, symptoms), medical history, and current treatments.\n",
    "   - **Actions:** Prescribe medication, adjust dosage, or order a test.\n",
    "   - **Rewards:** Positive points for health improvements, negative points for side effects or worsening conditions, and a big reward for full recovery.\n",
    "   - **Stretch Factor:** Here, the MDP has to approximate a complex, open-ended human health goal. Rewards are tricky, as they’re not just about immediate changes but about long-term well-being, which challenges the typical short-term focus of MDPs.\n",
    "\n",
    "Next is a proof of concept for an agent to solve mathematical expressions:\n",
    "\n",
    "### Agent Specification\n",
    "\n",
    "The objective of the agent is to determine if two given expressions, `expression1` and `expression2`, are mathematically equivalent by applying a series of valid transformations. The agent must use known mathematical properties and definitions to transform `expression1` into `expression2`. If the transformation is successful, the agent will have proven that `expression1 = expression2`. If not, and the process reaches a contradiction, it shows that the expressions are not equivalent.\n",
    "\n",
    "### Environment Specification\n",
    "\n",
    "The environment is modeled as a Markov Decision Process (MDP), where each element is defined as follows:\n",
    "\n",
    "#### 1. States\n",
    "- **State Space $S$**: Each state $S_i$ is a representation of an intermediate form of `expression1` after a series of transformations.\n",
    "  - **Initial State $S_0$**: The initial state corresponds to `expression1`.\n",
    "  - **Goal State $S_{target}$**: The goal state is reached when the agent’s current expression matches `expression2`.\n",
    "  - **Contradictory State**: A special terminal state that indicates the transformations cannot produce an equivalent form of `expression2`.\n",
    "\n",
    "#### 2. Actions\n",
    "- **Action Space $A$**: Each action represents a transformation based on mathematical properties or definitions known to the agent.\n",
    "  - **Examples of Actions**:\n",
    "    - **Exponentiation**: e.g., $X^2 = X \\times X$\n",
    "    - **Associative Product**: e.g., $(a + b)(c + d) = ac + ad + bc + bd$\n",
    "    - **Commutative Property of Addition and Multiplication**: e.g., $ a + b = b + a $, $ a \\times b = b \\times a $\n",
    "    - **Simplification**: e.g., $ A + A = 2A $\n",
    "  - Each action, when applied, results in a new state.\n",
    "\n",
    "#### 3. Transition Function\n",
    "- **Transition Function $ T(S, A) $**: Determines the resulting state after an action is applied to a given state.\n",
    "  - **Deterministic Transitions**: Each action leads deterministically to a new state by applying a mathematical rule directly to the expression.\n",
    "  - **Example**: Applying the associative product action to $ (a + b)(a + b) $ yields $ aa + ab + ba + bb $.\n",
    "\n",
    "#### 4. Rewards\n",
    "- **Reward Function $ R $**:\n",
    "  - **Goal Achievement**: A positive reward for reaching the goal state $ S_{target} $ (proving `expression1 = expression2`).\n",
    "  - **Intermediate Rewards**: Small positive rewards for transformations that bring the expression closer in structure to `expression2`.\n",
    "  - **Contradiction Penalty**: A negative reward (penalty) if a transformation leads to a contradiction or state where no further useful transformations are possible.\n",
    "\n",
    "#### 5. Episode End Conditions\n",
    "- **Success Condition**: The episode ends successfully if the agent reaches the target expression, showing that `expression1` is equivalent to `expression2`.\n",
    "- **Failure Condition**: The episode ends in failure if the agent reaches a contradictory state or if no further transformations progress toward the goal.\n",
    "\n",
    "### Additional Notes\n",
    "- **Heuristic Guidance**: The environment may employ heuristics to estimate structural similarity to the target expression, guiding the agent’s actions.\n",
    "- **Exploration**: If multiple transformations apply, the agent may prioritize transformations that reduce the complexity or number of terms in the expression.\n",
    "- **Cycles**: It may be useful to verify if the agent ends up in the same state as it was in some previous time-step to avoid an infinite loop of actions, or give a negative reward for each time-step with truncation if the objective was not reached after lots of actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "\n",
    "**Q**\n",
    "\n",
    "Is the MDP framework adequate to usefully represent *all* goal-directed learning tasks? Can you think of any clear exceptions?\n",
    "\n",
    "**A**\n",
    "\n",
    "The MDP framework works well for a lot of goal-oriented tasks, but it has some real limitations too. It assumes that only the current state matters for deciding the next move, which isn’t always true—some tasks need memory of past states. Think about language tasks, where you need context from earlier sentences to understand what’s going on; MDPs aren’t great for that.\n",
    "\n",
    "Also, for tasks with complex goals, like multi-step or layered objectives (say, safe driving while optimizing travel time), MDPs can fall short. They assume a straightforward reward system, which doesn’t always fit real-world problems with ethical or social layers, like in healthcare advice.\n",
    "\n",
    "MDPs usually work in discrete time steps, but that’s not ideal for continuous-time environments, like financial trading, where events are irregular. And for open-ended tasks, like creating art, there’s no clear goal or reward, so MDPs feel pretty limiting there too.\n",
    "\n",
    "In short, MDPs are super helpful, but for memory-dependent tasks, hierarchical goals, continuous events, and tasks with ethical complexity, we’d need to look beyond them to other frameworks that can handle those nuances better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "\n",
    "**Q**\n",
    "\n",
    "Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out — say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in — say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?\n",
    "\n",
    "**A**\n",
    "\n",
    "The appropriate choice in each case depends on how the agent interacts with the environment:\n",
    "\n",
    "- In the case of a person, it would be natural for the actions to be in terms of the accelerator, steering wheel, and brake (but keeping the high level with the choices of where to drive in mind).\n",
    "\n",
    "- If you are a passenger asking for the driver to take you somewhere, the high level with the choices of where to drive should be the most appropriate.\n",
    "\n",
    "- If the agent is a robot or android, then the actions being muscle twitches to control the limbs (or mechanical joints) could be appropriate, with an appropriate reward related to the destination. It could also has the same actions as the 1st example (person).\n",
    "\n",
    "- If the agent is an autonomous car, then the actions being tire torques, where the rubber meets the road, would be appropriate.\n",
    "\n",
    "**Basis for Choosing the Action Level:** Each level can be chosen based on factors such as:\n",
    "\n",
    "- **Control and Perception:** The agent’s sensory and control capacities determine what it can directly influence and observe in the environment.\n",
    "\n",
    "- **Complexity and Efficiency:** Higher-level actions simplify decision-making by abstracting low-level details, making it easier to focus on broader goals. For example, an autonomous car might find torque control useful, while a human prefers using controls like the accelerator or steering wheel, which abstract these details.\n",
    "\n",
    "- **Adaptability and Learning:** The chosen level affects the agent's ability to adapt and learn efficiently. If actions are defined too low-level (e.g., muscle twitches for a robot driver), the learning process may become too complex for meaningful real-world application.\n",
    "\n",
    "There’s no fundamentally \"correct\" level, as this is generally a design choice based on task requirements and the specific agent. The right level for actions is often a balance between simplicity, observability, and control relevant to the agent’s capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give a table analogous to that in Example 3.3, but for $p(s', r|s, a)$. It should have columns for $s, a, s', r$, and $p(s', r|s, a)$, and a row for every 4-tuple for which $p(s', r|s, a) > 0$.\n",
    "\n",
    "**A**\n",
    "\n",
    "| s              | a              | | s'             | r              | | p(s', r\\|s, a) |\n",
    "| -------------- | -------------- |-| -------------- | -------------- |-| -------------- |\n",
    "| high           | search         | | high           | $r_{search}$   | | $\\alpha$       |\n",
    "| high           | search         | | low            | $r_{search}$   | | $1 - \\alpha$   |\n",
    "| low            | search         | | high           | -3             | | $1 - \\beta$    |\n",
    "| low            | search         | | low            | $r_{search}$   | | $\\beta$        |\n",
    "| high           | wait           | | high           | $r_{wait}$     | | 1              |\n",
    "| low            | wait           | | low            | $r_{wait}$     | | 1              |\n",
    "| low            | recharge       | | high           | 0              | | 1              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5\n",
    "\n",
    "**Q**\n",
    "\n",
    "The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of (3.3).\n",
    "\n",
    "**A**\n",
    "\n",
    "Given: \n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1 \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "\\end{align*}\n",
    "\n",
    "In episodic tasks, $\\mathcal{S}$ represents the set of all states *minus* the terminal state. \n",
    "\n",
    "To make the equation apply to episodic tasks, the terminal state have to be considered as a possible state of s'.\n",
    "\n",
    "The modified version of the equation is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s' \\in \\mathcal{S^+}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1 \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "\\end{align*}\n",
    "\n",
    "The only change was from $\\mathcal{S}$ to $\\mathcal{S^+}$ for the possible values of s', but the possible values of s must still be in $\\mathcal{S}$ (and not in $\\mathcal{S^+}$), because a terminal state can't have a next state (s')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6 \n",
    "\n",
    "**Q**\n",
    "\n",
    "Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for 1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?\n",
    "\n",
    "**A**\n",
    "\n",
    "The return each time would be $-\\gamma^K$, just like in the continuing case, where $K$ is the number of time-steps in the episode.\n",
    "\n",
    "The difference is that in the episodic case the discount reverts back to 1 when starting a new episode, while in the continuing case it continues to decrease more and more (if $\\gamma \\lt 1$).\n",
    "\n",
    "Assuming that in the episodic case there were $n$ episodes, with $K_i$ time-steps in the episode $i+1$, $0 \\leq i \\lt n$, then the return would be $- \\sum_{i=0}^{n-1} \\gamma^{K_i}$.\n",
    "\n",
    "The equivalent for the continuing case after $n$ failures with $K_i$ time-steps between the failure $i$ and $i + 1$, $0 \\leq i \\lt n$, and considering $\\gamma_i$ the value of $\\gamma$ right after failure $i$ ($\\gamma_0 = 1$), we have $\\gamma_n = \\gamma^{\\sum_{k=0}^{n-1} K_k}$ the discount after n failures. \n",
    "\n",
    "So, the return $r_n$ after $n$ failures would be:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "r_n &= r_{n-1} + \\gamma_{n-1} (-\\gamma^{K_{n-1}}) \n",
    "\\\\\n",
    "&\\text{the return after failure $n$ is the return after failure $n-1$}\n",
    "\\\\\n",
    "&\\text{plus the discount at the time-step right after the failure $n-1$}\n",
    "\\\\\n",
    "&\\text{times the return that would be received in the amount of time-steps between failure $n-1$ and $n$}\n",
    "\\\\\n",
    "&\\text{(that is, $K_{n-1}$) if this was an episodic case with $\\gamma$ starting with 1}\n",
    "\\\\\n",
    "&= r_{n-1} - \\gamma^{\\sum_{k=0}^{n-2} K_k} \\gamma^{K_{n-1}} \n",
    "\\\\\n",
    "&= r_{n-1} - \\gamma^{\\sum_{k=0}^{n-1} K_k} \n",
    "\\\\\n",
    "&= r_{n-1} - \\gamma_n \n",
    "\\\\\n",
    "&= r_{n-2} - \\gamma_{n-1} - \\gamma_n \n",
    "\\\\\n",
    "&= ... \n",
    "\\\\\n",
    "&= r_0 - \\sum_{i=0}^{n-1} \\gamma_{n-i} \\tag{the initial return, before the 1st time-step, $r_0 = 0$}\n",
    "\\\\\n",
    "&= - \\sum_{i=0}^{n-1} \\gamma_{n-i} \n",
    "\\\\\n",
    "&= - \\sum_{i=0}^{n-1} \\gamma^{\\sum_{k=0}^{n-i-1} K_k}\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "Considering the above expression, the discount continues decreasing after failures, by a factor of $\\gamma_n$ after $n$ failures. It's important to note that what actually impact this decrease in discount is not the number of failures itself, but the amount of time-steps since the beginning, as shown in the equation above, so if an agent1 had the 10th failure at the 1000th time-step, and agent2 had its 1st failure at this time-step, then both agents would have the same total discount at this point, but agent2 performed much better and would have a higher return at this point.\n",
    "\n",
    "The sum can be seen as the following:\n",
    "\n",
    "- For $i = 0$, it will consider the last failure: $-\\gamma^{\\sum_{k=0}^{n-1} K_k}$ (note that $\\sum_{k=0}^{n-1} K_k$ is the number of time-steps until the failure $n$).\n",
    "\n",
    "- For $i = n-1$, it will consider the first failure: $-\\gamma^{\\sum_{k=0}^{n-(n-1)-1} K_k} = -\\gamma^{K_0}$ ($K_0$ is the number of time-steps until the first failure).\n",
    "\n",
    "- For $i$ between these 2 extremes, it will give the return for the intermediate failures.\n",
    "\n",
    "Due to the symmetry, the above result can also be written as:\n",
    "\n",
    "$$\n",
    "- \\sum_{i=0}^{n-1} \\gamma^{\\sum_{k=0}^i K_k}\n",
    "$$\n",
    "\n",
    "(in this case, it starts with the partial return of the first failure, and goes until the partial return of the failure $n$)\n",
    "\n",
    "So, the above sum is the sum of the partial returns after each failure, with the partial return of a given failure equal to the number of time-steps from the beginning (time-step 0) until the point of failure. In the example above, agent1 and agent2 would receive the same return for that specific failure at the 1000th time-step, but while this would be the total return of agent2, the return of agent1 is the sum of this partial return plus the returns of each previous failure (and the partial return of each failure is negative, so agent1 will have a worse total return than agent2).\n",
    "\n",
    "The difference in the agent learning regarding the episodic and continuing cases would depend on how and when the agent consider the returns. If in the episodic case the agent updated its policy after each episode based on its return, and in the returning case the agent updated its policy after each failure considering the return after that failure minus the return of the previous failure, and then divided by $\\gamma_{i-1}$ (for the failure $i$), then both agents would behave the same way, for example. On the other hand, if the full return was used by the agent in the continuing case, with every other calculation when updating the policy the same as in the episodic case, it would update the policy slower than the episodic case, as the number of time-steps increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.7\n",
    "\n",
    "**Q**\n",
    "\n",
    "Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes — the successive runs through the maze — so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n",
    "\n",
    "**A** \n",
    "\n",
    "The reward will be the same no matter if the robot escape the maze in 10, 100, or any number of steps. The robot should receive a greater reward if it reaches in less time steps (or penalized if it reaches in more time steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.8\n",
    "\n",
    " **Q**\n",
    " \n",
    " Suppose $\\gamma = 0.5$ and the following sequence of rewards is received $R_1 = -1$, $R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0$, $G_1$, ..., $G_5$? Hint: Work backwards.\n",
    "\n",
    " **A**\n",
    "\n",
    " The return at time step t is:\n",
    " \n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "\n",
    "Considering $g = k - 1$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t &= R_{t+1} + \\sum_{k=1}^{\\infty} \\gamma^k R_{t+k+1} \\\\\n",
    "&= R_{t+1} + \\sum_{g=0}^{\\infty} \\gamma^{g+1} R_{t+[g+1]+1} \\\\\n",
    "&= R_{t+1} + \\gamma \\sum_{g=0}^{\\infty} \\gamma^g R_{[t+1]+g+1} \\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{align*}\n",
    "\n",
    "Valid for all $t < T$, with $G_T = 0$\n",
    "\n",
    "So, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_5 = 0 \\\\\n",
    "G_4 = R_5 + \\gamma G_5 = 2 + 0.5 \\cdot 0 = 2 \\\\\n",
    "G_3 = R_4 + \\gamma G_4 = 3 + 0.5 \\cdot 2 = 4 \\\\\n",
    "G_2 = R_3 + \\gamma G_3 = 6 + 0.5 \\cdot 4 = 8 \\\\\n",
    "G_1 = R_2 + \\gamma G_2 = 2 + 0.5 \\cdot 8 = 6 \\\\\n",
    "G_0 = R_1 + \\gamma G_1 = -1 + 0.5 \\cdot 6 = 2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.9\n",
    "\n",
    "**Q**\n",
    "\n",
    "Suppose γ = 0.9 and the reward sequence is R1 = 2 followed by an infinite sequence of 7s. What are G1 and G0?\n",
    "\n",
    "**A**\n",
    "\n",
    "\\begin{align*}\n",
    "G_1 &= \\sum_{k=0}^{\\infty} \\gamma^k R_{1+k+1} \\\\\n",
    "&= \\sum_{k=0}^{\\infty} (\\gamma^k \\cdot 7) \\\\\n",
    "&= 7 \\sum_{k=0}^{\\infty} \\gamma^k \\\\\n",
    "&= 7 \\frac{1}{1 - \\gamma} \\\\\n",
    "&= \\frac{7}{1 - 0.9} \\\\\n",
    "&= \\frac{7}{0.1} \\\\\n",
    "&= 70\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "G_0 &= R_1 + \\gamma G_1 \\\\\n",
    "&= 2 + 0.9 \\cdot 70 \\\\\n",
    "&= 2 + 63 \\\\\n",
    "&= 65\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.10\n",
    "\n",
    "**Q**\n",
    "\n",
    "Prove the second equality in:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "**A**\n",
    "\n",
    "Initially, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\gamma^0 + \\gamma^1 + \\gamma^2 + ...\n",
    "\\end{align*}\n",
    "\n",
    "For $0 < \\gamma < 1$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - \\gamma G_t &= [\\gamma^0 + \\gamma^1 + \\gamma^2 + ...] - [\\gamma^1 + \\gamma^2 + \\gamma^3 + ...] \\\\\n",
    "&= \\gamma^0 + [\\gamma^1 - \\gamma^1] + [\\gamma^2 - \\gamma^2] + ... \\\\\n",
    "&= \\gamma^0 \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "(it's valid only for $\\gamma < 1$ because $\\gamma^k$ converges to 0 as k goes to $\\infty$ in this case, but not otherwise)\n",
    "\n",
    "Isolating $G_t$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t - \\gamma G_t = G_t \\cdot (1 - \\gamma) = 1\n",
    "\\end{align*}\n",
    "\n",
    "And the solution:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.11\n",
    "\n",
    "**Q**\n",
    "\n",
    "If the current state is $S_t$, and actions are selected according to stochastic policy π, then what is the\n",
    "expectation of $R_{t+1}$ in terms of π and the four-argument function p (3.2)?\n",
    "\n",
    "**A**\n",
    "\n",
    "The four-argument function is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(s', r|s, a) \\stackrel{\\cdot}{=} Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\}\n",
    "\\end{align*}\n",
    "\n",
    "The probability of choosing the action a, given the state s, is $\\pi(a|s)$, and the probability of reward r (and state s'), given the state s and action a is $p(s', r|s, a)$, so the probability of a given state generate a specific action a, a new state s' and the reward r is $\\pi(a|s) \\cdot p(s', r|s, a)$.\n",
    "\n",
    "It's also important to note that the sum of these probabilities over all possible actions in state $S_t=s$ (all $a \\in \\mathcal{A}(s)$), for all possible new states and rewards, is 1:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r|s, a) = 1\\text{, with } S_t=s\n",
    "\\end{align*}\n",
    "\n",
    "The expectation of $R_{t+1}$ is the sum of the probability of each action a, given the state s, multiplied by the sum of each possible reward r multiplied by its probability of occurence given action a, so:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [R_{t+1}|S_t=s] = \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Which is the sum of the weighted rewards of each action in state $S_t=s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.12\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $v_\\pi$ in terms of $q_\\pi$ and $\\pi$.\n",
    "\n",
    "**A**\n",
    "\n",
    "For a state $S_t=s$, the probability of $A_t=a$ is $\\pi(a|s)$ (by definition). The expectation of $G_t$ given s, is the sum of the expectations of $G_t$ for every possible action in state s, multiplied for the probability of that action happening:\n",
    "\n",
    "\\begin{align*}\n",
    "v_\\pi(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s \\right] \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\right) \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\cdot q_{\\pi}(s, a)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.13\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $q_\\pi$ in terms of $v_\\pi$ and the four-argument p.\n",
    "\n",
    "**A**\n",
    "\n",
    "The four-argument function is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(s', r|s, a) \\stackrel{\\cdot}{=} Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\}\n",
    "\\end{align*}\n",
    "\n",
    "$v_{\\pi}$ is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "v_\\pi(s) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s] = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s \\right]\n",
    "\\end{align*}\n",
    "\n",
    "$q_{\\pi}$ is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi} [G_t | S_t=s, A_t=a] = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right]\n",
    "\\end{align*}\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) &= \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ \\gamma^0 R_{t+1} + \\sum_{k=1}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^{k+1} R_{[t+1]+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma \\cdot \\sum_{k=0}^{\\infty} \\gamma^k R_{[t+1]+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right]\n",
    "\\end{align*}\n",
    "\n",
    "Solving the left side:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a)\n",
    "\\end{align*}\n",
    "\n",
    "Because, according to *Exercise 3.11*:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [R_{t+1}|S_t=s] = \\sum_{a \\in \\mathcal{A}(s)} \\left( \\pi(a|s) \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "and in this case we already have a, so the probability of choosing it is 1, and every other action in $\\mathcal{A}(s)$ has probability 0:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} [ R_{t+1} | S_t=s, A_t=a ] = \\sum_{x \\in \\mathcal{A}(s)} \\left( \\mathbb{1}_{x = a} \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, x) \\right) = \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a)\n",
    "\\end{align*}\n",
    "\n",
    "Solving the right side:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right] &= \\gamma \\cdot \\mathbb{E}_{\\pi} \\left[ G_{t+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\gamma \\cdot \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r | s, a) \\cdot \\mathbb{E}_{\\pi} \\left[ G_{t+1} | S_{t+1}=s' \\right] \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s')\n",
    "\\end{align*}\n",
    "\n",
    "($v_{\\pi}(s')$ can only be considered when state s and action a causes the new state s', but this can be considered for every new state, so we multiply the probability of every new state ocurring, which can be calculated with $p(s', r | s, a)$ for every $s' \\in \\mathcal{S}$ and $r \\in \\mathcal{R}$)\n",
    "\n",
    "Finally:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s, a) &=  \\mathbb{E}_{\\pi} \\left[ R_{t+1} | S_t=s, A_t=a \\right] + \\mathbb{E}_{\\pi} \\left[ \\gamma G_{t+1} | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\left[ \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) \\right] + \\left[ \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s') \\right] \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} r \\cdot p(s', r|s, a) + \\gamma \\cdot p(s', r | s, a) \\cdot v_{\\pi}(s') \\\\\n",
    "&= \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s', r|s, a) \\cdot (r + \\gamma \\cdot v_{\\pi}(s'))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.14\n",
    "\n",
    "**Q**\n",
    "\n",
    "The Bellman equation (3.14) must hold for each state for the value function $v_π$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighbouring states, valued at +2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n",
    "\n",
    "**A**\n",
    "\n",
    "The Bellman equation is:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G{t+1} | S_t=s] \\\\\n",
    "&= \\sum_a \\pi (a|s) \\sum_{s'} \\sum_r p(s', r | s, a) \\left[ r + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s'] \\right] \\\\\n",
    "&= \\sum_a \\pi (a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] \\text{, for all } s \\in \\mathcal{S} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The center state, values at +0.7 (s), has the following possible actions:\n",
    "\n",
    "- Go North: direct reward 0, v(s') = 2.3\n",
    "- Go East: direct reward 0, v(s') = 0.4\n",
    "- Go South: direct reward 0, v(s') = -0.4\n",
    "- Go West: direct reward 0, v(s') = 0.7\n",
    "\n",
    "We have $\\gamma = 0.9$ and $\\pi (a|s) = 0.25$ (1/4) for each of the 4 possible actions (go north, east, south or west), because the agent selects all four actions with equal probability in all states (as stated in the Gridworld example definition).\n",
    "\n",
    "Also, for each action, there's only one next state possible (and reward), so $p(s', r | s, a) = 1$ for every $s in \\{a_n, a_e, a_s, a_w\\}$ (all possible actions). \n",
    "\n",
    "So, we have only a single next state s' and reward r for a given state s and action a:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] = r + \\gamma v_{\\pi}(s') \\text{, given } S_t=s \\text{ and } A_t=a\n",
    "\\end{align*}\n",
    "\n",
    "Considering the possible actions and state-values ($v_{\\pi}$) relative to the center position in the example (s, with $v_{\\pi}(s) = 0.7$):\n",
    "\n",
    "- $a_n$: go north, gives $s_n'$, with $v_{\\pi}(s_n') = 2.3$ and $r_n = 0$\n",
    "- $a_e$: go east, gives $s_e'$, with $v_{\\pi}(s_e') = 0.4$ and $r_e = 0$\n",
    "- $a_s$: go south, gives $s_s'$, with $v_{\\pi}(s_s') = -0.4$ and $r_s = 0$\n",
    "- $a_w$: go west, gives $s_w'$, with $v_{\\pi}(s_w') = 0.7$ and $r_w = 0$\n",
    "\n",
    "Applying the Bellman equation:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &= \\sum_a \\pi (a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] \\\\\n",
    "&= \\sum_{a \\in \\{a_n, a_e, a_s, a_w\\}} \\pi (a|s) \\cdot 1 \\cdot [ r + \\gamma v_{\\pi}(s') ] \\\\ \n",
    "&= \\sum_{a \\in \\{a_n, a_e, a_s, a_w\\}} \\pi (a|s) [ r + \\gamma v_{\\pi}(s') ] \\\\ \n",
    "&= \\pi (a_n|s) [ r_n + \\gamma v_{\\pi}(s_n') ] + \\pi (a_e|s) [ r_e + \\gamma v_{\\pi}(s_e') ] + \\pi (a_s|s) [ r_s + \\gamma v_{\\pi}(s_s') ] + \\pi (a_w|s) [ r_w + \\gamma v_{\\pi}(s_w') ] \\\\ \n",
    "&= 0.25 \\cdot [ r_n + \\gamma v_{\\pi}(s_n') ] + 0.25 \\cdot [ r_e + \\gamma v_{\\pi}(s_e') ] + 0.25 \\cdot [ r_s + \\gamma v_{\\pi}(s_s') ] + 0.25 \\cdot [ r_w + \\gamma v_{\\pi}(s_w') ] \\\\ \n",
    "&= 0.25 \\cdot [ r_n + \\gamma v_{\\pi}(s_n') + r_e + \\gamma v_{\\pi}(s_e') + r_s + \\gamma v_{\\pi}(s_s') + r_w + \\gamma v_{\\pi}(s_w') ]\n",
    "\\end{align*}\n",
    "\n",
    "The above solution works for every cell in the gridworld. \n",
    "\n",
    "For any cell that is not A or B, and is not in the edge of the grid (which is the case of the cell in the center), all direct rewards are 0, so:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) = 0.25 \\cdot [ \\gamma v_{\\pi}(s_n') + \\gamma v_{\\pi}(s_e') + \\gamma v_{\\pi}(s_s') + \\gamma v_{\\pi}(s_w') ]\n",
    "\\end{align*}\n",
    "\n",
    "With $\\gamma = 0.9$, for the cell in the center, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &= 0.25 \\cdot [ \\gamma v_{\\pi}(s_n') + \\gamma v_{\\pi}(s_e') + \\gamma v_{\\pi}(s_s') + \\gamma v_{\\pi}(s_w') ] \\\\\n",
    "&= 0.25 \\cdot [ 0.9 \\cdot 2.3 + 0.9 \\cdot -0.4 + 0.9 \\cdot 0.4 + 0.9 \\cdot 0.7 ] \\\\\n",
    "&= 0.25 \\cdot 0.9 \\cdot [ 2.3 - 0.4 + 0.4 + 0.7 ] \\\\\n",
    "&= 0.225 \\cdot 3.0 \\\\\n",
    "&= 0.675 \\approx 0.7 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.15\n",
    "\n",
    "**Q**\n",
    "\n",
    "In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant c to all the rewards adds a constant, $v_c$, to the values of all states, and thus does not affect the relative values of any states under any policies. What is $v_c$ in terms of c and γ?\n",
    "\n",
    "**A**\n",
    "\n",
    "The equation 3.8 is as follows, for $0 <= \\gamma <= 1$:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t \\stackrel{.}{=} R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "\n",
    "Assuming the new value function $G_c$ ($G_{c0}, G_{c1}, G_{c1}, ..., G_{ct}, ...$) and the corresponding reward $R_c$ ($R_{ct}$ at time step t), with $R_{ct} = R_t + c$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} &\\stackrel{.}{=} R_{c[t+1]} + \\gamma R_{c[t+2]} + \\gamma^2 R_{c[t+3]} + ... \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\gamma^k R_{c[t+k+1]} \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\gamma^k [c + R_{t+k+1}] \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\gamma^k \\cdot c + \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\\\\n",
    "&= c \\cdot \\sum_{k=0}^{\\infty} \\gamma^k + G_t\n",
    "\\end{align*}\n",
    "\n",
    "And we have, as shown in a previous exercise (3.10):\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} &= c \\cdot \\sum_{k=0}^{\\infty} \\gamma^k + G_t \\\\\n",
    "&= \\frac{c}{1 - \\gamma} + G_t \\\\\n",
    "&= v_c + G_t\n",
    "\\end{align*}\n",
    "\n",
    "With:\n",
    "\n",
    "\\begin{align*}\n",
    "v_c = \\frac{c}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "And also:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{c \\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_{ct} | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[v_c + G_t | S_t=s] \\\\\n",
    "&= v_c + \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= v_c + v_{\\pi}(s) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "($\\mathbb{E}_{\\pi}[v_c | S_t=s] = v_c$, because $\\gamma$, and consequently $v_c$, is a constant)\n",
    "\n",
    "Based on what was defined and proven previously, the signs of the rewards are not important, as long as the difference between them is kept, because adding a constant c to all rewards adds the constant $v_c = \\frac{c}{1-\\gamma}$ to the values of all states (this means that states with higher values will remain with higher values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.16\n",
    "\n",
    "**Q**\n",
    "\n",
    "Now consider adding a constant c to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.\n",
    "\n",
    "**A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous exercise, we had:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} = c \\cdot \\sum_{k=0}^{\\infty} \\gamma^k + G_t\n",
    "\\end{align*}\n",
    "\n",
    "And:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "\n",
    "Resulting in:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} = \\frac{1}{1 - \\gamma} + G_t\n",
    "\\end{align*}\n",
    "\n",
    "In an episodic task, there's a finite number of states, with a terminal state T.\n",
    "\n",
    "We would have:\n",
    "\n",
    "\\begin{align*}\n",
    "G_{ct} = c \\cdot \\sum_{k=0}^{T-t} \\gamma^k + G_t\n",
    "\\end{align*}\n",
    "\n",
    "And\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{k=0}^{T-t} \\gamma^k - \\gamma \\sum_{k=0}^{T-t} \\gamma^k &= \\sum_{k=0}^{T-t} \\gamma^k - \\sum_{k=0}^{T-t} \\gamma^{k+1} \\\\\n",
    "(1 - \\gamma) \\sum_{k=0}^{T-t} \\gamma^k &= \\gamma^0 - \\gamma^{T-t+1} \\\\\n",
    "\\sum_{k=0}^{T-t} &= \\frac{1 - \\gamma^{T-t+1}}{(1 - \\gamma)}\n",
    "\\end{align*}\n",
    "\n",
    "And similarly:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{c \\pi}(s) = c \\cdot \\frac{1 - \\gamma^{T-t+1}}{(1 - \\gamma)} + v_{\\pi}(s) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This value is not constant, because t is increased by 1 after every time step.\n",
    "\n",
    "The difference between a state value and the next starts with $c \\cdot \\frac{1 - \\gamma^{T+1}}{(1 - \\gamma)}$ (an absolute value higher than c) in the first time step and decreases the absolute value added after each time step, ending in $c \\cdot \\frac{1 - \\gamma^{T-T+1}}{(1 - \\gamma)} = c \\cdot \\frac{1 - \\gamma}{(1 - \\gamma)} = c$ at the last time step T.\n",
    "\n",
    "In other words, this would have an effect because the difference of the values are not the same.\n",
    "\n",
    "**Example 01**\n",
    "\n",
    "Consider:\n",
    "\n",
    "- $\\gamma = 0.5$\n",
    "- 4 time steps t = 0, 1, 2, 3 (T = 3)\n",
    "- 4 states $S_0$, $S_1$, $S_2$, $S_3$, with $S_t$ as the state at time step t ($S_3$ is the final state at t=T=3)\n",
    "- $R_t$ the reward at time step t, t > 0\n",
    "- $A_t$ is the action at time t, t < 4, that changes $S_t$ to $S_{t+1}$ and returns the reward $R_{t+1}$\n",
    "- $v_{\\pi}(s)$ the state value of state s under some policy $\\pi$, with (for the sake of the example) $v_{\\pi}(S_0) = 2$, $v_{\\pi}(S_1) = -3$, $v_{\\pi}(S_2) = 0$, $v_{\\pi}(S_3) = 5$\n",
    "\n",
    "Adding a constant c=10 to the rewards will change the state values to:\n",
    "\n",
    "- $v_{c \\pi}(S_0) = 10 \\cdot \\frac{1 - 0.5^{3-0+1}}{(1 - 0.9)} + 2 = 10 \\cdot \\frac{\\frac{15}{16}}{0.1} + 2 = 100 \\cdot \\frac{15}{16} + 2 = 1500 / 16 + 2 = 93.75 + 2 = 95.75$\n",
    "- $v_{c \\pi}(S_1) = 10 \\cdot \\frac{1 - 0.5^{3-1+1}}{(1 - 0.9)} - 3 = 10 \\cdot \\frac{\\frac{7}{8}}{0.1} - 3 = 100 \\cdot \\frac{14}{16} - 3 = 1400 / 16 - 3 = 87.5 - 3 = 90.5$\n",
    "- $v_{c \\pi}(S_2) = 10 \\cdot \\frac{1 - 0.5^{3-2+1}}{(1 - 0.9)} + 0 = 10 \\cdot \\frac{\\frac{3}{4}}{0.1} + 0 = 100 \\cdot \\frac{12}{16} + 0 = 1200 / 16 + 0 = 75 + 0 = 75$\n",
    "- $v_{c \\pi}(S_3) = 10 \\cdot \\frac{1 - 0.5^{3-3+1}}{(1 - 0.9)} + 5 = 10 \\cdot \\frac{\\frac{1}{2}}{0.1} + 5 = 100 \\cdot \\frac{8}{16} + 5 = 800 / 16 + 5 = 50 + 5 = 55$\n",
    "\n",
    "Initially: \n",
    "\n",
    "- $v_{\\pi}(S_0) = 2$\n",
    "- $v_{\\pi}(S_1) = -3$\n",
    "- $v_{\\pi}(S_2) = 0$\n",
    "- $v_{\\pi}(S_3) = 5$\n",
    "\n",
    "After adding c=10 to the rewards:\n",
    "\n",
    "- $v_{c \\pi}(S_0) = 95.75$\n",
    "- $v_{c \\pi}(S_1) = 90.5$\n",
    "- $v_{c \\pi}(S_2) = 75$\n",
    "- $v_{c \\pi}(S_3) = 55$\n",
    "\n",
    "The difference of the state values changed, as well as the order:\n",
    "\n",
    "- $v_{\\pi}(S_1) < v_{\\pi}(S_2) < _{\\pi}(S_0) < v_{\\pi}(S_3)$\n",
    "- $v_{c \\pi}(S_3) < v_{c \\pi}(S_2) < v_{c \\pi}(S_1) < v_{c \\pi}(S_0)$ (states in the initial steps were more affected by adding c to the rewards, considering the actions and new states the same; in practice, the agent will try to choose other actions to procrastinate and receive more rewards when c > 0, and to end faster when c < 0, but that will depend on the possible actions and new states for each state)\n",
    "\n",
    "The previous example considered only the state values, keeping all actions, new states and number of time steps the same, to show that the state values changed and have not kept the same order as before.\n",
    "\n",
    "To explain how the agent may choose different actions, below is a simpler example (from the perspective of states and possible actions).\n",
    "\n",
    "\n",
    "**Example 02**\n",
    "\n",
    "- 2 states $S_0$ and $S_1$\n",
    "- 2 actions $A_0$ and $A_1$\n",
    "- Reward 5 if chooses $A_1$, going to state $S_1$ and ending the episode\n",
    "- Reward -1 if chooses $A_0$, ending in the same state $S_0$ (non-terminal)\n",
    "- The agent will try maximize as a function of T: $\\gamma^T \\cdot (c+5) + \\sum_{t=0}^{T-1} \\gamma^t \\cdot (c-1) = \\gamma^T \\cdot (c+5) + (c-1) \\cdot \\frac{1-\\gamma^T}{1-\\gamma}$ (it will choose $A_0$ repeatedly until it is not beneficial anymore)\n",
    "- For c=0 (the original case), the best value of T is 0 (will only choose $A_1$ and end the episode)\n",
    "\n",
    "If $\\gamma = 0.9$ and $c=10$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\gamma^T \\cdot (c+5) + (c-1) \\cdot \\frac{1-\\gamma^T}{1-\\gamma} &= 0.9^T \\cdot (10+5) + (10-1) \\cdot \\frac{1-0.9^T}{1-0.9} \\\\\n",
    "&= 15 \\cdot 0.9^T + 90 \\cdot (1-0.9^T) \\\\\n",
    "&= 15 \\cdot 0.9^T + 90 - 90 \\cdot 0.9^T \\\\\n",
    "&= 90 - 75 \\cdot 0.9^T\n",
    "\\end{align*}\n",
    "\n",
    "As T approaches $\\infty$, $90 - 75 \\cdot 0.9^T$ approaches 90 (it increases as T increases), so the agent will instead choose $A_0$ infinitely (in the original case, for $c=0$, it will choose $A_1$ instead and end the episode).\n",
    "\n",
    "As a general case:\n",
    "\n",
    "\\begin{align*}\n",
    "\\gamma^T \\cdot (c+5) + (c-1) \\cdot \\frac{1-\\gamma^T}{1-\\gamma} &= \\gamma^T \\cdot (c+5) + (1-\\gamma^T) \\cdot \\frac{c-1}{1-\\gamma} \\\\\n",
    "&= (c+5) \\cdot \\gamma^T + \\frac{c-1}{1-\\gamma} - \\frac{c-1}{1-\\gamma} \\cdot \\gamma^T \\\\\n",
    "&= \\frac{c-1}{1-\\gamma} + [c + 5 - \\frac{c-1}{1-\\gamma}] \\cdot \\gamma^T \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The objective would be to maximize $\\frac{c-1}{1-\\gamma} + \\left[c + 5 - \\frac{c-1}{1-\\gamma}\\right] \\cdot \\gamma^T$\n",
    "\n",
    "For given values of c and $\\gamma$ (0 < $\\gamma$ < 1), the objective is actually to maximize $\\left[c + 5 - \\frac{c-1}{1-\\gamma}\\right] \\cdot \\gamma^T$ over T (because $\\frac{c-1}{1-\\gamma}$ would be constant). The optimal values of T are:\n",
    "\n",
    "- $T=0$ if $c + 5 - \\frac{c-1}{1-\\gamma} > 0$, because $\\gamma^T$ decreases as T increases, and the expression would reduce its value as T increases (the original case, c=0, gives result 5, resulting in T=0 no matter the value of $\\gamma$, which is expected)\n",
    "- $T=\\infty$ if $c + 5 - \\frac{c-1}{1-\\gamma} < 0$, because $\\gamma^T$ decreases as T increases, and the expression would increase its value as T increases (the negative part would decrease in its absolute value, increasing the result)\n",
    "- T can be anything if $c + 5 - \\frac{c-1}{1-\\gamma} = 0$, (no matter how many times you can $A_0$, and then $A_1$, the resulting cumulative reward would be the same; this is because the cumulative reward of choosing $A_0$ until T-1, and then $A_1$, is the same for any T, because the reduction in the part of the reward of $A_1$ due to the increase of T, is equally compensated by calling $A_0$ more times)\n",
    "\n",
    "*Note:* the difference in the cumulative reward of changing $T=T_1$ to $T=T_2$, in the example above, when $c + 5 - \\frac{c-1}{1-\\gamma} = 0$, is 0, as shown below:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left[(c+5) \\cdot \\gamma^{T_2} + (c-1) \\cdot \\frac{1-\\gamma^{T_2}}{1-\\gamma}\\right] - \\left[(c+5) \\cdot \\gamma^{T_1} + (c-1) \\cdot \\frac{1-\\gamma^{T_1}}{1-\\gamma}\\right] &= \\\\\n",
    "(c+5) \\cdot (\\gamma^{T_2} - \\gamma^{T_1}) + \\frac{c-1}{1-\\gamma} \\cdot ((1-\\gamma^{T_2}) - (1-\\gamma^{T_1})) &= \\\\\n",
    "(c+5) \\cdot (\\gamma^{T_2} - \\gamma^{T_1}) + \\frac{c-1}{1-\\gamma} \\cdot (\\gamma^{T_1} - \\gamma^{T_2}) &= \\\\\n",
    "(\\gamma^{T_2} - \\gamma^{T_1})\\left((c+5) - \\frac{c-1}{1-\\gamma}\\right) &= 0\n",
    "\\end{align*}\n",
    "\n",
    "with $(c+5) \\cdot \\gamma^T$ due the reward of the last action ($A_1$, imediate reward $c+5$), and $(c-1) \\cdot \\frac{1-\\gamma^T}{1-\\gamma}$ due to the reward of all the actions ($A_0$, imediate reward $c-1$), except the last.\n",
    "\n",
    "This means that there's no change in the cumulative reward in the case $c + 5 - \\frac{c-1}{1-\\gamma} = 0$, and any number of actions $A_0$ folowed by the last action $A_1$ will always give the same cumulative reward.\n",
    "\n",
    "In this case, for a given $\\gamma$, the value of c is:\n",
    "\n",
    "\\begin{align*}\n",
    "c + 5 - \\frac{c-1}{1-\\gamma} &= 0 \\\\\n",
    "c + 5 &= \\frac{c-1}{1-\\gamma} \\\\\n",
    "c + 5 - c \\cdot \\gamma - 5 \\gamma &= c - 1 \\\\\n",
    "5 + 1 - 5 \\gamma &= c \\cdot \\gamma \\\\\n",
    "c \\cdot \\gamma &= 6 - 5 \\gamma \\\\\n",
    "c &= \\frac{6}{\\gamma} - 5\n",
    "\\end{align*}\n",
    "\n",
    "This means that, for $c = \\frac{6}{\\gamma} - 5$, choosing any number of actions $A_0$ followed by the final action $A_1$ gives the same cumulative result (this can be seen as a special case). It's valid to note that because $0 < \\gamma < 1$, the value of c will be necessarily positive. \n",
    "\n",
    "For lower values of c, it will behave as the original case (with $c = 0$ being the original case).\n",
    "\n",
    "For higher values of c, it will call $A_0$ infinitely, never ending the episode.\n",
    "\n",
    "So, for the states, actions and rewards defined previously, choosing $c >= \\frac{6}{\\gamma} - 5$ changes the state values and the choices the agent will make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.17\n",
    "\n",
    "**Q**\n",
    "\n",
    "What is the Bellman equation for action values, that is, for $q_π$? It must give the action value $q_π(s, a)$ in terms of the action values, $q_π(s', a')$, of possible successors to the state–action pair (s, a). Hint: The backup diagram to the right corresponds to this equation. Show the sequence of equations analogous to (3.14), but for action values.\n",
    "\n",
    "**A**\n",
    "\n",
    "The Bellman equation for state values is:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G{t+1} | S_t=s] \\\\\n",
    "&= \\sum_a \\pi (a|s) \\sum_{s'} \\sum_r p(s', r | s, a) \\left[ r + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s'] \\right] \\\\\n",
    "&= \\sum_a \\pi (a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] \\text{, for all } s \\in \\mathcal{S} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The Bellman equation for action values is:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_{s'} \\sum_r p(s', r | s, a) \\left[ r + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s'] \\right] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a' | s') \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s', A_{t+1}=a'] \\right] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a' | s') q_{\\pi}(s', a') \\right] \\text{, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.18\n",
    "\n",
    "**A**\n",
    "\n",
    "By definition:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a]\n",
    "\\end{align*}\n",
    "\n",
    "The equation of $v_{\\pi}(s)$ in terms of the expectation of $q_{\\pi}(s, a)$ is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[q_{\\pi}(s, a) | S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "The equation of $v_{\\pi}(s)$ in terms of $q_{\\pi}(s, a)$ with no expected value notation is:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[q_{\\pi}(s, a) | S_t=s] \\\\\n",
    "&= \\sum_a \\pi(a | s) \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_a \\pi(a | s) q_{\\pi}(s, a)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.19\n",
    "\n",
    "**A**\n",
    "\n",
    "By definition:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a]\n",
    "\\end{align*}\n",
    "\n",
    "The equation of $q_{\\pi}(s, a)$ in terms of the expectation of $R_{t+1}$ and $v_{\\pi}(S_{t+1})$, but not conditioned on following the policy ($\\pi$), is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The equation of $q_{\\pi}(s, a)$ explicitly in terms of $p(s', r | s, a)$ with no expected value notation is:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_{\\pi}(s')]\n",
    "\\end{align*}\n",
    "\n",
    "because:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[R_{t+1} | S_t=s, A_t=a] = \\sum_{s', r} p(s', r | s, a) \\cdot r\n",
    "\\end{align*}\n",
    "\n",
    "and:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] &= \\mathbb{E}[G_{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) \\mathbb{E}[G_{t+1} | S_{t+1}=s'] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) v_{\\pi}(s')\n",
    "\\end{align*}\n",
    "\n",
    "(this can also be seen in the answer of Exercise 3.13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.20\n",
    "\n",
    "**Q**\n",
    "\n",
    "Draw or describe the optimal state-value function for the golf example.\n",
    "\n",
    "**A**\n",
    "\n",
    "Considering $\\mathcal{S} = {-3, -2, green}$ as all possible states, more specifically, $s = -3$, $s = -2$ and $s = green$ as the regions between the contour line labeled -3 and -2, between -2 and the green area and the green area, respectively, the optimal actions are: \n",
    "\n",
    "- To call the driver at $s = -3$ and $s = -2$; and \n",
    "- To call the putter at $s = green$ \n",
    "\n",
    "(I haven't added $s = -1$ as another state, where you could either use the putter or the driver with the same result, for simplicity, and consider only the green area, that contains the area $s = -1$, as a single state, because at this point the putter can be used with better or equal results than the driver)\n",
    "\n",
    "Considering $v_*(s)$ as the optimal state-value at state s, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_*(-3) &= -3 \\\\\n",
    "v_*(-2) &= -2 \\\\\n",
    "v_*(green) &= -1\n",
    "\\end{align*}\n",
    "\n",
    "Because at $s = -3$ we need exactly 3 strokes to complete the hole in the optimal case (2 drivers and 1 putter), at $s = -2$ we need exactly 2 strokes, and at $s = green$ we need exactly 1 stroke (using the putter, although if added another state $s = -1$ we could consider the driver too, but the state value would still remain as -1, so that's not necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.21\n",
    "\n",
    "**Q**\n",
    "\n",
    "Draw or describe the contours of the optimal action-value function for putting, $q_∗(s, putter)$, for the golf example.\n",
    "\n",
    "**A**\n",
    "\n",
    "The optimal policy is being followed, so even if using the putter is not the best action at the given state, the following actions must be optimals.\n",
    "\n",
    "The actions will then be:\n",
    "\n",
    "1. Use the putter the first time (because it is considering $q_*(s, putter)$ initially, even if putter is not the best action). If it's in the green area, it will reach the hole (it will start at step 3 already).\n",
    "2. Use the driver until it reaches the green area. It might not need to use the driver tough, if the ball was close enough to the green area; in the picture with the countors of $v_{putt}$, $q_*(-2, putter) = -2$. Otherwise, the value will depend based on the contours of the driver (second picture). \n",
    "3. Use the putter in the green area to complete the hole, so $q_*(green, putter) = -1$.\n",
    "\n",
    "If using the putter the first time takes the ball to the next region of the driver, or to the green area, or the state is already in the green area initially, then the total number of strokes will be equal to the optimal case (driver until green, then putter), and $q_*(s, putter) = v_*(s)$, otherwise it will be $q_*(s, putter) = v_*(s) - 1$ (it will need one more stroke than the case of choosing an optimal first action / driver).\n",
    "\n",
    "In terms of the equation:\n",
    "\n",
    "\\begin{align*}\n",
    "q_*(s, a) = \\sum_{s', r} p(s', r | s, a) \\left[r + \\gamma \\cdot \\operatorname*{argmax}_{a'} q(s', a')\\right]\n",
    "\\end{align*}\n",
    "\n",
    "In the golf example, $r = -1$ (each stroke penalizes with -1) and $\\gamma = 1$ (each value is directly added to the reward, as can be seen by the values of v and q in the example).\n",
    "\n",
    "In this specific case, using the putter in the green area will complete the hole, otherwise, considering the driver regions, it will either stay in the same region ($s' = s_{same} = s$, worse than the driver) or go to the next ($s' = s_{next}$, same as the driver), so we have the 3 following cases (for a given $S_{t+1}=s'$, the reward r is -1 and $p(s', r | s, a) = 1$):\n",
    "\n",
    "1. $q_*(green, putter) = 1 \\cdot [-1 + 1 * 0] = -1 = v_*(green) = v_*(s)$ (note: $q(s', a') = 0$ in this case, because s' is the final state)\n",
    "2. $q_*(s, putter) = 1 \\cdot [-1 + v_*(s_{same})] = v_*(s_{same}) - 1 = v_*(s) - 1$ (suboptimal action; the driver was a better first action)\n",
    "3. $q_*(s, putter) = 1 \\cdot [-1 + v_*(s_{next})] = v_*(s_{next}) - 1 = v_*(s)$ (optimal action; equal value as the driver, because the next state is the same as it would be if the driver was used; note: $v_*(s_{next}) - 1 = v_*(s)$ because, in the optimal policy, each next state is separated by exactly one stroke, with a reward of -1)\n",
    "\n",
    "The actual value of $q_*(s, putter)$ will depend on the initial state s, but it will end up being one of the 3 cases above.\n",
    "\n",
    "These solutions are the same as what was explained above:\n",
    "\n",
    "- When the initial state is the green area, the value is $q_*(s, putter) = -1 = v_*(green) = v_*(s)$;\n",
    "- Otherwise, it will be either $v_*(s)$ or $v_*(s) - 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.22\n",
    "\n",
    "**A**\n",
    "\n",
    "The state values of the policies are:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{left}(s) &= \\sum_{s', r} p(s', r | s, left) [r + \\gamma \\cdot v_{left}(s')] \\\\\n",
    "&= 1 \\cdot [r_{left} + \\gamma \\cdot v_{left}(s')] \\\\\n",
    "&= r_{left} + \\gamma \\cdot v_{left}(s') \\\\\n",
    "&= r_{left} + \\gamma \\cdot [r' + \\gamma \\cdot v_{left}(s)] \\\\\n",
    "&= r_{left} + \\gamma \\cdot [0 + \\gamma \\cdot v_{left}(s)] \\\\\n",
    "&= r_{left} + \\gamma^2 \\cdot v_{left}(s) \\\\\n",
    "&= 1 + \\gamma^2 \\cdot v_{left}(s)\n",
    "\\end{align*}\n",
    "\n",
    "**Important:** This is a continuing MDP, so there's no final state, but it happens in cycles, starting at the top state s and returning to it, to repeat the cycle again. In this case, because the cycles of both policies have the same number of steps, instead of considering it as a continuing MDP, it could be considered as a finite MDP with the episode corresponding to the first cycle to determine the best policy for a given $\\gamma$.\n",
    "\n",
    "*Note:* $p(s', r | s, left) = 1$ because there's only one possible next state s', the reward of choosing left is $r_{left} = 1$, and the next reward $r' = 0$, returning to the initial state s.\n",
    "\n",
    "Solving for $v_{left}$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{left}(s) &= 1 + \\gamma^2 \\cdot v_{left}(s) \\\\\n",
    "v_{left}(s) - \\gamma^2 \\cdot v_{left}(s) &= 1 \\\\\n",
    "v_{left}(s) &= \\frac{1}{1 - \\gamma^2}\n",
    "\\end{align*}\n",
    "\n",
    "And similarly for $v_{right}$:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{right}(s) &= \\sum_{s', r} p(s', r | s, right) [r + \\gamma \\cdot v_{right}(s')] \\\\\n",
    "&= 1 \\cdot [r_{right} + \\gamma \\cdot v_{right}(s')] \\\\\n",
    "&= r_{right} + \\gamma \\cdot v_{right}(s') \\\\\n",
    "&= r_{right} + \\gamma \\cdot [r' + \\gamma \\cdot v_{right}(s)] \\\\\n",
    "&= 0 + \\gamma \\cdot [2 + \\gamma \\cdot v_{right}(s)] \\\\\n",
    "&= 2 \\gamma + \\gamma^2 \\cdot v_{right}(s) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "*Note:* $p(s', r | s, right) = 1$ because there's only one possible next state s', the reward of choosing right is $r_{right} = 0$, and the next reward $r' = 2$, returning to the initial state s.\n",
    "\n",
    "Solving for $v_{right}$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{right}(s) &= 2 \\gamma + \\gamma^2 \\cdot v_{right}(s) \\\\\n",
    "v_{right}(s) - \\gamma^2 \\cdot v_{right}(s) &= 2 \\gamma \\\\\n",
    "v_{right}(s) &= \\frac{2 \\gamma}{1 - \\gamma^2}\n",
    "\\end{align*}\n",
    "\n",
    "The best policy, for a given $\\gamma$, is the policy that has the best state value in the initial (top) state s.\n",
    "\n",
    "**Q:** What policy is optimal if $\\gamma = 0$?\n",
    "\n",
    "**A:** We have $v_{left}(s) = \\frac{1}{1 - 0^2} = 1$ and $v_{right}(s) = \\frac{2 \\cdot 0}{1 - 0^2} = 0$. So, the best policy is $\\pi_{left}$ (*note:* $\\gamma = 0$ means that only the direct reward is considered).\n",
    "\n",
    "**Q:** If $\\gamma = 0.9$?\n",
    "\n",
    "**A:** We have $v_{left}(s) = \\frac{1}{1 - 0.9^2} = \\frac{1}{1 - 0.81} = \\frac{1}{0.19} \\approx 5.263$ and $v_{right}(s) = \\frac{2 \\cdot 0.9}{1 - 0.9^2} = \\frac{1.8}{0.19} \\approx 9.474$. So, the best policy is $\\pi_{right}$.\n",
    "\n",
    "**Q:** If $\\gamma = 0.5$?\n",
    "\n",
    "**A:** We have $v_{left}(s) = \\frac{1}{1 - 0.5^2} = \\frac{1}{1 - 0.25} = \\frac{1}{0.75} = \\frac{4}{3} \\approx 1.333$ and $v_{right}(s) = \\frac{2 \\cdot 0.5}{1 - 0.5^2} = \\frac{1}{0.75} = \\frac{4}{3} \\approx 1.333$. So, using both policies will give the same value, and any of the policies $\\pi_{left}$ and $\\pi_{right}$ can be choosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.23\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give the Bellman equation for $q_∗$ for the recycling robot.\n",
    "\n",
    "**A**\n",
    "\n",
    "Considering v_*(h) and v_*(l) as the optimal states described in the example, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "q_*(h, s) &= p(h|h, s)[r(h, s, h) + \\gamma v_*(h)] + p(l|h, s)[r(h, s, l) + \\gamma v_*(l)] \\\\\n",
    "&= \\alpha[r_s + \\gamma v_*(h)] + (1 - \\alpha)[r_s + \\gamma v_*(l)] \\\\\n",
    "&= \\alpha \\cdot r_s + \\alpha \\cdot \\gamma v_*(h) + r_s + \\gamma v_*(l) - \\alpha \\cdot r_s - \\alpha \\cdot \\gamma v_*(l) \\\\\n",
    "&= \\alpha \\cdot \\gamma v_*(h) + r_s + \\gamma v_*(l) - \\alpha \\cdot \\gamma v_*(l) \\\\\n",
    "&= r_s + \\gamma [\\alpha \\cdot v_*(h) + (1 - \\alpha) v_*(l)]\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "q_*(h, w) &= p(h|h, w)[r(h, w, h) + \\gamma v_*(h)] + p(l|h, w)[r(h, w, l) + \\gamma v_*(l)] \\\\\n",
    "&= 1 \\cdot [r_w + \\gamma v_*(h)] + 0 \\cdot [r_w + \\gamma v_*(l)] \\\\\n",
    "&= r_w + \\gamma v_*(h)\n",
    "\\end{align*}\n",
    "\n",
    "*Note:* The robot should not recharge when the battery is high (this is not a possible action), so $q_*(h, re)$ is not defined.\n",
    "\n",
    "\\begin{align*}\n",
    "q_*(l, s) &= p(h|l, s)[r(l, s, h) + \\gamma v_*(h)] + p(l|l, s)[r(l, s, l) + \\gamma v_*(l)] \\\\\n",
    "&= (1 - \\beta)[-3 + \\gamma v_*(h)] + \\beta[r_s + \\gamma v_*(l)] \\\\\n",
    "&= -3(1 - \\beta) + \\gamma(1 - \\beta)v_*(h) + \\beta r_s + \\gamma \\beta v_*(l) \\\\\n",
    "&= \\beta r_s + -3(1 - \\beta) + \\gamma[(1 - \\beta)v_*(h) + \\beta v_*(l)]\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "q_*(l, w) &= p(h|l, w)[r(l, w, h) + \\gamma v_*(h)] + p(l|l, w)[r(l, w, l) + \\gamma v_*(l)] \\\\\n",
    "&= 0 \\cdot [r_w + \\gamma v_*(h)] + 1 \\cdot [r_w + \\gamma v_*(l)] \\\\\n",
    "&= r_w + \\gamma v_*(l)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "q_*(l, re) &= p(h|l, re)[r(l, re, h) + \\gamma v_*(h)] + p(l|l, re)[r(l, re, l) + \\gamma v_*(l)] \\\\\n",
    "&= 1 \\cdot [0 + \\gamma v_*(h)] + 0 \\cdot [0 + \\gamma v_*(l)] \\\\\n",
    "&= \\gamma v_*(h)\n",
    "\\end{align*}\n",
    "\n",
    "*Note:* The robot should recharge when the battery is low until the battery is high (so $p(h|l, re) = 1$ and $p(l|l, re) = 0$). \n",
    "\n",
    "Also, as stated originally, there's no reward when recharging ($r(l, re, h) = 0$).\n",
    "\n",
    "**Important:** An interesting point is that the values of $v_*(h)$ and $v_*(l)$ stated in the example can be defined based on the values of $q_*$ defined in this answer:\n",
    "\n",
    "\\begin{align*}\n",
    "v_*(h) &= max\\{q_*(h, s), q_*(h, w)\\} \\\\\n",
    "v_*(l) &= max\\{q_*(l, s), q_*(l, w), q_*(l, re)\\}\n",
    "\\end{align*}\n",
    "\n",
    "which is expected, as the state-value of a given state s is the best action-value among all possible actions in that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.24\n",
    "\n",
    "**Q**\n",
    "\n",
    "Figure 3.5 gives the optimal value of the best state of the gridworld as 24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express this value symbolically, and then to compute it to three decimal places.\n",
    "\n",
    "**A**\n",
    "\n",
    "As defined in equation 3.8:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t \\stackrel{.}{=} R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "\n",
    "Following the best policy, once the agent is in state A, it will go to A', and then it will try to return to A using the smallest path (because all other rewards will be 0, considering that it will not go to B, that has a smaller reward than A), restarting the cycle.\n",
    "\n",
    "It can be seen in the figure that after it goes to A', it goes up vertically until it reaches A in 4 steps (5 steps in total, when starting in A), so the return, considering the cycle, and starting in A (because we want to find $v_*(A)$), is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t &\\stackrel{.}{=} [R_A + \\gamma \\cdot 0 + \\gamma^2 \\cdot 0 + \\gamma^3 \\cdot 0 + \\gamma^4 \\cdot 0] + [\\gamma^5 R_A + \\gamma^6 \\cdot 0 + \\gamma^7 \\cdot 0 + \\gamma^8 \\cdot 0 + \\gamma^9 \\cdot 0] + ... \\\\\n",
    "&= R_A + \\gamma^5 R_A + \\gamma^{10} R_A + ... \\\\\n",
    "&= \\sum_{k=0}^{\\infty} \\gamma^{5k} R_A \\\\\n",
    "&= 10 \\sum_{k=0}^{\\infty} \\gamma^{5k}\n",
    "\\end{align*}\n",
    "\n",
    "Solving for $\\sum_{k=0}^{\\infty} \\gamma^{5k}$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{k=0}^{\\infty} \\gamma^{5k} - \\gamma^5 \\sum_{k=0}^{\\infty} \\gamma^{5k} &= \\sum_{k=0}^{\\infty} \\gamma^{5k} - \\sum_{k=1}^{\\infty} \\gamma^{5k} \\\\\n",
    "(1 - \\gamma^5) \\sum_{k=0}^{\\infty} \\gamma^{5k} &= \\gamma^0 + (\\gamma^5 - \\gamma^5) + (\\gamma^{10} - \\gamma^{10}) + ... \\\\\n",
    "(1 - \\gamma^5) \\sum_{k=0}^{\\infty} \\gamma^{5k} &= \\gamma^0 = 1 \\\\\n",
    "\\sum_{k=0}^{\\infty} \\gamma^{5k} &= \\frac{1}{1 - \\gamma^5}\n",
    "\\end{align*}\n",
    "\n",
    "So, to express the return symbolically, the expression is:\n",
    "\n",
    "\\begin{align*}\n",
    "G_t = \\frac{10}{1 - \\gamma^5}\n",
    "\\end{align*}\n",
    "\n",
    "Computing it to 3 decimal places (with the value of $\\gamma = 0.9$, as defined in the original gridworld example):\n",
    "\n",
    "\\begin{align*}\n",
    "G_t &= \\frac{10}{1 - \\gamma^5} \\\\\n",
    "&= \\frac{10}{1 - (\\frac{9}{10})^5} \\\\\n",
    "&= \\frac{10}{\\frac{10^5 - 9^5}{10^5}} \\\\\n",
    "&= \\frac{10^6}{10^5 - 9^5} \\\\\n",
    "&= \\frac{10^6}{100000 - 59049} \\\\\n",
    "&= \\frac{10^6}{40951} \\\\\n",
    "&= 24.419\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.25\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $v_*$ in terms of $q_*$.\n",
    "\n",
    "**A**\n",
    "\n",
    "As shown in exercise 3.18:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[q_{\\pi}(s, a) | S_t=s] \\\\\n",
    "&= \\sum_a \\pi(a | s) \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_a \\pi(a | s) q_{\\pi}(s, a)\n",
    "\\end{align*}\n",
    "\n",
    "The above is the expression of v in terms of q following the policy $\\pi$.\n",
    "\n",
    "When following the optimal policy $\\pi_*$, the action choosen will be the one to maximize q and, consequently, v.\n",
    "\n",
    "This means that $\\pi_*(a|s)$ is 0 if the value of q is not the best, and a non-zero value otherwise (with the sum being 1, and, in the special case of only one best action, the policy will return 1).\n",
    "\n",
    "In other words:\n",
    "\n",
    "\\begin{align*}\n",
    "v_*(s) = \\sum_a \\pi_*(a | s) q_*(s, a) = \\operatorname*{max}_{a \\in \\mathcal{A}(s)} q_*(s, a)\n",
    "\\end{align*}\n",
    "\n",
    "The state-value is equal to the best action-value among all actions (which was seen in a practical example in Exercise 3.23).\n",
    "\n",
    "*Note:* This equation is already defined in the Example 3.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.26\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $q_*$ in terms of $v_*$ and the four-argument p.\n",
    "\n",
    "**A**\n",
    "\n",
    "By definition:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[\\sum_{s', r} p(s', r | s, a) [r + \\gamma v_{\\pi}(s')] | S_t=s, A_t=a, S_{t+1}=s', R_{t+1}=r] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_{\\pi}(s')]\n",
    "\\end{align*}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{align*}\n",
    "q_*(s, a) = \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_*(s')] \\text{, with } S_t=s, A_t=a, S_{t+1}=s', R_{t+1}=r\n",
    "\\end{align*}\n",
    "\n",
    "The action-value is the sum of each probability of a new state and reward from the current state and action, multiplied by the sum of the reward and the optimal state-value of the new state with the discount factor applied (to give less importance to rewards in the far future, unless the discount is 1).\n",
    "\n",
    "The four-argument p is defined by the environment, so the agent can't change it to maximize the value (while the optimal policy to choose an action is defined so as to maximize the value, because the agent owns the policy).\n",
    "\n",
    "It's important to keep in mind that even tough $q_*$ is the optimal action-value, $q_*(s, a)$ may not be the best action-value in state $S_t=s$. In this case, $A_t=a$ was given, and may not be the best action, but all following actions will be the best action, because it follows the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.27\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $\\pi_*$ in terms of $q_*$.\n",
    "\n",
    "**A**\n",
    "\n",
    "The best policy is the policy that chooses the action that gives the highest value in a given state:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_*(a | s) = 1_{q_*(s, a) = \\operatorname*{max}_{a' \\in \\mathcal{A}(s)} q_*(s, a')}\n",
    "\\end{align*}\n",
    "\n",
    "Which means that the policy will have value 1 if the action gives the highest value, otherwise 0.\n",
    "\n",
    "The above considers only one best action. If there are n actions in which $q_*(s, a) = \\operatorname*{max}_{a' \\in \\mathcal{A}(s)} q_*(s, a')$, it can simply be multiplied by $\\frac{1}{n}$ so that the sum of the policy for all actions is exactly 1 (the value of n is at least 1, because at least one action will cause the maximum value).\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_*(a | s) = \\left(\\frac{1}{n}\\right)_{q_*(s, a) = \\operatorname*{max}_{a' \\in \\mathcal{A}(s)} q_*(s, a')}\n",
    "\\end{align*}\n",
    "\n",
    "Or more generally:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_*(a | s) = \\frac{1_{q_*(s, a) = \\operatorname*{max}_{a' \\in \\mathcal{A}(s)} q_*(s, a')}}{\\sum_{a'' \\in \\mathcal{A}(s)} \\left[1_{q_*(s, a'') = \\operatorname*{max}_{a' \\in \\mathcal{A}} q_*(s, a')}\\right]}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.28\n",
    "\n",
    "**Q**\n",
    "\n",
    "Give an equation for $\\pi_*$ in terms of $v_*$ and the four-argument p.\n",
    "\n",
    "**A**\n",
    "\n",
    "According to the previous exercise, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_*(a | s) = \\frac{1_{q_*(s, a) = \\operatorname*{max}_{a' \\in \\mathcal{A}(s)} q_*(s, a')}}{\\sum_{a'' \\in \\mathcal{A}(s)} \\left[1_{q_*(s, a'') = \\operatorname*{max}_{a' \\in \\mathcal{A}} q_*(s, a')}\\right]}\n",
    "\\end{align*}\n",
    "\n",
    "and according the Exercise 3.26:\n",
    "\n",
    "\\begin{align*}\n",
    "q_*(s, a) = \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_*(s')] \\text{, with } S_t=s, A_t=a, S_{t+1}=s', R_{t+1}=r\n",
    "\\end{align*}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_*(a | s) = \\frac{1_{\\sum_{s', r} p(s', r | s, a) [r + \\gamma v_*(s')] = \\operatorname*{max}_{a' \\in \\mathcal{A}(s)} \\sum_{s', r} p(s', r | s, a') [r + \\gamma v_*(s')]}}{\\sum_{a'' \\in \\mathcal{A}(s)} \\left[1_{\\sum_{s', r} p(s', r | s, a'') [r + \\gamma v_*(s')] = \\operatorname*{max}_{a' \\in \\mathcal{A}} \\sum_{s', r} p(s', r | s, a') [r + \\gamma v_*(s')]}\\right]}\n",
    "\\end{align*}\n",
    "\n",
    "Or, simplifying, considering the predicate function:\n",
    "\n",
    "\\begin{align*}\n",
    "f(s, a) = \\left[ \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_*(s')] = \\operatorname*{max}_{a' \\in \\mathcal{A}(s)} \\sum_{s', r} p(s', r | s, a') [r + \\gamma v_*(s')] \\right]\n",
    "\\end{align*}\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_*(a | s) = \\frac{1_{f(s, a)}}{\\sum_{a'' \\in \\mathcal{A}(s)} 1_{f(s, a'')}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.29\n",
    "\n",
    "**Q**\n",
    "\n",
    "Rewrite the four Bellman equations for the four value functions ($v_{\\pi}$, $v_*$, $q_{\\pi}$, and $q_*$) in terms of the three argument function p (3.4) and the two-argument function r (3.5).\n",
    "\n",
    "**A**\n",
    "\n",
    "The three argument function p (3.4) is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(s' | s, a) \\stackrel{.}{=} Pr\\{S_t=s' | S_{t-1} = s, A_{t-1} = a\\} = \\sum_{r \\in \\mathcal{R}} p(s', r | s, a)\n",
    "\\end{align*}\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "\\begin{align*}\n",
    "p(s' | s, a) \\stackrel{.}{=} Pr\\{S_{t+1}=s' | S_t = s, A_t = a\\} = \\sum_{r \\in \\mathcal{R}} p(s', r | s, a)\n",
    "\\end{align*}\n",
    "\n",
    "The two-argument function r (3.5) is:\n",
    "\n",
    "\\begin{align*}\n",
    "r(s, a) \\stackrel{.}{=} \\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r | s, a)\n",
    "\\end{align*}\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "\\begin{align*}\n",
    "r(s, a) \\stackrel{.}{=} \\mathbb{E}[R_{t+1} | S_t = s, A_t = a] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r | s, a)\n",
    "\\end{align*}\n",
    "\n",
    "The state-value function is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "The action-value function is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) \\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a]\n",
    "\\end{align*}\n",
    "\n",
    "Solving for $v_{\\pi}$ in terms of $p(s' | s, a)$ and $r(s, a)$:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}\\left[G_t | S_t=s\\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi}\\left[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s\\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi}\\left[\\sum_{a \\in \\mathcal{A}(s)} \\pi (a | s) [r(s, a) + \\gamma v_{\\pi}(S_{t+1})] | S_t=s, A_t=a\\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi}\\left[\\sum_{a \\in \\mathcal{A}(s)} \\pi (a | s) \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) [r(s, a) + \\gamma v_{\\pi}(s')] | S_t=s, A_t=a, S_{t+1}=s'\\right] \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}(s)} \\pi (a | s) \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) [r(s, a) + \\gamma v_{\\pi}(s')]\n",
    "\\end{align*}\n",
    "\n",
    "with $\\pi$ as a given because $v_{\\pi}$ is the state-value following the policy $\\pi$.\n",
    "\n",
    "It's not stated that $v_{\\pi}(s)$ can't depend on $v_{\\pi}(s')$, after all, it's the same function, just in the next state, and considering the resolution above, this seems as the expected resolution.\n",
    "\n",
    "The optimal state-value $v_*$ is the state-value following the optimal policy $\\pi_*$:\n",
    "\n",
    "\\begin{align*}\n",
    "v_*(s) &= \\sum_{a \\in \\mathcal{A}(s)} \\pi_* (a | s) \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) [r(s, a) + \\gamma v_*(s')] \\\\\n",
    "&= \\operatorname*{max}_a \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) [r(s, a) + \\gamma v_*(s')]\n",
    "\\end{align*}\n",
    "\n",
    "(the optimal policy will choose the action $A_t = a$ that maximizes $v_*(s)$ in state $S_t = s$)\n",
    "\n",
    "Solving for $q_{\\pi}$ in terms of $p(s' | s, a)$ and $r(s, a)$:\n",
    "\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\stackrel{\\cdot}{=} \\mathbb{E}_{\\pi}\\left[G_t | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi}\\left[r(s, a) + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\mathbb{E}_{\\pi}\\left[\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) [r(s, a) + \\gamma v_{\\pi}(s')] | S_t=s, A_t=a, S_{t+1}=s' \\right] \\\\\n",
    "&= p(s' | s, a) [r(s, a) + \\gamma v_{\\pi}(s')]\n",
    "\\end{align*}\n",
    "\n",
    "($q_{\\pi}$ is given in terms of $v_{\\pi}$, and $v_{\\pi}$ can be solved with the corresponding equation for it, defined previously)\n",
    "\n",
    "The optimal action-value $q_*$ is the action-value following the optimal policy $\\pi_*$ for all actions after the first action (because the first action is given, and may not be the best action):\n",
    "\n",
    "\\begin{align*}\n",
    "q_*(s, a) &= p(s' | s, a) [r(s, a) + \\gamma v_*(s')]\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
